# Simple linear regression

Regression analysis is a statistical procedure that allows us to model relationships between variables so that the causal relationship between variables is defined. It can be considered as the main statistical technique used in economics, i.e. econometrics.  There is a large variety of regression models, depending on estimation method, model specification and assumed distributions. This section introduces the most basic of these, the simple linear regression model, i.e. ordinary least squares (OLS) with one independent variable.

See @navarro_learning_2018 section 12.3 for an introduction.

## Ordinary least squares

### Model specification

Simple linear regression model has one predictor and its model is mathematically expressed as follows:

$$y = \alpha + \beta x + \varepsilon,$$

where

- $y$ is dependent or explained variable, **response** or regressand, 
- $\alpha$ is the intercept or constant, 
- $\beta$ is a coefficient of $x$, 
- $x$ is independent or explanatory variable, **predictor** or regressor, and
- $\varepsilon$ is the model error.

### Calculation

The underlying idea behind (ordinary) least squares regression is the minimization of (squared) residuals. Model parameters are calculated (unlike maximum likelihood estimation based on iterations). To estimate the model $Y = \alpha + \beta x + \varepsilon$ we estimate the parameters $\hat{\beta}$ and $\hat{\alpha}$ as follows:

$\hat{\beta} = \frac{\sum{x_{i} y_{i}} - \frac{1}{n} \sum{x_{i}}\sum{y_{i}}}{\sum{x_{i}^{2}} - \frac{1}{n} (\sum{x_{i}})^{2}} = \frac{Cov[x,y]} {Var[x]}$

$\hat{\alpha} = \overline{y} - \beta \overline{x}$

For a simple model $y = \beta x + \varepsilon$ we can simply use matrix algebra on values of $x$ and $y$ to find $\hat{\beta}$:

$$\hat{\beta} = (X^{T} X)^{-1} X^{T} Y,$$

where $X$ is the matrix of predictor and $Y$ the matrix of response.

See @navarro_learning_2018 section 12.4 or illustrations of the idea.

In Jamovi: `Regression > Linear regression     `.  
In R: `lm(formula, data)` or `summary(lm(formula, data))`

## Elements of (OLS) regression models

### Intercept

In the equation this is the $\alpha$ and often referred to as the constant. Intercept is the value of $y$ where regression line crosses the Y-axis, so intercept is the value of $y$ when $x$ is zero ( $y|x=0$ ). Intercept does not need to be theoretically valid but it sometimes is. The statistical significance of the intercept is usually not relevant.

### Coefficient(s)

In the equation the $\beta$ represents coefficient of $x$. It indicates y how many units $y$ increases when $x$ increases by one unit. 

We can not be sure if the coefficients are actually significant (when estimation is done on a sample). It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating t-statistic from coefficient and standard error. Coefficients are only relevant if their difference from 0 is statistically significant. 

### Fitted values

These are the values of $y$ calculated using the model for every $x$ in the data. In other words, fitted values are predictions.

### Residuals

Residuals are model errors, represented by the $\varepsilon$ in the equation. Residuals are the difference in response between observed and fitted (model predicted) values. We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good.

### The $R^2$ 

This is a way to measure *goodness of fit*, i.e. how well model fits data. $R^2$ indicates the part of variation in response variable that is explained by the model:

$$R^{2} = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS},$$

where the elements are defined a follows:

- **e**xplained sum of squares, $ESS$
  $\sum_{i = 1}^{n} (\hat{y}_{i} - \overline{y})^2$
- **r**esidual sum of squares, $RSS$
  $\sum_{i = 1}^{n} (y_{i} - \hat{y}_{i})^2$
- **t**otal sum of squares, $TSS$
  $\sum_{i = 1}^{n} (y_{i} - \overline{y}_{i})^2$

Mathematically, the $R^2$ measures how much better is model at explaining the variance of $y$ compared to just the mean.

### The adjusted- $R^2$ 

The more variables we add, the more the model explains. So $R^2$ can be inflated just by adding variables. To penalize a model for the number of predictors ( $k$ ) while considering the number of observations ( $n$ ), the adjusted $R^{2}$ can also be used, particularly for model comparison:

$$\overline{R^{2}} = 1 - \frac{RSS/(n-k)}{TSS/(n-k)}$$

## Assumptions and diagnostics

After the estimation of a regression model it should be diagnosed to make sure that it meets the following assumptions:

- Residuals are normally distributed
- Residuals have equal variance, i.e. variance of residuals does not depend on the value of $x$
- There is no multicollinearity

In Jamovi: `Regression > Linear regression > Assumption Checks    `.  
In R: `plot(lm(formula, data))`

#### Multicollinarity

Multicollinearity may be problem when model has multiple predictors. It should not be possible to linearly predict any of the predictors from others predictors, i.e. predictors should not be (highly) correlated. Otherwise the coefficients are not reliable. Multicollinearity can be detected with variance inflation factor (VIF) by using $R^2$ to estimate for each predictor how much of the variation in one predictor can be predicted from others.

$$VIF_{k} = \frac{1}{1 - R^{2}_{k}},$$

where $R^{2}_{k}$ is $R^{2}$ for a model that has initial predictor $k$ as a response and all other predictors as predictors.

### Gauss-Markov assumptions

In addition to the practical considerations outlined above, a  theoretical way of expressing the assumptions of OLS is via the Gaussâ€“Markov theorem. It posits the following assumptions:

- linear in parameters  
  $Y = \alpha + \beta x + \varepsilon$
- expected error is zero  
  $E(\varepsilon) = 0$
- homoscedasticity  
  $var(\varepsilon) = E(\varepsilon^{2})$
- no autocorrelation  
  $cov(\varepsilon_{i}, \varepsilon_{j}) = 0, i \neq j$
- independence of predictor(s) and residuals  
  $cov(x,\varepsilon) = 0$

If these are true, then the model is the *best linear unbiased estimator* (BLUE).