# Simple linear regression

Regression analysis is a statistical procedure that allows us to model relationships between variables so that the causal relationship between variables is defined. It can be considered as the main statistical technique used in economics, i.e. econometrics.  There is a large variety of regression models, depending on estimation method, model specification and assumed distributions. This section introduces the most basic of these, the simple linear regression model, i.e. ordinary least squares (OLS) with one independent variable.

See @navarro_learning_2018 section 12.3 for an introduction.

## Ordinary least squares

### Model specification

Simple linear regression model has one predictor and its model is mathematically expressed as follows:

$$y = \alpha + \beta x + \varepsilon,$$

where

- $y$ is dependent or explained variable, **response** or regressand, 
- $\alpha$ is the intercept or constant, 
- $\beta$ is a coefficient of $x$, 
- $x$ is independent or explanatory variable, **predictor** or regressor, and
- $\varepsilon$ is the model error.

### Calculation

The underlying idea behind (ordinary) least squares regression is the minimization of (squared) residuals. Model parameters are calculated (unlike maximum likelihood estimation based on iterations). To estimate the model $Y = \alpha + \beta x + \varepsilon$ we estimate the parameters $\hat{\beta}$ and $\hat{\alpha}$ as follows:

$\hat{\beta} = \frac{\sum{x_{i} y_{i}} - \frac{1}{n} \sum{x_{i}}\sum{y_{i}}}{\sum{x_{i}^{2}} - \frac{1}{n} (\sum{x_{i}})^{2}} = \frac{Cov[x,y]} {Var[x]}$

$\hat{\alpha} = \overline{y} - \beta \overline{x}$

For a simple model $y = \beta x + \varepsilon$ we can simply use matrix algebra on values of $x$ and $y$ to find $\hat{\beta}$:

$$\hat{\beta} = (X^{T} X)^{-1} X^{T} Y,$$

where $X$ is the matrix of predictor and $Y$ the matrix of response.

See @navarro_learning_2018 section 12.4 for illustrations of the idea.

In Jamovi: `Regression > Linear regression     `.  
In R: `lm(y ~ x, data)` or `summary(lm(y ~ x, data))`

## Elements of (OLS) regression models

### Intercept

In the equation this is the $\alpha$ and often referred to as the constant. Intercept is the value of $y$ where regression line crosses the Y-axis, so intercept is the value of $y$ when $x$ is zero ( $y|x=0$ ). Intercept does not need to be theoretically valid but it sometimes is. The statistical significance of the intercept is usually not relevant.

Interpretation of the intercept:

``` {r, echo = F}
calves <- agridat::urquhart.feedlot
wtMod <- lm(weight2 ~ weight1, calves)
```

$$weight2_{i} = `r round(wtMod$coef[[1]], 3)` + `r round(wtMod$coef[[2]], 3)` * weight1_{i} + \varepsilon_{i}$$

When `weight1` is 0 units, then the value of `weight2` is `r round(wtMod$coef[[1]], 3)` units.

### Coefficient(s)

In the equation the $\beta$ represents coefficient of $x$. It indicates y how many units $y$ increases when $x$ increases by one unit. 

We can not be sure if the coefficients are actually significant (when estimation is done on a sample). It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating t-statistic from coefficient and standard error. Coefficients are only relevant if their difference from 0 is statistically significant. 

#### Numeric predictors

Interpretation of coefficient(s):

``` {r, echo = F}
calves <- agridat::urquhart.feedlot
wtMod <- lm(weight2 ~ weight1, calves)
calves$diet <- factor(calves$diet, levels = c('Low', 'Medium', 'High'))
wtModCat <- lm(weight2 ~ diet, calves)
```

$$weight2_{i} = `r round(wtMod$coef[[1]], 3)` + `r round(wtMod$coef[[2]], 3)` * weight1_{i} + \varepsilon_{i}$$

When `weight1` increases by 1 unit, then `weight2` increases by **`r round(wtMod$coef[[2]], 3)`** units.

#### Categorical predictors

Interpretation of coefficient(s):

Suppose that a categorical variable called `diet` has the following levels: *`r levels(calves$diet)`*. Then in regression analysis the first level (*`r levels(calves$diet)[1]`*) is considered as the base level. All other factor values are compared against this base value.

$$weight2_{i} = `r round(wtModCat$coef[[1]], 3)` + `r round(wtModCat$coef[[2]], 3)`dietMedium_{i} + `r round(wtModCat$coef[[3]], 3)`dietHigh_{i} + \varepsilon_{i}$$

When `diet` is *`r levels(calves$diet)[1]`*, then `weight2` is **`r round(wtModCat$coef[[1]], 3)`**.

When `diet` is *Medium*, then `weight2` is higher by **`r abs(round(wtModCat$coef[[2]], 3))`** units compared to when `diet` is *`r levels(calves$diet)[1]`*, i.e. `r round(wtModCat$coef[[1]] + wtModCat$coef[[2]], 3)`.

Suppose now that the coefficient for `dietHigh` is not statistically significant. When `diet` is *High*, `weight2` is no different compared to when `diet` is *`r levels(calves$diet)[1]`*. If it was statistically significant, we could say that when `diet` is *High*, then `weight2` is higher by **`r abs(round(wtModCat$coef[[3]], 3))`** units compared to when `diet` is *`r levels(calves$diet)[1]`*.

### Fitted values

These are the values of $y$ calculated using the model for every $x$ in the data. In other words, fitted values are predictions.

### Residuals

Residuals are model errors, represented by the $\varepsilon$ in the equation. Residuals are the difference in response between observed and fitted (model predicted) values. We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good.

### The $R^2$ 

This is a way to measure *goodness of fit*, i.e. how well model fits data. $R^2$ indicates the part of variation in response variable that is explained by the model:

$$R^{2} = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS},$$

where the elements are defined a follows:

- **e**xplained sum of squares, $ESS$; 
  $\sum_{i = 1}^{n} (\hat{y}_{i} - \bar{y})^2$
- **r**esidual sum of squares, $RSS$; 
  $\sum_{i = 1}^{n} (y_{i} - \hat{y}_{i})^2$
- **t**otal sum of squares, $TSS$; 
  $\sum_{i = 1}^{n} (y_{i} - \bar{y})^2.$

Above

- $\hat{y}_{i}$ are fitted values, 
- $\overline{y}$ is the mean value, and
- $y_{i}$ are the actual values of $y$.

Mathematically, the $R^2$ measures how much better is model at explaining the variance of $y$ compared to just the mean.

### The adjusted- $R^2$ 

The more variables we add, the more the model explains. So $R^2$ can be inflated just by adding variables. To penalize a model for the number of predictors ( $k$ ) while considering the number of observations ( $n$ ), the adjusted $R^{2}$ can also be used, particularly for model comparison:

$$\overline{R^{2}} = 1 - \frac{RSS/(n-k)}{TSS/(n-1)}$$

## Assumptions and diagnostics

After the estimation of a regression model it should be diagnosed to make sure that it meets at least the following assumptions:

- Residuals are normally distributed
- Residuals have equal variance, i.e. variance of residuals does not depend on the value of $x$

In Jamovi: `Regression > Linear regression > Assumption Checks    `.  
In R: `plot(lm(formula, data))`

See @navarro_learning_2018 section 12.9 for an explanation of assumptions and section 12.10 on model checking.

### Gauss-Markov assumptions

In addition to the practical considerations outlined above, a  theoretical way of expressing the assumptions of OLS is via the Gaussâ€“Markov theorem. It posits the following assumptions:

- linear in parameters  
  $Y = \alpha + \beta x + \varepsilon$
- expected error is zero  
  $E(\varepsilon) = 0$
- homoscedasticity  
  $var(\varepsilon) = E(\varepsilon^{2})$
- no autocorrelation  
  $cov(\varepsilon_{i}, \varepsilon_{j}) = 0, i \neq j$
- independence of predictor(s) and residuals  
  $cov(x,\varepsilon) = 0$

If these are true, then the model is the *best linear unbiased estimator* (BLUE).