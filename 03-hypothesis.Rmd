# Hypothesis testing

Statistical hypothesis testing is a traditional procedure of determining the statistical significance of a given difference. More precisely, it allows to demonstrate whether some difference observed in data can be generalized. As such, it is the main method in inferential statistics. 

## Sample and population

Distinguishing between sample and population is important when we wish to draw inferences about population when we only have data on a sample. 

### Sample, population, sampling

See @navarro_learning_2018 section 8.1.

When we have data on the entire population we can simply describe it using descriptive statistics. But often we only have a part of the population, i.e. a sample. Since we only have data on sample, we can't be sure if same properties hold for population. Hence, we need to make inferences about the population by estimating population parameters from random sample data. This sample must be taken from the population randomly. For this, every value in the population must have an equal chance of being in sample. We can make inferences with a particular certainty, i.e. confidence interval or error ratio. 

### Estimating population parameters

#### Population mean

See @navarro_learning_2018 section 8.4.1.

Our best estimate of population mean is sample mean, i.e. 

$$\hat{\mu} = \bar{x}.$$

#### Population standard deviation

See @navarro_learning_2018 section 8.4.2.

Unlike mean, population standard error estimate needs to be corrected for bias as follows:

$$\hat{\sigma} = \sqrt{\frac{1}{n-1} \sum{^n_{i=1}{(x_i-\bar{x})^2}}}.$$

### Confidence intervals

See @navarro_learning_2018 section 8.5

A 95% confidence interval (CI) of the mean $X \sim N(\mu,\sigma^2)$ is calculated as

$$CI_{95} = \hat{x} \pm(1.96\times\frac{\sigma}{\sqrt{N}})$$

CI indicates that if we take many samples from the population, then in case of 95% of these samples the means will lie between the intervals. There is no intuitive interpretation. 

CI is **not** the 95% probility that the true value lies between the intervals.

## Hypothesis testing

### Null and alternative hypotheses

See @navarro_learning_2018 section 9.1.2.

Statistical hypothesis testing involves two hypotheses: null ($H_0$) and alternative ($H_1$) hypothesis. These are usually defined as follows:

$H_0$: There **is no** statistically significant difference.  
$H_1$: There **is** a statistically significant difference.

We always test if we can reject $H_0$. If we reject $H_0$, then we accept $H_1$. If we fail to reject $H_0$, then we accept $H_1$.

### Types of errors

See @navarro_learning_2018 section 9.2.

|                | We accept $H_0$             | We reject $H_0$            |
| -------------- | --------------------------- | -------------------------- |
| $H_0$ is true  | We are correct              | We commit **type I error** |
| $H_0$ is false | We commit **type II error** | We are correct             |

The rate of type I error is considered the significance level of a test (denoted as $\alpha$). 

### Test statistic

See @navarro_learning_2018 sections 9.3 and 10.1.6.

We use a test statistic to make a decision whether we can or can not reject $H_0$. We reject $H_0$ if the value of a test statistic is sufficiently extreme, i.e. different from 0. 

### P-value

See @navarro_learning_2018 sections 9.5 and 9.6.

P-value is the probability of obtaining at least as extreme value of test statistic if $H_0$ is correct. Thus, it's the rate of committing a type I error, i.e. the significance level $\alpha$. 

If the p-value is below $\alpha$, we reject $H_0$ and accept $H_1$. 

$p \ge \alpha \implies H_0$  
$p \lt \alpha \implies H_1$

P-value is **not** the probability of $H_0$ being false or $H_{1}$ being true. Theoretically, a lower p-value does **not** show a greater difference or vice versa. Only it's location relative to is what $\alpha$ matters.

### Multiple comparisons problem

Every time you find a statistically significant result, there is a possibility that you got this extreme data by chance. In this case, you committed a type 1 error. Therefore, if you do a lot of tests, the possibility that at least one of the statistically significant results is actually not true. Thus, the number of tests you do should be limited.

### Courtroom analogy

The procedure of hypothesis testing might be confusing and not very intuitive at first. In such case it might help to think about hypothesis testing as a trial with the aim of establishing if there is enough evidence to convict a person. The hypotheses would be the following:

$H_0$: Person is innocent,  
$H_1$: Person is guilty.

Person is innocent until proven guilty. Thus, we begin with assuming that $H_0$ is true (there is no difference) and make a conclusion after analyzing the evidence (data):

- if there is enough evidence (data) that demonstrates the crime (difference), we can reject $H_0$ and must accept $H_1$ that the person is guilty (there is a difference); 
- if there is not enough evidence (data) to demonstrate the crime (difference), we can not reject $H_0$ and must assume that the person is innocent (there is no difference). 

Note that in the latter case we are still not certain if the person is actually innocent (there is no difference), we might just not have enough evidence (data) to prove it. Similarly, accepting $H_0$ in hypothesis testing does not show that there is no difference but only that our data did not indicate a difference.

The analogy also applies to errors as well. A type I error would be convicting an innocent person while a type II error would be letting a guilty person free.
