# Factor analysis

If we think that there might be some underlying variables behind the variables that we observed, we can use Pactor Analysis (FA) to discover these. Procedurally FA is essentially PCA with an extra step that allows us to better separate variables. Here we will only be covering exploratory FA. 

See @crawley_r_2013 section 25.2 and @navarro_learning_2018 section 15.1 (and @hastie_elements_2017 sectin 14.7.1). 

## FA and PCA

The extra step that distinguishes FA from PCA is the rotation of principal axes. This is done in order to obtain more meaningful factors (equivalent to PCs in PCA). Thus, the factors are not maximizing variance but instead describing some underlying variance in data. As such, factors in FA are the underlying **latent variables** that are loaded by the observed variables that we have. A common example are psychological traits: we can not measure these directly but we can ask subjects various questions and then use factor analysis to summarize the responses into factors that are then traits. This leads to some distinct differences between FA and PCA:

- PCA aims to explain total variance while FA aims to explain covariances.
- PCs are uniquely defined while factors depend on rotation method

How do we decide which method to use? If our goal would be to summarize variables into a smaller set of variables, then we should use PCA. If the goal is to determine some underlying variables, then we should use FA.

## Orthogonal factor model

In PCA each PC was a linear function of variables but in FA the variables are expressed as linear combinations of factors. Because of this, the model is also be defined differently:

$$x_j = q_{jl} F_l + U_j +\mu_j,$$

where 

- $x_j$ is a variable $j$, 
- $q_{jl}$ the loading of the $j$th variable on the $l$th common factor, 
- $F_l$ the $l$th common factor, 
- $U_j$ the $j$th specific factor and 
- $\mu_j$ the mean of $x_j$. 

## Estimating the model

There are three methods for extracting factors. We are not going to cover the differences but these are:

- minimum residuals, 
- maximum likelihood, and
- principal axis method.

In Jamovi: `Factor > Exploratory Factor Analysis`.  

## Rotations

To discover factors that best separate our variables, we need apply a rotation on PCs. There are three commonly used methods for doing this.

### Varimax

This rotation involves maximizing variances of squared loadings within factors. Factors remain orthogonal, thus uncorrelated.

In Jamovi: `Factor > Exploratory Factor Analysis | Method | Rotation: Varimax `.  

### Oblimin

This rotation also involves maximization of squared loading variances within factors. The difference from Varimax rotation is that factors are not necessarily orthogonal but oblique, thus can be correlated. The resulting factors may be more similar to each other than in case of Varimax but the variances of loadings within factors are likely to be higher.

In Jamovi: `Factor > Exploratory Factor Analysis | Method | Rotation: Oblimin `.  

### Quartimax

This rotation aims to maximize variances of factor loadings within variables (not factors). The result is a factor structure in which each variable loads as few factors as possible.

In Jamovi: `Factor > Exploratory Factor Analysis | Method | Rotation: Quartimax `.  

## Assumptions

While PCA has essentially no assumptions, FA has a few. These can be described and tested for as explained below.

### Sphericity

Variables need to be correleated to a certain extent for us to able to summarize them into a smaller set of factors. Whether or not correlation matrix is statistically significantly different from zero can be tested with Bartlett's test.

$H_0:$ Correlation matrix is zero
$H_1:$ Correlation matrix is different from zero

If the test returns $p \ge 0.05$, then we should not trust the results of factor analysis.

In Jamovi: `Factor > Exploratory Factor Analysis | Assumption Checks | Bartlett's test of sphericity `.  

### Sampling adequacy

In addition to correlations it is necessary that each factor is not related to only few variables. We can use Kaiser-Meyer-Olkin (KMO) measure to determine if sum of partial correlations is higher than sum of correlations or not. The value of the measure is interpreted as follows regarding the suitability of FA.

- 0 ... 0.6 - Not suitable
- 6... 0.7 - Suitable
- 0.7 ... 0.9 - Adequate
- 0.9... 1 - Excellent

In Jamovi: `Factor > Exploratory Factor Analysis | Assumption Checks | KMO measure of sampling adequacy `.  

## Number of components

The methods that can be used to determine an optimal number of components are the same as those for PCA. For FA a further consideration can be the factor loadings or uniqueness. If some variable has a very high uniqueness, adding factors might increase the respective communality.

In Jamovi: `Factor > Exploratory Factor Analysis | Number of Components`.

## Interpretation

### Loadings

The interpretation of loadings in FA is similar to PCA. For FA only loadings that are higher than 0.5 should be considered and used for interpretation of factors.

In Jamovi: `Factor > Exploratory Factor Analysis | Factor Loadings | Hide loadings below: 0.5`.

### Uniqueness and communality

Uniqueness is a measure of proportion of variation explained by a particular variable and not by factors. Communality is the inverse of this ($1-Uniquenss$), i.e. variation of a variable that is shared with other variables.

### Complexity

This can be calculated for each variable. It expresses the number of factors on which a variable has a substantial loading.

## Controversies

FA is a controversial method in statistics. This is due to the different options for choosing the number of factors, methods of estimating the model, rotation techniques and subjectivity of interpreting factors.
