# Factor analysis

If we think that there might be some underlying variables behind the variables that we observed, we can use Pactor Analysis (FA) to discover these. Here we will only be covering exploratory FA. 

See @crawley_r_2013 section 25.2 and @navarro_learning_2018 section 15.1 (and @hastie_elements_2017 section 14.7.1). 

## FA and PCA

Both FA and PCA are variation maximization techniques. In PCA we are attempting to maximize the total variation in data. In factor analysis we instead try to maximize some underlying shared variation between variables. As such, factors in FA are the underlying **latent variables** that load the observed variables that we have. Factors are equivalent to principal components (PCs) in PCA. A common example are personality traits: we can not measure these directly but we can ask subjects various questions and then use factor analysis to summarize the responses into factors that represent these traits. The distinction leads to some differences between FA and PCA:

- PCA aims to explain total variance while FA aims to explain covariances.
- PCs are uniquely defined while factors depend on estimation and rotation method
- In PCA each PC is a linear combination of variables but in FA the variables are expressed as linear combinations of factors.

How do we decide which method to use? If our goal would be to summarize variables into a smaller set of variables, then we should use PCA. If the goal is to determine some underlying variables, then we should use FA.

## Orthogonal factor model

The factor model is defined as

$$x_j = q_{jl} F_l + U_j +\mu_j,$$

where 

- $x_j$ is a variable $j$, 
- $q_{jl}$ the loading of the $j$th variable on the $l$th common factor, 
- $F_l$ the $l$th common factor, 
- $U_j$ the $j$th specific factor and 
- $\mu_j$ the mean of $x_j$. 

There are three methods for extracting factors. We are not going to cover the differences but these are:

- minimum residuals, 
- maximum likelihood, and
- principal axis method.

In Jamovi: `Factor > Exploratory Factor Analysis`.  

## Rotations

To better define factors that best separate our variables, we can apply a rotation on factors. This results in an adjusted loadings matrix. There are several techniques for rotation. Of these the three most common are described below.

### Varimax

This rotation involves maximizing variances of squared loadings within factors. Factors remain orthogonal, thus uncorrelated.

In Jamovi: `Factor > Exploratory Factor Analysis | Method | Rotation: Varimax `.  

### Oblimin

This rotation also involves maximization of squared loading variances within factors. The difference from Varimax rotation is that factors are not necessarily orthogonal but oblique, thus can be correlated. The resulting factors may be more similar to each other than in case of Varimax but the variances of loadings within factors are likely to be higher.

In Jamovi: `Factor > Exploratory Factor Analysis | Method | Rotation: Oblimin `.  

### Quartimax

This rotation aims to maximize variances of factor loadings within variables (not factors). The result is a factor structure in which each variable loads as few factors as possible.

In Jamovi: `Factor > Exploratory Factor Analysis | Method | Rotation: Quartimax `.  

## Assumptions

While PCA has essentially no assumptions, FA has a few. These can be described and tested for as explained below.

### Sphericity

Variables need to be correleated to a certain extent for us to able to summarize them into a smaller set of factors. Whether or not correlation matrix is statistically significantly different from zero can be tested with Bartlett's test.

$H_0:$ Correlation matrix is zero
$H_1:$ Correlation matrix is different from zero

If the test returns $p \ge 0.05$, then we should not trust the results of factor analysis.

In Jamovi: `Factor > Exploratory Factor Analysis | Assumption Checks | Bartlett's test of sphericity `.  

### Sampling adequacy

In addition to correlations it is necessary that each factor is not related to only few variables. We can use Kaiser-Meyer-Olkin (KMO) measure to determine if sum of partial correlations is higher than sum of correlations or not. The value of the measure is interpreted as follows regarding the suitability of FA.

- 0 ... 0.6 - Not suitable
- 6... 0.7 - Suitable
- 0.7 ... 0.9 - Adequate
- 0.9... 1 - Excellent

In Jamovi: `Factor > Exploratory Factor Analysis | Assumption Checks | KMO measure of sampling adequacy `.  

## Number of components

The methods that can be used to determine an optimal number of components are the same as those for PCA. For FA a further consideration can be the factor loadings or uniqueness. If some variable has a very high uniqueness, adding factors might increase the respective communality.

In Jamovi: `Factor > Exploratory Factor Analysis | Number of Components`.

## Interpretation

### Loadings

The interpretation of loadings in FA is similar to PCA. For FA only loadings that are higher than 0.3 or 0.5 should be considered and used for interpretation of factors.

In Jamovi: `Factor > Exploratory Factor Analysis | Factor Loadings | Hide loadings below: 0.5`.

### Uniqueness and communality

Uniqueness is a measure of proportion of variation explained by a particular variable and not by factors. Communality is the inverse of this ($1-Uniquenss$), i.e. the variation of a variable that is shared with other variables.

### Complexity

This can be calculated for each variable. It expresses the number of factors which load the variable.

---

FA is a controversial method in statistics because we can never be sure if the factors actually exist and were correctly estimated. This is due to the different options for choosing the number of factors, methods of estimating the model, rotation techniques and subjectivity of interpreting factors.
