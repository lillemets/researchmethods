---
title: Principal component analysis
subtitle: Research methods
author: Jüri Lillemets
date: "`r Sys.Date()`"
output_yaml: _output.yml
knit: rmarkdown::render
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, dpi = 200, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
# Options
options(scipen = F, digits = 3)
# Set colors
source('/home/jrl/work/resmeth/slides/variables.R')
```

class: center middle clean

# How to summarize variables?

---

class: center middle inverse

# Intuition

---

## Multivariate statistics

In general, we do not test any hypotheses nor estimate statistical significance.

There are no response or predictor variables. 

Response variable is not predicted from a model.

Instead we examine the overall **structure of data to find patterns**.

*Principal component analysis* (*PCA*) is one of the methods in the field of multivariate statistics. 

---

## Principal component analysis

The aim is to **summarize continuous variables** into one, few or several new variables.

This is done by determining a set of standardized **linear combinations** that would explain a maximum amount of variation in data.

A **dimensionality reduction** technique.

The maximum number of PCs we can determine is $min(p, n - 1)$. 

---

Let's use data on top performances in the *Decathlon* from 1985 to 2006 and examine results for each event in 2000.

``` {r}
Decathlon <- GDAdata::Decathlon
Decathlon <- Decathlon[Decathlon$yearEvent == 2000, ]
Oie <- Decathlon[, 4:13]
rownames(Oie) <- Decathlon$DecathleteName
```

--

Below are the first rows in the data set.

.smaller[
``` {r}
head(Oie, 10) %>% kable
```
]

--

Why would we want to summarize these variables?

---

Variables are linearly correlated.

.smaller[
``` {r}
cor(Oie) %>% kable
```
]

---

``` {r}
par(family = 'RobotoCondensed')
pairs(Oie, pch = 20, cex = .2, col = Col['clear'], oma = c(2,2,2,2), )
```

--

> How could we summarise all of this variation into a single score?

---

### Linear combinations

Each linear combination is called a principal component (PC). A $j$th PC can be expressed as linear combination $\xi_j$ as:

$$\xi_j = b_{j1}x_1 + b_{j2}x_2+ ... a_{jp}x_p,$$ 

where $b_{jp}$ is a weight of variable $x_p$ of a particular $\xi_j$. 

.smaller[
``` {r}
head(Oie) %>% kable
```
]

???
PCs are thus essentially weighted sum of variables.
We need to give each sports event some weight. 

---

## Estimation of PCs

We can think of the estimation as a process starting from estimation of first PC.

The first PC is chosen so that

1. it follows the direction of largest variance in data cloud and
2. it is a line with smallest orthogonal distance to all data points.

Each following PC is chosen so that it is orthogonal to preceding PC.

``` {r}
# Recode some variables
names(Oie) <- c('run100', 'longjump', 'shotput', 'highjump', 'run400', 'run110hurdle', 'polevault', 'discus', 'javelin', 'run1500')
Oie[, c(1,5,6,10)] <- Oie[, c(1,5,6,10)] * -1
```

---

Suppose we have two variables:

```{r, fig.height = 4.5}
vars <- Oie[, c('shotput', 'polevault')]
vars <- list(Raw = vars, 
             Center = vars %>% lapply(scale, scale = F) %>% data.frame, 
             Scale = vars %>% lapply(scale) %>% data.frame)
plotEst <- function(data, ...) {
  par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), pty = 's')
  plot(data[[2]] ~ data[[1]], xlab = "Pole vault", ylab = "Shot put",
       pch = 20, cex = 1, col = Col['clear'], ...)
}
plotEst(vars$Raw)
```

---

The line that maximizes variation and has smallest orthogonal distance to data points is the first PC.

``` {r}
par(mfrow = 1:2)
# Data cloud
plotEst(vars$Raw)
# PC1
plotEst(vars$Raw)
eigHiLo <- cov(vars$Cen) %>% eigen
lines(vars$Raw[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Cen[[1]] + mean(vars$Raw[[2]])), 
      col = Col['red'])
```

--

> What does this line remind you of?

---

PCA is somewhat similar to least squares estimation.

```{r}
par(mfrow = 1:2)
# OLS
plotEst(vars$Raw, main = "OLS")
lmHiLo <- lm(vars$Raw[[2]] ~ vars$Raw[[1]])
abline(lmHiLo, col = Col['blue'])
# PC1
plotEst(vars$Raw, main = "PC1")
eigHiLo <- cov(vars$Cen) %>% eigen
lines(vars$Raw[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Cen[[1]] + mean(vars$Raw[[2]])), 
      col = Col['red'])
```

OLS minimizes residuals with respect to response, PC1 minimizes orthogonal distances to data points.

???
Except that in PCA we consider variation in all variables (not just the response)

---

Each following PC is orthogonal to the first PC.

``` {r}
par(mfrow = 1:2)
# PC1
plotEst(vars$Scale)
eigHiLo <- cov(vars$Scale) %>% eigen
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['red'])
# PC1 and PC2
plotEst(vars$Scale)
eigHiLo <- cov(vars$Scale) %>% eigen
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['red'])
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['blue'])
# Legend
pcLeg <- function() {
  legend('topleft', legend = c('PC1', 'PC2'), col = Col[c('red', 'blue')],
         lwd = 1, border = NA, box.col = NA)
}
pcLeg()
```

Each PC explains the variation unexplained by previous PC as much as possible.

---

The process involves transforming data points to a new coordinate system: 

1. centering data points, 
2. scaling the axes so that they would be equal, 
3. rotating axes into the direction of a PC.

---

### Center

``` {r}
par(mfrow = 1:2)
# Raw
plotEst(vars$Raw, main = "Raw data")
eigHiLo <- cov(vars$Cen) %>% eigen
lines(vars$Raw[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Cen[[1]] + mean(vars$Raw[[2]])), 
      col = Col['red'])
lines(vars$Raw[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Cen[[1]] + mean(vars$Raw[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
# Center
plotEst(vars$Cen, main = "Center")
eigHiLo <- cov(vars$Cen) %>% eigen
lines(vars$Cen[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Cen[[1]] + mean(vars$Cen[[2]])), 
      col = Col['red'])
lines(vars$Cen[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Cen[[1]] + mean(vars$Cen[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
```

We subtract the mean from all variables so that mean value of each PC is 0.

---

### Scale

``` {r}
par(mfrow = 1:2)
# Center
plotEst(vars$Cen, main = "Center")
eigHiLo <- cov(vars$Cen) %>% eigen
lines(vars$Cen[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Cen[[1]] + mean(vars$Cen[[2]])), 
      col = Col['red'])
lines(vars$Cen[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Cen[[1]] + mean(vars$Cen[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
# Scale
plotEst(vars$Scale, main = "Scale")
eigHiLo <- cov(vars$Scale) %>% eigen
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['red'])
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
```

We divide each variable by its standard deviation so that PCs have similar scales relative to variables.

---

### Rotate

``` {r}
par(mfrow = 1:2)
# Scale
plotEst(vars$Scale, main = "Scale")
eigHiLo <- cov(vars$Scale) %>% eigen
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['red'])
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
# Rotate
pcaHiLo <- prcomp(vars$Raw, scale = T) %>% summary
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), pty = 's')
plotEst(as.data.frame.matrix(pcaHiLo$x), main = "Rotate")
abline(h = 0, col = Col['red'])
abline(v = 0, col = Col['blue'])
pcLeg() # Legend
```

We rotate the data so that each PC represents an axis.

``` {r, eval = F}
# All in one
par(mfrow = c(1,4))
# Raw
plotEst(vars$Raw, main = "Raw data")
eigHiLo <- cov(vars$Cen) %>% eigen
lines(vars$Raw[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Cen[[1]] + mean(vars$Raw[[2]])), 
      col = Col['red'])
lines(vars$Raw[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Cen[[1]] + mean(vars$Raw[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
# Center
plotEst(vars$Cen, main = "Center")
eigHiLo <- cov(vars$Cen) %>% eigen
lines(vars$Cen[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Cen[[1]] + mean(vars$Cen[[2]])), 
      col = Col['red'])
lines(vars$Cen[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Cen[[1]] + mean(vars$Cen[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
# Scale
plotEst(vars$Scale, main = "Scale")
eigHiLo <- cov(vars$Scale) %>% eigen
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['red'])
lines(vars$Scale[[1]], 
      (eigHiLo$vec[2,2]/eigHiLo$vec[1,2] * vars$Scale[[1]] + mean(vars$Scale[[2]])), 
      col = Col['blue'])
pcLeg() # Legend
# Rotate
pcaHiLo <- prcomp(vars$Raw, scale = T) %>% summary
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), pty = 's')
plotEst(as.data.frame.matrix(pcaHiLo$x), main = "Rotate")
abline(h = 0, col = Col['red'])
abline(v = 0, col = Col['blue'])
pcLeg() # Legend
```

---

How do we know how exactly to scale and rotate axes? 

We use some parameters derived from data:

- for scaling we use *eigenvalues* and 
- for rotating we use *eigenvectors*.

---

## Calculation

There are two methods for calculating eigenvalues and eigenvectors from data, depending on whether we use 

1. covariance or correlation matrix $\Sigma$ of data matrix $X : n \times p$ or
2. the raw data matrix $X : n \times p$.

---

### Spectral decomposition

This calculation uses correlation or covariance matrix $\Sigma$.

Because $\Sigma = V D V^T$, we can find eigenvalues from $D$ and corresponding eigenvectors from $V$.

Computationally more simple.

---

### Singular value decomposition

This calculation uses data matrix with raw values $X : n \times p$.

Because $X = UDV^T$, we can find eigenvalues from $D$ and corresponding eigenvectors from $U$.

Numerically more stable.

---

Because PCA involves maximizing variation, variables with higher variance are given more importance. Consider scaling variables ( $x\prime = (x - \mu) / \sigma$ ) prior to estimation.

``` {r}
# Example of scales
par(bty = 'n', family = 'RobotoCondensed', mar = c(6,4,1,0), mfrow = c(1,3), cex = .8)
boxplot(Oie, main = "Raw data", las = 2)
boxplot(lapply(Oie, function(x) x - mean(x)), main = "Normalized data", las = 2)
boxplot(scale(Oie), main = "Scaled data", las = 2)
```

---

## Number of PCs

Aside from theoretical considerations, there also several possible criteria for determining the number of estimated PCs:

1. choose a threshold of variation in data that PCs should cumulatively explain, e.g. 50%; 
2. use PCs that have eigenvalue above average (one); 
3. use PCs that are above an "elbow" in scree plot.

---

The choice can be made by looking at scree plots.

``` {r}
pcDec <- prcomp(Oie, scale = T)
```

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mfrow = c(1,3))
plot(pcDec$sdev^2 * 10, type = 'b', 
     xlab = "Principal component", ylab = "Variation explained, %")
plot(cumsum(pcDec$sdev^2 * 10), type = 'b', 
     xlab = "Principal component", ylab = "Cumulative variation explained, %")
plot(pcDec$sdev^2, type = 'p', 
     xlab = "Principal component", ylab = "Eigenvalue")
abline(h = 1)
```

---

class: center middle inverse

# Interpretation

---

## Variation explained

The proportion of total *variation explained* by a $j$th PC can be found from its eigenvalues: it is the proportion of sum of eigenvalues, i.e. $\lambda_j / \Sigma^p_{j=1} \lambda$.

Total variation explained by PCs illustrates how much information is preserved when summarizing data into a particular number of PCs.

--

``` {r}
kable(t(cbind(`Variation explained, %` = pcDec$sdev^2 * 10, 
              `Cumulative variation, %` = cumsum(pcDec$sdev^2 * 10))), 
      col.names = attributes(pcDec$x)$dimnames[[2]],
      digits = 0)
```

--

> How much of total variation in data do the first two PCs explain? 

---

## Loadings

Variables "load" PCs and thus each variable has a particular *loading* on each PC. These can be understood as *weights*.

.smaller[
``` {r}
kable(pcDec$rotation)
```
]

--

> How would you describe these PCs?

???
Loadings can be used to give a theoretical meaning to each PC. This is creative.

---

### Scores

Principal component scores are the original variables weighted by PCs.

Recall the representation of linear combinations $\xi_j = b_{j1}x_1 + b_{j2}x_2+ ... a_{jp}x_p$.

.smaller[
``` {r}
head(pcDec$x, 10) %>% kable
```
]

---

## Biplot

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,2,4))
biplot(pcDec, xlabs = rep('·', nrow(Oie)))
```

--

> How would you interpret these first two PCs?

---

How are our results related to actual points given to athletes?

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), mfrow = c(1,3))
for (i in 1:3) {
  plot(pcDec$x[, i], Decathlon$Totalpoints, 
       xlab = paste0("PC", i), ylab = "Points awarded", 
       pch = 20, cex = 1, col = Col['clear'])
}
```

???
It seems like technical skills are given higher importance. Methodology for total points calculation is from 1985.

---

class: center middle inverse

# Practical application

---

Use the data set `UN98`.

> Are variables linearly correlated?

> Summarize the social indicators into a single index. What are the weights of these variables?

> Use the index to describe social situation in world regins.

---

class: inverse
