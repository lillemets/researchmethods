[["index.html", "Research methods Section 1 Syllabus 1.1 Teaching 1.2 Scoring 1.3 Topics", " Research methods Section 1 Syllabus These course notes include information and study materials on the quantitative part of MS.0825 Research methods course of Agri-Food Business Management Master’s program. 1.1 Teaching Flipped classroom approach is used for teaching. This means that students are expected to learn the methods before meetings. During the meetings we revise the theoretical material, address any questions and apply methods in practice. After meetings students apply the methods on their own datasets. 1.1.1 Schedule Meetings take place on Mondays, Wednesdays and Thursdays at 9 o’clock in the morning. Link to meetings is provided under the course page in Moodle. 1.1.2 Aims After completing the course students should be able to … … understand basic concepts in statistics; … know how to describe data, both numerically and visually; … choose an appropriate method to solve a problem; … use a statistical package for data analysis; … communicate results (interpret, explain, present); … learn about statistical methods individually; … find the courage to apply statistical methods. 1.2 Scoring This part of the course gives 50% of total course points. This 50% consists of 50 points that can be obtained by submitting: feedback on reading material for upcoming meeting (10*1=10 points), and report containing the application of methods learned (10*4=40 points). To gain the 40 points students are required to combine the applications of a method into a report. Participation in meetings does not affect points but meetings are not recorded. 1.2.1 Feedback on reading material Submit your questions or thoughts on given reading material before each meeting in Moodle. Feedback can be either an explanation of something you learned, a question on something you don’t understand, or an answer to another student’s question. Compulsory reading is from two books: Navarro and Foxcroft (2018) and Crawley (2013). Each part of feedback is graded as follows: 0 points - Feedback is missing or lacking 1 point - Feedback demonstrates that student has revised relevant material 1.2.2 Research project Students are required to create a research report as follows: Choose a data set Apply at least one method from each meeting on this data set Make changes according to feedback Present the results as a written report Submit the report before the end of May. Each method is graded as follows: 2 points 4 points 1.2.2.1 Datasets To apply methods, datasets need to have multiple variables. We mostly learn to explore continuous data but some methods require at least one discrete variable. Students are free to choose a dataset with the requriment that chosen data is different from what other students have chosen. Possible sources for data: Goodle dataset search Kaggle datasets Data hub collections 1.3 Topics We cover some common and traditional statistical methods. Descriptive statistics Hypothesis testing. Comparing categorical variables Comparing numerical variables Analysis of variance Correlation analysis Regression. Ordinary Least Squares Regression. Factors and transformations Regression. Maximum likelihood and binary models Principal component analysis Factor analysis Hierarchical clustering K-means clustering "],["introduction.html", "Section 2 Introduction 2.1 Quantitative methods 2.2 Statistics 2.3 Software 2.4 Data management", " Section 2 Introduction 2.1 Quantitative methods 2.1.1 Quantitative and qualitative methods QuaNTitative QuaLitative Numeric data Semantic data Large-N Small-N, case studies Generalize Explore Measure and test Understand Statistical analysis Interpretation 2.2 Statistics See Navarro and Foxcroft (2018) sections 1.1.1 and 1.5 for a brief introduction. Sections 2.3-2.7 also provide useful background on validity and reliability of statistical methods. Dash (2016) has summarized statistics as follows: Figure 2.1: Flowchart of statistics 2.2.1 Descriptive and inferential statistics Descriptive statistics Inferential statistics Data on entire population Only a sample of a population Simple measures (e.g. mean) Point estimates and intervals Describing Generalizing 2.2.2 Frequentist and bayesian approach to statistics Frequentist Bayesian Traditional Modern What’s the probability of data given some estimate? What’s the probability of an estimate? Prior is not relevant Estimates are conditional on priors Single parametric inference Multiplication of an inference to get a posterior estimate 2.2.3 Statistics and data science, machine learning, artificial intelligence, … Statistics DS, ML, AI, … Less data Big data Clean datasets Untidy data in various formats Traditonal methods Novel methods Mathematics and calculations Programming approach Aim is to explain Aim is to predict 2.3 Software 2.3.1 Commonly used software Spreadsheet vs statistical software Spreadsheet - table management (e.g. Microsoft Excel, Google Sheets, LibreOffice Calc) SPSS - simple GUI-based app with limited functionality R - extensible programming language designed for data analysis Stata - Mostly CLI-based, well suited for econometrics 2.3.2 Jamovi To apply the methods we learn, we use Jamovi. Jamovi is a new “3rd generation” statistical spreadsheet. designed from the ground up to be easy to use, jamovi is a compelling alternative to costly statistical products such as SPSS and SAS. Jamovi is built on top of the R statistical language, giving you access to the best the statistics community has to offer. You can try it online without installing at https://cloud.jamovi.org/. 2.3.3 R language and RStudio This allows us to get under the hood of Jamovi so to speak. R language is based on S language orginating from 1970’s. Developed during 1990’s and became public around 2000. R is a language and environment for statistical computing and graphics. R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, …) and graphical techniques, and is highly extensible. 2.4 Data management 2.4.1 Data structure Usually data is stored in 2-dimensonal tables, i.e. data has rows and columns. To apply statistical procedures, most software requires data to be formatted in a conventional way. In this respect it’s useful to follow “tidy data” principle (Wickham 2014): Every column is a variable. Every row is an observation. Every cell is a single value. This is how data should be formatted before running most statistical procedures. One way to tidy a data table is to use the Pivot table feature in a spreadsheet app. 2.4.2 Scales of measurement Scale of measurement determines the procedures that can be applied to the data. The following typology was first published by Stevens (1946). See Navarro and Foxcroft (2018) section 2.2 for a more detailed explanation. Each scale in the table includes also the properties, operations and measures a on previous rows. Scale Property Operations Central tendency Nominal Classification =, ≠ Mode Ordinal Level &gt;, &lt; Median Interval Difference +, − Arithmetic mean Ratio Magnitude ×, / Geometric mean, harmonic mean 2.4.2.1 Nominal scale Nominal variables contain names (i.e. characters, factors or strings) that do not have a natural order. Such data can be summarized only by counting values or determining the mode. 2.4.2.2 Ordinal scale Ordinal variables have the characteristics of nominal variables with the added possibility of naturally ordering these names. As such, ordinal variables can be said to have levels. It’s important to note that an increase of one level to the next is not necessarily numerically equivalent to an increase of another level to the next. Thus, it is not always meaningful to convert these levels to numbers and do calculations on them. 2.4.2.3 Interval scale Interval variables are expressed numerically. While differences between numbers on interval scale are meaningful, there isn’t a natural zero value. Thus, calculations on interval variables is limited to finding differences and division or multiplication of such values is not reasonable. A classic example of an interval variable is temperature. Difference between 5°C and 15°C is 10°C but 15°C is not 3 times warmer than 5°C. 2.4.2.4 Ratio scale Ratio variables are also numeric but have a natural zero value. This means that division and multiplication is meaningful. 2.4.2.5 Binary scale Binary (or dichotomous) variables do not constitute a distinct scale but can be considered as a subgroup of nominal, ordinal or interval variables. Binary variables can only take two values. Many statistical procedures convert or require the conversion of nominal variables to binary for technical reasons (e.g. “dummy” variables). Binary variables are often coded as 0 for false and 1 for true. As such, calculating the sum or mean of binary variables conveys useful information (think why that is!). 2.4.2.6 Continuous or discrete variables In addition to the previous typology, we can also distinguish between continuous and discrete variables. These are well defined by Navarro and Foxcroft (2018, 20): \"A continuous variable is one in which, for any two values that you can think of, it’s always logically possible to have another value in between. A discrete variable is, in effect, a variable that isn’t continuous. For a discrete variable it’s sometimes the case that there’s nothing in the middle.\" 2.4.2.7 Scales in statistical software The difference between ratio and interval variables is only theoretical. Statistical software treats all numbers as numeric variables and does not distinguish between interval and ratio variables. When binary variables are coded so that they only have values 0 and 1, these are also considered numerical. A lot of statistical software (e.g. Jamovi) distinguish only between three types of variables: nominal, ordinal and numeric. "],["descriptive-statistics.html", "Section 3 Descriptive statistics 3.1 Central tendency 3.2 Variability 3.3 Plots", " Section 3 Descriptive statistics Any data analysis should begin with an exploration of the structure of and values in dataset. Description of values should also be part of the reporting of analysis. This can be done using some simple methods. When we use the methods of descriptive statistics, we need to be aware of that we merely describe the dataset at hand. If observations in the data set constitute a sample, we cannot draw any wider conclusions on population using these methods. Below we are thus defining the statistics for populations and not samples. Description of nominal or ordinal variables is simple and limited to just counting unique values and finding the mode and/or median value. But suppose we have variables that contain sequences of numbers. How can we describe these? 3.1 Central tendency A concise way to represent numeric values is expressing the center of values. This can also be understood the typical value or expected value (expectation of \\(x\\), i.e. \\(E(x)\\)). Note that estimating a central tendency is only meaningful if there is a natural center. i.e. the distribution of values is unimodal. See Navarro and Foxcroft (2018) section 4.1. 3.1.1 Mode Mode is simply the most frequent value in a sequence. This statistic can be useful for nominal and ordinal variables but it’s rarely used for numeric variables, especially on a continuous scale. 3.1.2 Median When sequence of values is ordered, the middle value is median. In case there are even number of values (\\(N\\)) in a sequence, median is the average of the two values in the middle. Thus, there are equal number of values above and below the median value. When \\(N\\) is an even number, half of the values are above and half below the median value. 3.1.3 Mean While there are several forms of mean, arithmetic mean is most useful in statistics and also considered here. Mean is in other words the average value: it’s the sum of values (\\(X\\)) divided by number of values: \\[\\bar{X} = \\frac{1}{N} \\sum{^N_{i=1}}{X_i}\\] A feature of mean value is that if you only know the \\(\\bar{X}\\) and \\(N\\), you can still calculate \\(\\sum{^N_{i=1}}{X_i}\\). 3.2 Variability Central tendency describes values only partially as it tells nothing about the deviation of values from that mean. Estimating also the variability of values provides a more manifold understanding of values. See Navarro and Foxcroft (2018) section 4.2. 3.2.1 Range Most simple way to express variability is to indicate the range of all values, i.e. the minimum and maximum values. 3.2.2 Quantiles and interquartile range Quantiles are in essence calculated just like median, because median is the lowest quantile, the 2-quantile. In addition to dividing sequence of ordered values into two halves separated by median, such sequence can be diveded into any number of equal groups as long as the number of groups (\\(k\\)) is smaller than or equal to the number of values, i.e. \\(k \\le N\\). Some common quantiles are quartiles (\\(k = 4\\)), deciles (\\(k = 10\\)) and centiles/percentiles (\\(k = 100\\)). Arguably quartile is most frequently used among quantiles because the 2nd and 4th quartiles contain half of all values. This interquartile range (IQR) thus contains the middle half of all values. 3.2.3 Variance Although difficult to interpret, variance is used in many statistical calculations due to its mathematical properties. Variance is mean squared difference from the mean value: \\[Var(X) = \\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}\\] 3.2.4 Standard deviation Interpretability issues of variance arise from the fact that the calculation includes squaring and thus the value of the statistic is vastly different to understand regarding initial scale. A solution would be to find a square root of variance. This is what the standard deviation is: the square root of variance. \\[\\sigma = \\sqrt{\\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}}\\] 3.3 Plots Previously explained measures allow us to only summarize values into fewer numbers. Plotting allows to present these values more intuitively or even depict all of the values at once. 3.3.1 Boxplot Boxplots illustrate the quartiles of values (including interquartile range) and extreme values (outliers). Outliers are usually values that are below the 2nd quartile or above the 4th quartile by 1.5 times the interquartile range. On boxplots, half of all the values lie within the box and another half on the lines, outliers excluded. See Navarro and Foxcroft (2018) section 5.2. 3.3.2 Histogram and density plot Histograms are essentially barplots where all values are divided between ranges of equal with, i.e. bins. Instead of bins, distribution of values can also be expressed continuously by smoothing the bins. This results in a density plot. See Navarro and Foxcroft (2018) section 5.1. "],["hypothesis-testing.html", "Section 4 Hypothesis testing 4.1 Sample and population 4.2 Hypothesis testing", " Section 4 Hypothesis testing 4.1 Sample and population Distinguishing between sample and population is important when we wish to draw inferences about population when we only have data on a sample. 4.1.1 Sample, population, sampling See Navarro and Foxcroft (2018) section 8.1. What is the difference between population, sample and (simple) random sample? 4.1.2 Estimating population parameters 4.1.2.1 Population mean See Navarro and Foxcroft (2018) section 8.4.1. How do you get population mean if you only know the sample mean? 4.1.2.2 Population standard deviation See Navarro and Foxcroft (2018) section 8.4.2. \\[\\hat{\\sigma} = \\sqrt{\\frac{1}{n-1} \\sum{^n_{i=1}{(x_i-\\bar{x})^2}}}\\] Why and how is sample standard deviation different from population standard deviation? 4.1.3 Confidence intervals See Navarro and Foxcroft (2018) section 8.5 A 95% CI of the mean \\(X \\sim N(\\mu,\\sigma^2)\\) is calculated as \\[CI_{95} = \\hat{x} \\pm(1.96\\times\\frac{\\sigma}{\\sqrt{N}})\\] What is the interpretation of confidence interval? 4.2 Hypothesis testing 4.2.1 Null and alternative hypotheses See Navarro and Foxcroft (2018) section 9.1.2. Statistical hypothesis testing involves two hypotheses: null (\\(H_0\\)) and alternative (\\(H_1\\)) hypothesis. These are usually defined as follows: \\(H_0\\): There is no statistically significant difference. \\(H_1\\): There is a statistically significant difference. We always test if we can reject \\(H_0\\). If we reject \\(H_0\\), then we accept \\(H_1\\). If we fail to reject \\(H_0\\), then we accept \\(H_1\\). 4.2.2 Types of errors See Navarro and Foxcroft (2018) section 9.2. We accept \\(H_0\\) We reject \\(H_0\\) \\(H_0\\) is true We are correct We commit type I error \\(H_0\\) is false We commit type II error We are correct The rate of type I error is considered the significance level of a test (denoted as \\(\\alpha\\)). 4.2.3 Test statistic See Navarro and Foxcroft (2018) sections 9.3 and 10.1.6. We use a test statistic to make a decision whether we can or can not reject \\(H_0\\). We reject \\(H_0\\) if the value of a test statistic is sufficiently extreme, i.e. different from 0. 4.2.4 P-value See Navarro and Foxcroft (2018) sections 9.5 and 9.6. P-value is the probability of obtaining at least as extreme value of test statistic if \\(H_0\\) is correct. Thus, it’s the rate of committing a type I error, i.e. the significance level \\(\\alpha\\). If the p-value is below \\(\\alpha\\), we reject \\(H_0\\) and accept \\(H_1\\). \\(p \\ge \\alpha \\implies H_0\\) \\(p \\lt \\alpha \\implies H_1\\) P-value is not the probability of \\(H_0\\) being false or \\(H_{1}\\) being true. Theoretically, a lower p-value does not show a greater difference or vice versa. Only it’s location relative to is what \\(\\alpha\\) matters. 4.2.5 Multiple comparisons problem Every time you find a statistically significant result, there is a possibility that you got this extreme data by chance. In this case, you committed a type 1 error. Therefore, if you do a lot of tests, the possibility that at least one of the statistically significant results is actually not true. Thus, the number of tests you do should be limited. "],["comparing-categorical-data.html", "Section 5 Comparing categorical data 5.1 Contingency tables 5.2 \\(\\chi^2\\)-test 5.3 Degrees of freedom", " Section 5 Comparing categorical data 5.1 Contingency tables For analyzing categorical data we can compare the counts of categories. This gives us contingency tables where cells indicate how many times each value appears in a categorical variable. When we compare more than one variable, contingency tables have more than one dimension. Usually we tabulate two variables against each other. If these variables have \\(m\\) and \\(n\\) unique values (categories), then we obtain a 2-dimensional \\(m*n\\) contingency table. 5.2 \\(\\chi^2\\)-test 5.2.1 Goodness of fit \\(\\chi^2\\)-test For goodness of fit \\(\\chi^2\\)-test see Navarro and Foxcroft (2018) section 10.1.3. We test if frequencies of categories are different from some expected frequencies for a single variable: \\(H_0\\): Frequencies of categories are as expected. \\(H_1\\): Frequencies of categories are different from what is expected. The test statistic is calculated as \\[\\chi^2 = \\sum{^k_{i=1}{\\frac{(O_i-E_i)^2}{E_i}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency of \\(i\\)th category (\\(k\\)). In Jamovi: Frequencies &gt; N Outcomes In R: chisq.test(x) 5.2.2 \\(\\chi^2\\)-test of independence For \\(\\chi^2\\)-test of independence see Navarro and Foxcroft (2018) section 10.2. In this case we test if two variables are associated: \\(H_0\\): Variables are independent. \\(H_1\\): Variables are associated. The test statistic is calculated as \\[\\chi^2 = \\sum{^r_{i=1}\\sum{^c_{j=1}}{\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency in \\(i\\)th row (\\(r\\)) and \\(j\\)th column (\\(c\\)). An assumption of \\(\\chi^2\\)-test is that there are \\(&gt;5\\) observations in each cell of a contingency table. In Jamovi: Frequencies &gt; Independent Samples In R: chisq.test(x, y) 5.3 Degrees of freedom See Navarro and Foxcroft (2018) section 10.1.5 and pages 227-228. Degrees of freedom (DoF) indicates the number of values that are free to vary. This is relevant for evaluating the \\(\\chi^2\\)-test statistic as it indicates the critical value on \\(\\chi^2\\)-distribution. For goodness of fit \\(\\chi^2\\)-test DOF is calculated simply as \\(df = k - 1\\) where \\(k\\) is the number of categories. For \\(\\chi^2\\)-test of independence DoF is calculated as \\[df = (r - 1)(c - 1),\\] where \\(r\\) is the number of rows and \\(c\\) the number of columns in a contingency table. "],["comparing-numerical-data.html", "Section 6 Comparing numerical data 6.1 One or two samples 6.2 Unpaired or paired samples 6.3 One-tailed or two-tailed tests 6.4 Parametric or nonparametric tests 6.5 How to decide which test to use?", " Section 6 Comparing numerical data Statistical testing for numerical data is different from when we have categorical data as we are no longer limited to using frequencies but parameters, ranks and other measures. However, note that nonparametric tests explained at the end can also be used for ordinal variables. This section covers only comparisons with one or two samples. Comparing multiple mean values is done using analysis of variance (Anova) and is considered in the next section. Also, this section only outlines some classic tests and there are many more. 6.1 One or two samples 6.1.1 One sample 6.1.1.1 One sample T-test This test is used to determine if our sample is taken from a population with a given mean. So we test if the sample mean \\(\\bar{x}\\) is equal to a hypothetical population mean \\(\\mu\\). See Navarro and Foxcroft (2018) section 11.2. Hypotheses: \\(H_0: \\bar{x} = \\mu\\) \\(H_1: \\bar{x} \\neq \\mu\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x} - \\mu}{s \\div \\sqrt{n}},\\] where \\(\\bar{x}\\) is the sample mean and \\(\\mu\\) the hypothetical population mean that it is tested against and \\(s\\) sample standard deviation. Test statistic is evaluated on t-distribution with \\(n-1\\) degrees of freedom (df). Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.2.3. In Jamovi: T-tests &gt; One Sample T-test. In R: t.test(x, mu) 6.1.2 Two samples 6.1.2.1 Independent samples T-test assuming equal variances (Student test) This test compares the mean values to determine if these are equal. Equality of means implies that samples come from the same population. The test assumes that variances of the samples are equal. This is true if \\(\\frac{1}{2} &lt; \\frac{s_1}{s_2}&lt;2\\). See Navarro and Foxcroft (2018) section 11.3. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = s_p\\sqrt{\\frac{1}{n_1} +{\\frac{1}{n_2}}},\\] where \\(n_1\\) and \\(n_2\\) are sample sizes and \\(s_p\\) is the pooled standard deviation calculated as \\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}},\\] where \\(s_1\\) and \\(s_2\\) are standard deviations of the samples. Test statistic is evaluated on t-distribution with \\(n_1+n_2-2\\) df. Test assumes normality and independence of data and homogeneity of variance. See Navarro and Foxcroft (2018) section 11.3.7. In Jamovi: T-tests &gt; Independent Samples T-test. In R: t.test(x, y) 6.1.2.2 Independent samples T-test not assuming equal variances (Welch test) This test is equivalent to previously described test but now we don’t assume equal variances. Variances are unequal if \\(s_1 &gt; 2s_2\\) or \\(s_2 &gt; 2s_1\\). See Navarro and Foxcroft (2018) section 11.4. Hypotheses are also the same as in case of Student test \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic is the same as for Student test: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}},\\] where \\(s_1\\) and \\(s_2\\) are unbiased standard deviations of the samples. Test statistic is evaluated on t-distribution where df is calculated as \\[df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(s_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(s_2^2/n_2\\right)^2}{n_2-1}}.\\] Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.4.2 In Jamovi: T-tests &gt; Independent Samples T-test | Welch's. In R: t.test(x, y, var.equal = FALSE) 6.2 Unpaired or paired samples Previous tests assumed independence of samples. This is not true if we have paired values. For instance, if samples contain measurements of same observations in two different points in time, then the values representing same observations are paired. 6.2.0.1 Paired samples T-test See Navarro and Foxcroft (2018) section 11.5. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\hat{D}}{s_\\Delta\\div \\sqrt{n_1 + n_2}},\\] where \\(s_\\Delta\\) is difference in standard deviation expressed as \\(s_\\Delta = s_1 - s_2\\) and \\(\\hat{D}\\) is the mean of differences between paired values calculated as \\[\\hat{D} = \\frac{1}{n} \\sum_{i=1}^i{(X_{i1} - X_{i2})}.\\] Test statistic is evaluated on t-distribution with \\(n-1\\) df where \\(n\\) is the number of pairs. Test assumes normality of data. In Jamovi: T-tests &gt; Paired Samples T-test. In R: t.test(x, y, paired = T) 6.3 One-tailed or two-tailed tests All the tests described above were presented as one-tailed tests. This means that we were not interested in whether the differences are positive or negative. Two-tailed versions of these tests also take this into consideration. See Navarro and Foxcroft (2018) section 11.6. If we wish to test if mean of sample (\\(\\bar{x}\\)) is greater than hypothetical population mean (\\(\\mu\\)), then our hypotheses are the following: \\(H_0: \\bar{x} \\le \\mu\\) \\(H_1: \\bar{x} \\gt \\mu\\) If we wish to test if mean of one sample (\\(\\bar{x}_1\\)) is greater than mean of another sample (\\(\\bar{x}_2\\)) , then our hypotheses are the following: \\(H_0: \\bar{x}_1 \\le \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\gt \\bar{x}_2\\) In Jamovi: T-tests &gt; ... T-test | Group 1 &gt; Group 2 or Group 1 &lt; Group 2. In R: t.test(x, y, alternative = 'less') or t.test(x, y, alternative = 'greater') 6.4 Parametric or nonparametric tests Population parameters (e.g. mean, standard deviance) can be estimated from sample parameters only if we can assume that the distribution of values in population (and thus sample) follows normal distribution. If we can not make assumptions about the distribution or parameters of underlying population values we need to use nonparametric tests. 6.4.1 Normality There are various ways to determine whether or not values are normally distributed or not. Here we look at QQ plots and Shapiro-Wilk test. 6.4.1.1 QQ plot On such plots quantiles of data are plotted against theoretical quantiles representing normal distribution. If quantiles of data are highly correlated to these theoretical quantiles (relationship follows a straight line), then data is normally distributed. Interpretation of QQ plot is thus not precise. See Navarro and Foxcroft (2018) section 11.8.1. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Q-Q. In R: qqnorm(y) 6.4.1.2 Shapiro-Wilk test This test determines if data is normally distributed or not. In other words, it tests if sample comes from a normally distributed population. See Navarro and Foxcroft (2018) section 11.8.2. Test statistic \\(W\\) is calculated as \\[W = \\frac{(\\sum^n_{i=1}{a_ix_i})^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2}\\] The exact explanation of \\(a\\) and thus the logic is complicated but higher value of \\(W\\) indicates non-normality. If the test statistic is statistically significant, then data is not normally distributed: \\(H_0\\): Data is normally distributed \\(H_1\\): Data is not normally distributed In Jamovi: Exploration &gt; Descriptives &gt; Statistics | Shapiro-Wilk. In R: shapiro.test(x) 6.4.2 Nonparametric tests 6.4.2.1 Mann-Whitney U test This is a test to compare distributions (and medians) of two unpaired samples if we can’t assume normality. The test is also known as Wilcoxon rank-sum test. The test is also known as Wilcoxon rank-sum test. See Navarro and Foxcroft (2018) section 11.9.1. Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. Formal definition of \\(H_{0}\\): A randomly selected value from one sample is equally likely to be less than or greater than a randomly selected value from a second sample. Test statistic \\(U\\) is calculated as \\[U = \\sum^n_{i=1} \\sum^m_{j=1} S(X_1, X_2),\\] where \\(n\\) are rows and \\(m\\) columns of a matrix \\(S(X_1, X_2)\\) described as below. \\[S(X_1, X_2) = \\begin{cases} 1 &amp; \\text{if } Y &lt; X\\\\ \\frac12 &amp; \\text{if } Y = X\\\\\\ 1 &amp; \\text{if } Y &gt; X\\ \\end{cases}\\] Basically, we compare all values and count the times when values from one sample are higher than values from another sample. The \\(U\\) is just the count of these differences. For \\(n \\ge 20\\), p-value for \\(U\\) is calculated on a normal distribution. Test assumes independence of samples. In Jamovi: T-tests &gt; Independent Samples T-test | Mann-Whitney U. In R: wilcox.test(x, y) 6.4.2.2 Wilcoxon signed-rank test This is similar to Mann-Whitney U test but used for paired samples. The test is also known as one sample Wilcoxon test. See Navarro and Foxcroft (2018) section 11.9.2 Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. The W statistic is calculated as: \\[W = \\sum^n_{i = 1}(sgn(x_{1i}x_{2i}) \\times R_i),\\] where \\(sgn(x_{1i}x_{2i})\\) is sign function (1 for positive difference, -1 for negative) and \\(R_i\\) is the rank of absolute difference. Basically, we are comparing how different is the ranking of values between two samples. The \\(W\\) is just the sum of ranked differences. For \\(n \\ge 20\\), p-value for \\(W\\) is calculated on normal distribution. Test has no relevant assumptions. In Jamovi: T-tests &gt; Paired Samples T-test | Wilcoxon rank. In R: wilcox.test(x, y) 6.4.2.3 Kolmogorov-Smirnov test This test is equivalent to Mann-Whitney U test, although the calculation is very different. This test compares the overall shape of two distributions using cumulative distribution function. Hypotheses: \\(H_{0}\\): Distributions of both samples are the same. \\(H_{1}\\): Distributions of samples are different. Test statistic \\(D\\) is simply the maximum absolute difference between two cumulative distribution functions. P-value is determined by the extremity of the test statistic \\(D\\) on Kolmogorov distribution. In R: ks.test(x, y) 6.5 How to decide which test to use? One sample Unpaired samples Normally distributed One sample T-test Not normally distributed Wilcoxon signed-rank test Two samples Unpaired samples Normally distributed Equal variance Independent samples T-test assuming equal variances (Student test) Unequal variance Independent samples T-test assuming unequal variances (Welch test) Not normally distributed Mann-Whitney U test Paired samples Normally distributed Paired samples T-test / Wilcoxon rank-sum test Not normally distributed Wilcoxon signed-rank test Three samples Unpaired samples Normally distributed Analysis of Variance "],["references.html", "Section 7 References", " Section 7 References Crawley, Michael J. 2013. The r Book. Second edition. Chichester, West Sussex, UK: Wiley. https://www.cs.upc.edu/~robert/teaching/estadistica/TheRBook.pdf. Dash, Ian. 2016. “Flowchart of Statistics for Research.” https://doi.org/10.13140/RG.2.2.12014.41283/1. Navarro, Danielle J, and David R Foxcroft. 2018. Learning Statistics with Jamovi: A Tutorial for Psychology Students and Other Beginners. Danielle J. Navarro; David R. Foxcroft. https://doi.org/10.24384/HGC3-7P15. Stevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677. Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10. "]]
