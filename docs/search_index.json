[["index.html", "Research methods Section 1 Syllabus 1.1 Teaching 1.2 Scoring 1.3 Topics", " Research methods Section 1 Syllabus These course notes include information and study materials on the quantitative part of MS.0825 Research methods course of Agri-Food Business Management Master’s program. 1.1 Teaching Flipped classroom approach is used for teaching. This means that students are expected to learn the methods before meetings. During the meetings we revise the theoretical material, address any questions and apply methods in practice. After meetings students apply the methods on their own datasets. 1.1.1 Schedule Meetings take place on Mondays, Wednesdays and Thursdays at 9 o’clock in the morning. Link to meetings is provided under the course page in Moodle. 1.1.2 Aims After completing the course students should be able to … … understand basic concepts in statistics; … know how to describe data, both numerically and visually; … choose an appropriate method to solve a problem; … use a statistical package for data analysis; … communicate results (interpret, explain, present); … learn about statistical methods individually; … find the courage to apply statistical methods. 1.2 Scoring This part of the course gives 50% of total course points. This 50% consists of 50 points that can be obtained by submitting: feedback on reading material for upcoming meeting (10*1=10 points), and report containing the application of methods learned (10*4=40 points). To gain the 40 points students are required to combine the applications of a method into a report. Participation in meetings does not affect points but meetings are not recorded. 1.2.1 Feedback on reading material Submit your questions or thoughts on given reading material before each meeting in Moodle. Feedback can be either an explanation of something you learned, a question on something you don’t understand, or an answer to another student’s question. Compulsory reading is from the following books: Navarro and Foxcroft (2018), Crawley (2013) and Hastie, Tibshirani, and Friedman (2017). Each part of feedback is graded as follows: 0 points - Feedback is missing or lacking 1 point - Feedback demonstrates that student has revised relevant material 1.2.2 Research project Students are required to create a research report as follows: Choose at least one data set Apply at least one method from 10 topics on this data set (i.e. you can skip 2 topics) Make changes according to feedback Present the results as a written report Each of the 10 topics is graded as follows: 0 points - Method from a particular topic is not applied 1 point - Method is applied but does not suit data 2 points - Chosen method suits data but is incorrectly applied 3 points - Method is correctly applied but conclusions are incorrect or missing 4 points - Method is correctly applied and conclusions are correct 1.2.2.1 Datasets To apply methods, datasets need to have multiple variables. We mostly learn to explore continuous data but some methods require at least one discrete variable. Students are free to choose a dataset. Possible sources for data: Jamovi: Open &gt; Data Library Goodle dataset search Kaggle datasets Data hub collections Our World in Data World Values Survey European Social Survey 1.3 Topics We cover some common and traditional statistical methods. 1.3.0.1 Descriptive statistics Descriptive statistics 1.3.0.2 Hypothesis testing Hypothesis testing. Comparing categorical variables Comparing numerical variables Analysis of variance 1.3.0.3 Regression analysis Correlation analysis Simple linear regression Data transformations Multiple linear regression Logistic regression 1.3.0.4 Multivariate statistics Principal component analysis Factor analysis Clustering References "],["introduction.html", "Section 2 Introduction 2.1 Quantitative methods 2.2 Statistics 2.3 Software 2.4 Data management", " Section 2 Introduction 2.1 Quantitative methods 2.1.1 Quantitative and qualitative methods QuaNTitative QuaLitative Numeric data Semantic data Large-N Small-N, case studies Generalize Explore Measure and test Understand Statistical analysis Interpretation 2.2 Statistics See Navarro and Foxcroft (2018) sections 1.1.1 and 1.5 for a brief introduction. Sections 2.3-2.7 also provide useful background on validity and reliability of statistical methods. Dash (2016) has summarized statistics as follows: Figure 2.1: Flowchart of statistics 2.2.1 Descriptive and inferential statistics Descriptive statistics Inferential statistics Data on entire population Only a sample of a population Simple measures (e.g. mean) Point estimates and intervals Describing Generalizing 2.2.2 Frequentist and bayesian approach to statistics Frequentist Bayesian Traditional Modern What’s the probability of data given some estimate? What’s the probability of an estimate? Prior is not relevant Estimates are conditional on priors Single parametric inference Multiplication of an inference to get a posterior estimate 2.2.3 Statistics and data science, machine learning, artificial intelligence, … Statistics DS, ML, AI, … Less data Big data Clean datasets Untidy data in various formats Traditonal methods Novel methods Mathematics and calculations Programming approach Aim is to explain Aim is to predict 2.3 Software 2.3.1 Commonly used software Spreadsheet vs statistical software Spreadsheet - table management (e.g. Microsoft Excel, Google Sheets, LibreOffice Calc) SPSS - simple GUI-based app with limited functionality R - extensible programming language designed for data analysis Stata - Mostly CLI-based, well suited for econometrics 2.3.2 Jamovi To apply the methods we learn, we use Jamovi. Jamovi is a new “3rd generation” statistical spreadsheet. designed from the ground up to be easy to use, jamovi is a compelling alternative to costly statistical products such as SPSS and SAS. Jamovi is built on top of the R statistical language, giving you access to the best the statistics community has to offer. You can try it online without installing at https://cloud.jamovi.org/. 2.3.3 R language and RStudio This allows us to get under the hood of Jamovi so to speak. R language is based on S language orginating from 1970’s. Developed during 1990’s and became public around 2000. R is a language and environment for statistical computing and graphics. R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, …) and graphical techniques, and is highly extensible. 2.4 Data management 2.4.1 Data structure Usually data is stored in 2-dimensonal tables, i.e. data has rows and columns. To apply statistical procedures, most software requires data to be formatted in a conventional way. In this respect it’s useful to follow “tidy data” principle (Wickham 2014): Every column is a variable. Every row is an observation. Every cell is a single value. This is how data should be formatted before running most statistical procedures. One way to tidy a data table is to use the Pivot table feature in a spreadsheet app. 2.4.2 Scales of measurement Scale of measurement determines the procedures that can be applied to the data. The following typology was first published by Stevens (1946). See Navarro and Foxcroft (2018) section 2.2 for a more detailed explanation. Each scale in the table includes also the properties, operations and measures a on previous rows. Scale Property Operations Central tendency Nominal Classification =, ≠ Mode Ordinal Level &gt;, &lt; Median Interval Difference +, − Arithmetic mean Ratio Magnitude ×, / Geometric mean, harmonic mean 2.4.2.1 Nominal scale Nominal variables contain names (i.e. characters, factors or strings) that do not have a natural order. Such data can be summarized only by counting values or determining the mode. 2.4.2.2 Ordinal scale Ordinal variables have the characteristics of nominal variables with the added possibility of naturally ordering these names. As such, ordinal variables can be said to have levels. It’s important to note that an increase of one level to the next is not necessarily numerically equivalent to an increase of another level to the next. Thus, it is not always meaningful to convert these levels to numbers and do calculations on them. 2.4.2.3 Interval scale Interval variables are expressed numerically. While differences between numbers on interval scale are meaningful, there isn’t a natural zero value. Thus, calculations on interval variables is limited to finding differences and division or multiplication of such values is not reasonable. A classic example of an interval variable is temperature. Difference between 5°C and 15°C is 10°C but 15°C is not 3 times warmer than 5°C. 2.4.2.4 Ratio scale Ratio variables are also numeric but have a natural zero value. This means that division and multiplication is meaningful. 2.4.2.5 Binary scale Binary (or dichotomous) variables do not constitute a distinct scale but can be considered as a subgroup of nominal, ordinal or interval variables. Binary variables can only take two values. Many statistical procedures convert or require the conversion of nominal variables to binary for technical reasons (e.g. “dummy” variables). Binary variables are often coded as 0 for false and 1 for true. As such, calculating the sum or mean of binary variables conveys useful information (think why that is!). 2.4.2.6 Continuous or discrete variables In addition to the previous typology, we can also distinguish between continuous and discrete variables. These are well defined by Navarro and Foxcroft (2018, 20): \"A continuous variable is one in which, for any two values that you can think of, it’s always logically possible to have another value in between. A discrete variable is, in effect, a variable that isn’t continuous. For a discrete variable it’s sometimes the case that there’s nothing in the middle.\" 2.4.2.7 Scales in statistical software The difference between ratio and interval variables is only theoretical. Statistical software treats all numbers as numeric variables and does not distinguish between interval and ratio variables. When binary variables are coded so that they only have values 0 and 1, these are also considered numerical. A lot of statistical software (e.g. Jamovi) distinguish only between three types of variables: nominal, ordinal and numeric. References "],["descriptive-statistics-1.html", "Section 3 Descriptive statistics 3.1 Central tendency 3.2 Variability 3.3 Plots", " Section 3 Descriptive statistics Any data analysis should begin with an exploration of the structure of and values in dataset. Description of values should also be part of the reporting of analysis. This can be done using some simple methods. When we use the methods of descriptive statistics, we need to be aware of that we merely describe the dataset at hand. If observations in the data set constitute a sample, we cannot draw any wider conclusions on population using these methods. Below we are thus defining the statistics for populations and not samples. Description of nominal or ordinal variables is simple and limited to just counting unique values and finding the mode and/or median value. But suppose we have variables that contain sequences of numbers. How can we describe these? 3.1 Central tendency A concise way to represent numeric values is expressing the center of values. This can also be understood the typical value or expected value (expectation of \\(x\\), i.e. \\(E(x)\\)). Note that estimating a central tendency is only meaningful if there is a natural center. i.e. the distribution of values is unimodal. See Navarro and Foxcroft (2018) section 4.1. 3.1.1 Mode Mode is simply the most frequent value in a sequence. This statistic can be useful for nominal and ordinal variables but it’s rarely used for numeric variables, especially on a continuous scale. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Mode 3.1.2 Median When sequence of values is ordered, the middle value is median. In case there are even number of values (\\(N\\)) in a sequence, median is the average of the two values in the middle. Thus, there are equal number of values above and below the median value. When \\(N\\) is an even number, half of the values are above and half below the median value. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Median In R: median(x) 3.1.3 Mean While there are several forms of mean, arithmetic mean is most useful in statistics and also considered here. Mean is in other words the average value: it’s the sum of values (\\(X\\)) divided by number of values: \\[\\bar{X} = \\frac{1}{N} \\sum{^N_{i=1}}{X_i}\\] A feature of mean value is that if you only know the \\(\\bar{X}\\) and \\(N\\), you can still calculate \\(\\sum{^N_{i=1}}{X_i}\\). In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Mean In R: mean(x) 3.2 Variability Central tendency describes values only partially as it tells nothing about the deviation of values from that mean. Estimating also the variability of values provides a more manifold understanding of values. See Navarro and Foxcroft (2018) section 4.2. 3.2.1 Range Most simple way to express variability is to indicate the range of all values, i.e. the difference between minimum and maximum values. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Range, Minimum, Maximum In R: range(x) 3.2.2 Quantiles and interquartile range Quantiles are in essence calculated just like median, because median is the lowest quantile, the 2-quantile. In addition to dividing sequence of ordered values into two halves separated by median, such sequence can be diveded into any number of equal groups as long as the number of groups (\\(k\\)) is smaller than or equal to the number of values, i.e. \\(k \\le N\\). Some common quantiles are quartiles (\\(k = 4\\)), deciles (\\(k = 10\\)) and centiles/percentiles (\\(k = 100\\)). In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Percentile Values In R: quantile(x) Arguably quartile is most frequently used among quantiles because the 2nd and 4th quartiles contain half of all values. This interquartile range (IQR) thus contains the middle half of all values. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | IQR In R: IQR(x) 3.2.3 Variance Although difficult to interpret, variance is used in many statistical calculations due to its mathematical properties. Variance is mean squared difference from the mean value: \\[Var(X) = \\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}\\] In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Variance In R: var(x) 3.2.4 Standard deviation Interpretability issues of variance arise from the fact that the calculation includes squaring and thus the value of the statistic is vastly different to understand regarding initial scale. A solution would be to find a square root of variance. This is what the standard deviation is: the square root of variance. \\[\\sigma = \\sqrt{\\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}}\\] In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Std. deviation In R: sd(x) 3.3 Plots Previously explained measures allow us to only summarize values into fewer numbers. Plotting allows to present these values more intuitively or even depict all of the values at once. 3.3.1 Boxplot Boxplots illustrate the quartiles of values (including interquartile range) and extreme values (outliers). Outliers are usually values that are below the 2nd quartile or above the 4th quartile by 1.5 times the interquartile range. On boxplots, half of all the values lie within the box and another half on the lines, outliers excluded. See Navarro and Foxcroft (2018) section 5.2. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Box Plots In R: boxplot(x) 3.3.2 Histogram and density plot Histograms are essentially barplots where all values are divided between ranges of equal with, i.e. bins. Instead of bins, distribution of values can also be expressed continuously by smoothing the bins. This results in a density plot. See Navarro and Foxcroft (2018) section 5.1. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Histograms In R: histogram(x) and plot(density(x)) References "],["hypothesis-testing-1.html", "Section 4 Hypothesis testing 4.1 Sample and population 4.2 Hypothesis testing", " Section 4 Hypothesis testing 4.1 Sample and population Distinguishing between sample and population is important when we wish to draw inferences about population when we only have data on a sample. 4.1.1 Sample, population, sampling See Navarro and Foxcroft (2018) section 8.1. What is the difference between population, sample and (simple) random sample? 4.1.2 Estimating population parameters 4.1.2.1 Population mean See Navarro and Foxcroft (2018) section 8.4.1. How do you get population mean if you only know the sample mean? 4.1.2.2 Population standard deviation See Navarro and Foxcroft (2018) section 8.4.2. \\[\\hat{\\sigma} = \\sqrt{\\frac{1}{n-1} \\sum{^n_{i=1}{(x_i-\\bar{x})^2}}}\\] Why and how is sample standard deviation different from population standard deviation? 4.1.3 Confidence intervals See Navarro and Foxcroft (2018) section 8.5 A 95% CI of the mean \\(X \\sim N(\\mu,\\sigma^2)\\) is calculated as \\[CI_{95} = \\hat{x} \\pm(1.96\\times\\frac{\\sigma}{\\sqrt{N}})\\] What is the interpretation of confidence interval? 4.2 Hypothesis testing 4.2.1 Null and alternative hypotheses See Navarro and Foxcroft (2018) section 9.1.2. Statistical hypothesis testing involves two hypotheses: null (\\(H_0\\)) and alternative (\\(H_1\\)) hypothesis. These are usually defined as follows: \\(H_0\\): There is no statistically significant difference. \\(H_1\\): There is a statistically significant difference. We always test if we can reject \\(H_0\\). If we reject \\(H_0\\), then we accept \\(H_1\\). If we fail to reject \\(H_0\\), then we accept \\(H_1\\). 4.2.2 Types of errors See Navarro and Foxcroft (2018) section 9.2. We accept \\(H_0\\) We reject \\(H_0\\) \\(H_0\\) is true We are correct We commit type I error \\(H_0\\) is false We commit type II error We are correct The rate of type I error is considered the significance level of a test (denoted as \\(\\alpha\\)). 4.2.3 Test statistic See Navarro and Foxcroft (2018) sections 9.3 and 10.1.6. We use a test statistic to make a decision whether we can or can not reject \\(H_0\\). We reject \\(H_0\\) if the value of a test statistic is sufficiently extreme, i.e. different from 0. 4.2.4 P-value See Navarro and Foxcroft (2018) sections 9.5 and 9.6. P-value is the probability of obtaining at least as extreme value of test statistic if \\(H_0\\) is correct. Thus, it’s the rate of committing a type I error, i.e. the significance level \\(\\alpha\\). If the p-value is below \\(\\alpha\\), we reject \\(H_0\\) and accept \\(H_1\\). \\(p \\ge \\alpha \\implies H_0\\) \\(p \\lt \\alpha \\implies H_1\\) P-value is not the probability of \\(H_0\\) being false or \\(H_{1}\\) being true. Theoretically, a lower p-value does not show a greater difference or vice versa. Only it’s location relative to is what \\(\\alpha\\) matters. 4.2.5 Multiple comparisons problem Every time you find a statistically significant result, there is a possibility that you got this extreme data by chance. In this case, you committed a type 1 error. Therefore, if you do a lot of tests, the possibility that at least one of the statistically significant results is actually not true. Thus, the number of tests you do should be limited. References "],["comparing-categorical-data.html", "Section 5 Comparing categorical data 5.1 Contingency tables 5.2 \\(\\chi^2\\)-test 5.3 Degrees of freedom", " Section 5 Comparing categorical data 5.1 Contingency tables For analyzing categorical data we can compare the counts of categories. This gives us contingency tables where cells indicate how many times each value appears in a categorical variable. When we compare more than one variable, contingency tables have more than one dimension. Usually we tabulate two variables against each other. If these variables have \\(m\\) and \\(n\\) unique values (categories), then we obtain a 2-dimensional \\(m*n\\) contingency table. 5.2 \\(\\chi^2\\)-test 5.2.1 Goodness of fit \\(\\chi^2\\)-test For goodness of fit \\(\\chi^2\\)-test see Navarro and Foxcroft (2018) section 10.1.3. We test if frequencies of categories are different from some expected frequencies for a single variable: \\(H_0\\): Frequencies of categories are as expected. \\(H_1\\): Frequencies of categories are different from what is expected. The test statistic is calculated as \\[\\chi^2 = \\sum{^k_{i=1}{\\frac{(O_i-E_i)^2}{E_i}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency of \\(i\\)th category (\\(k\\)). In Jamovi: Frequencies &gt; N Outcomes In R: chisq.test(x) 5.2.2 \\(\\chi^2\\)-test of independence For \\(\\chi^2\\)-test of independence see Navarro and Foxcroft (2018) section 10.2. In this case we test if two variables are associated: \\(H_0\\): Variables are independent. \\(H_1\\): Variables are associated. The test statistic is calculated as \\[\\chi^2 = \\sum{^r_{i=1}\\sum{^c_{j=1}}{\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency in \\(i\\)th row (\\(r\\)) and \\(j\\)th column (\\(c\\)). An assumption of \\(\\chi^2\\)-test is that there are \\(&gt;5\\) observations in each cell of a contingency table. In Jamovi: Frequencies &gt; Independent Samples In R: chisq.test(x, y) 5.3 Degrees of freedom See Navarro and Foxcroft (2018) section 10.1.5 and pages 227-228. Degrees of freedom (DoF) indicates the number of values that are free to vary. This is relevant for evaluating the \\(\\chi^2\\)-test statistic as it indicates the critical value on \\(\\chi^2\\)-distribution. For goodness of fit \\(\\chi^2\\)-test DOF is calculated simply as \\(df = k - 1\\) where \\(k\\) is the number of categories. For \\(\\chi^2\\)-test of independence DoF is calculated as \\[df = (r - 1)(c - 1),\\] where \\(r\\) is the number of rows and \\(c\\) the number of columns in a contingency table. References "],["comparing-numerical-data.html", "Section 6 Comparing numerical data 6.1 One or two samples 6.2 Unpaired or paired samples 6.3 One-tailed or two-tailed tests 6.4 Parametric or nonparametric tests 6.5 How to decide which test to use?", " Section 6 Comparing numerical data Statistical testing for numerical data is different from when we have categorical data as we are no longer limited to using frequencies but parameters, ranks and other measures. However, note that nonparametric tests explained at the end can also be used for ordinal variables. This section covers only comparisons with one or two samples. Comparing multiple mean values is done using analysis of variance (Anova) and is considered in the next section. Also, this section only outlines some classic tests and there are many more. 6.1 One or two samples 6.1.1 One sample 6.1.1.1 One sample T-test This test is used to determine if our sample is taken from a population with a given mean. So we test if the sample mean \\(\\bar{x}\\) is equal to a hypothetical population mean \\(\\mu\\). See Navarro and Foxcroft (2018) section 11.2. Hypotheses: \\(H_0: \\bar{x} = \\mu\\) \\(H_1: \\bar{x} \\neq \\mu\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x} - \\mu}{s \\div \\sqrt{n}},\\] where \\(\\bar{x}\\) is the sample mean and \\(\\mu\\) the hypothetical population mean that it is tested against and \\(s\\) sample standard deviation. Test statistic is evaluated on t-distribution with \\(n-1\\) degrees of freedom (df). Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.2.3. In Jamovi: T-tests &gt; One Sample T-test. In R: t.test(x, mu) 6.1.2 Two samples 6.1.2.1 Independent samples T-test assuming equal variances (Student test) This test compares the mean values to determine if these are equal. Equality of means implies that samples come from the same population. The test assumes that variances of the samples are equal. This is true if \\(\\frac{1}{2} &lt; \\frac{s_1}{s_2}&lt;2\\). See Navarro and Foxcroft (2018) section 11.3. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = s_p\\sqrt{\\frac{1}{n_1} +{\\frac{1}{n_2}}},\\] where \\(n_1\\) and \\(n_2\\) are sample sizes and \\(s_p\\) is the pooled standard deviation calculated as \\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}},\\] where \\(s_1\\) and \\(s_2\\) are standard deviations of the samples. Test statistic is evaluated on t-distribution with \\(n_1+n_2-2\\) df. Test assumes normality and independence of data and homogeneity of variance. See Navarro and Foxcroft (2018) section 11.3.7. In Jamovi: T-tests &gt; Independent Samples T-test. In R: t.test(x, y) 6.1.2.2 Independent samples T-test not assuming equal variances (Welch test) This test is equivalent to previously described test but now we don’t assume equal variances. Variances are unequal if \\(s_1 &gt; 2s_2\\) or \\(s_2 &gt; 2s_1\\). See Navarro and Foxcroft (2018) section 11.4. Hypotheses are also the same as in case of Student test \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic is the same as for Student test: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}},\\] where \\(s_1\\) and \\(s_2\\) are unbiased standard deviations of the samples. Test statistic is evaluated on t-distribution where df is calculated as \\[df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(s_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(s_2^2/n_2\\right)^2}{n_2-1}}.\\] Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.4.2 In Jamovi: T-tests &gt; Independent Samples T-test | Welch's. In R: t.test(x, y, var.equal = FALSE) 6.2 Unpaired or paired samples Previous tests assumed independence of samples. This is not true if we have paired values. For instance, if samples contain measurements of same observations in two different points in time, then the values representing same observations are paired. 6.2.0.1 Paired samples T-test See Navarro and Foxcroft (2018) section 11.5. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\hat{D}}{s_\\Delta\\div \\sqrt{n_1 + n_2}},\\] where \\(s_\\Delta\\) is difference in standard deviation expressed as \\(s_\\Delta = s_1 - s_2\\) and \\(\\hat{D}\\) is the mean of differences between paired values calculated as \\[\\hat{D} = \\frac{1}{n} \\sum_{i=1}^i{(X_{i1} - X_{i2})}.\\] Test statistic is evaluated on t-distribution with \\(n-1\\) df where \\(n\\) is the number of pairs. Test assumes normality of data. In Jamovi: T-tests &gt; Paired Samples T-test. In R: t.test(x, y, paired = T) 6.3 One-tailed or two-tailed tests All the tests described above were presented as one-tailed tests. This means that we were not interested in whether the differences are positive or negative. Two-tailed versions of these tests also take this into consideration. See Navarro and Foxcroft (2018) section 11.6. If we wish to test if mean of sample (\\(\\bar{x}\\)) is greater than hypothetical population mean (\\(\\mu\\)), then our hypotheses are the following: \\(H_0: \\bar{x} \\le \\mu\\) \\(H_1: \\bar{x} \\gt \\mu\\) If we wish to test if mean of one sample (\\(\\bar{x}_1\\)) is greater than mean of another sample (\\(\\bar{x}_2\\)) , then our hypotheses are the following: \\(H_0: \\bar{x}_1 \\le \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\gt \\bar{x}_2\\) In Jamovi: T-tests &gt; ... T-test | Group 1 &gt; Group 2 or Group 1 &lt; Group 2. In R: t.test(x, y, alternative = 'less') or t.test(x, y, alternative = 'greater') 6.4 Parametric or nonparametric tests Population parameters (e.g. mean, standard deviance) can be estimated from sample parameters only if we can assume that the distribution of values in population (and thus sample) follows normal distribution. If we can not make assumptions about the distribution or parameters of underlying population values we need to use nonparametric tests. 6.4.1 Normality There are various ways to determine whether or not values are normally distributed or not. Here we look at QQ plots and Shapiro-Wilk test. 6.4.1.1 QQ plot On such plots quantiles of data are plotted against theoretical quantiles representing normal distribution. If quantiles of data are highly correlated to these theoretical quantiles (relationship follows a straight line), then data is normally distributed. Interpretation of QQ plot is thus not precise. See Navarro and Foxcroft (2018) section 11.8.1. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Q-Q. In R: qqnorm(y) 6.4.1.2 Shapiro-Wilk test This test determines if data is normally distributed or not. In other words, it tests if sample comes from a normally distributed population. See Navarro and Foxcroft (2018) section 11.8.2. Test statistic \\(W\\) is calculated as \\[W = \\frac{(\\sum^n_{i=1}{a_ix_i})^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2}\\] The exact explanation of \\(a\\) and thus the logic is complicated but higher value of \\(W\\) indicates non-normality. If the test statistic is statistically significant, then data is not normally distributed: \\(H_0\\): Data is normally distributed \\(H_1\\): Data is not normally distributed In Jamovi: Exploration &gt; Descriptives &gt; Statistics | Shapiro-Wilk. In R: shapiro.test(x) 6.4.2 Nonparametric tests 6.4.2.1 Mann-Whitney U test This is a test to compare distributions (and medians) of two unpaired samples if we can’t assume normality. The test is also known as Wilcoxon rank-sum test. The test is also known as Wilcoxon rank-sum test. See Navarro and Foxcroft (2018) section 11.9.1. Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. Formal definition of \\(H_{0}\\): A randomly selected value from one sample is equally likely to be less than or greater than a randomly selected value from a second sample. Test statistic \\(U\\) is calculated as \\[U = \\sum^n_{i=1} \\sum^m_{j=1} S(X_1, X_2),\\] where \\(n\\) are rows and \\(m\\) columns of a matrix \\(S(X_1, X_2)\\) described as below. \\[S(X_1, X_2) = \\begin{cases} 1 &amp; \\text{if } Y &lt; X\\\\ \\frac12 &amp; \\text{if } Y = X\\\\\\ 1 &amp; \\text{if } Y &gt; X\\ \\end{cases}\\] Basically, we compare all values and count the times when values from one sample are higher than values from another sample. The \\(U\\) is just the count of these differences. For \\(n \\ge 20\\), p-value for \\(U\\) is calculated on a normal distribution. Test assumes independence of samples. In Jamovi: T-tests &gt; Independent Samples T-test | Mann-Whitney U. In R: wilcox.test(x, y) 6.4.2.2 Wilcoxon signed-rank test This is similar to Mann-Whitney U test but used for paired samples. The test is also known as one sample Wilcoxon test. See Navarro and Foxcroft (2018) section 11.9.2 Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. The W statistic is calculated as: \\[W = \\sum^n_{i = 1}(sgn(x_{1i}x_{2i}) \\times R_i),\\] where \\(sgn(x_{1i}x_{2i})\\) is sign function (1 for positive difference, -1 for negative) and \\(R_i\\) is the rank of absolute difference. Basically, we are comparing how different is the ranking of values between two samples. The \\(W\\) is just the sum of ranked differences. For \\(n \\ge 20\\), p-value for \\(W\\) is calculated on normal distribution. Test has no relevant assumptions. In Jamovi: T-tests &gt; Paired Samples T-test | Wilcoxon rank. In R: wilcox.test(x, y) 6.4.2.3 Kolmogorov-Smirnov test This test is equivalent to Mann-Whitney U test, although the calculation is very different. This test compares the overall shape of two distributions using cumulative distribution function. Hypotheses: \\(H_{0}\\): Distributions of both samples are the same. \\(H_{1}\\): Distributions of samples are different. Test statistic \\(D\\) is simply the maximum absolute difference between two cumulative distribution functions. P-value is determined by the extremity of the test statistic \\(D\\) on Kolmogorov distribution. In R: ks.test(x, y) 6.5 How to decide which test to use? One sample Unpaired samples Normally distributed One sample T-test Not normally distributed Wilcoxon signed-rank test Two samples Unpaired samples Normally distributed Equal variance Independent samples T-test assuming equal variances (Student test) Unequal variance Independent samples T-test assuming unequal variances (Welch test) Not normally distributed Mann-Whitney U test Paired samples Normally distributed Paired samples T-test / Wilcoxon rank-sum test Not normally distributed Wilcoxon signed-rank test Three samples Unpaired samples Normally distributed Analysis of Variance References "],["analysis-of-variance.html", "Section 7 Analysis of variance 7.1 One-way Anova 7.2 Post-hoc tests 7.3 How to decide which test to use?", " Section 7 Analysis of variance Whereas tests in previous chapters could be used to compare only two parameters we often wish to compare parameters of several samples. We can use Analysis of variance (Anova) to do just that. 7.1 One-way Anova This test is used to compare mean values of three or more samples. Samples are distinguished by a factor variable, so each sample is a group of the factor. See Navarro and Foxcroft (2018) section 13.2. \\(H_0: \\bar{x}_1 = \\bar{x}_2 = \\bar{x}_3\\) \\(H_1: \\bar{x}_1 = \\bar{x}_2 \\neq \\bar{x}_3\\) or \\(\\bar{x}_1 \\neq \\bar{x}_2 \\neq \\bar{x}_3\\) In other words, our \\(H_1\\) is that at least one group mean is different from others. Test statistic \\(F\\) is calculated as \\[F = MSA / MSE,\\] where \\(MSA\\) expresses variation between group and \\(MSE\\) represents random variaton. In order to understand what this means let’s start from the initial measures, the sum of squares of variable \\(y\\) for observations \\(i\\) in groups \\(j\\): within-group sum of squares, i.e. sum of squares of errors (SSE) \\(SSE=\\sum_{j=1}^{k}\\sum_{i=1}^{n} (y_{ij}-\\overline{y_{j}})^{2}\\), \\(df = n - k\\) between group sum of squares, i.e. sum of squares between groups (SSA) \\(SSA=\\sum_{j=1}^{k} (\\overline{y_{j}}-\\overline{y})^{2}\\), \\(df = k - 1\\) total sum of squares (SST=SSE+SSA) \\(SST=\\sum_{j=1}^{k}\\sum_{i=1}^{n} (y_{ij}-\\overline{y})^{2}\\), \\(df = n - 1\\) We can use these variations to find \\(MSA\\) and \\(MSE\\) as follows: mean squares for SSE (MSE) \\(MSE = SSE / (n - k)\\) mean squares for SSA (MSA) \\(MSA = SSA / (k - 1)\\) Test statistic \\(F\\) is evaluated on F-distribution. Test assumes normality and independence of data and homogeneity of variance (only for Welch’s test). In Jamovi: Anova &gt; One-way Anova. In R: summary(aov(x, y)) 7.2 Post-hoc tests The result of Anova does not tell us about which groups are different but only that there is a difference. We can use post hoc tests to compare the means of all groups pariwise. See Navarro and Foxcroft (2018) section 14.8. In Jamovi: Anova &gt; One-way Anova &gt; Post-Hoc test | Games-Howell or Tukey. In R: TukeyHSD(aov(x, y)) 7.3 How to decide which test to use? Depending on the number of factors and whether or not the assumptions are satisfied, there are several types of Anova. These are explained in Navarro and Foxcroft (2018) sections 13 and 14. Two factors Unpaired groups Normally distributed Equal variances Fisher’s one-way Anova Unequal variances Welch’s one-way Anova Not normally distributed Kruskal-Wallis rank-sum test Paired groups (repeated measures) Normally distributed Repeated measures Anova Not normally distributed Friedman test Several factors Unpaired groups Normally distributed Factorial Anova References "],["correlation-analysis.html", "Section 8 Correlation analysis 8.1 Scatterplot 8.2 Pearson’s correlation coefficient 8.3 Spearman rank-order correlation coefficient 8.4 Correlation matrix and heatmap 8.5 Causality", " Section 8 Correlation analysis Correlation analysis involves measuring relationships between two continuous variables. A correlation coefficient indicates the direction of relationship on a scale -1 … 1 where 0 means lack of relationship. See Navarro and Foxcroft (2018) section 12.1. 8.1 Scatterplot Scatterplots illustrate where observations are positioned relative to two continuous variables. This allows us to see if and how two variables are related. Conventionally dependent variables are presented on vertical (y) axis and independent variable on horizontal (x) axis. See Navarro and Foxcroft (2018) section 12.2. In Jamovi: Exploration &gt; Scatterplot. In R: plot(x, y) 8.2 Pearson’s correlation coefficient Evaluates linear relationship. Thus, perfect positive correlation implies that when \\(x\\) increases by 1 unit, \\(y\\) always increases by fixed unit(s). \\[r = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2 \\sum_{i=1}^{n}(y_i-\\bar{y})^2}\\] Essentially, we compare differences from mean value for values of each group. Statistical significance can also be estimated by finding the probability of t-statistic on t-distribution. \\[t=r\\sqrt{n-2}/\\sqrt{1-r^{2}}\\] See Navarro and Foxcroft (2018) section 12.1.3. In Jamovi: Regression &gt; Correlation Matrix &gt; Correlation Coefficients | Pearson. In R: cor(x) 8.3 Spearman rank-order correlation coefficient Evaluation is based on the ranking of values. Evaluates a monotonic relationship. Thus perfect positive correlation implies that when \\(x\\) increases, then \\(y\\) also always increases. \\[r = 1 - \\frac{6\\sum (x_{i}-y_{i})^{2}}{n(n^{2}-1)}\\] Simply put, we compare rankings of values from each group. See Navarro and Foxcroft (2018) section 12.1.6. In Jamovi: Regression &gt; Correlation Matrix &gt; Correlation Coefficients | Spearman. In R: cor(x, method = 'spearman') 8.4 Correlation matrix and heatmap To find relationships among large number of variables, correlation coefficients can be summarized as a correlation matrix. In Jamovi: Regression &gt; Correlation Matrix &gt; Plot | Correlation matrix ​ or Factor &gt; Reliability Analysis &gt; Additional Options | Correlation heatmap. In R: heatmap(cor(x)) 8.5 Causality It’s important to understand that correlation itself does not imply how the causality between variables functions. References "],["simple-linear-regression.html", "Section 9 Simple linear regression 9.1 Ordinary least squares 9.2 Elements of (OLS) regression models 9.3 Assumptions and diagnostics", " Section 9 Simple linear regression Regression analysis is a statistical procedure that allows us to model relationships between variables so that the causal relationship between variables is defined. It can be considered as the main statistical technique used in economics, i.e. econometrics. There is a large variety of regression models, depending on estimation method, model specification and assumed distributions. This section introduces the most basic of these, the simple linear regression model, i.e. ordinary least squares (OLS) with one independent variable. See Navarro and Foxcroft (2018) section 12.3 for an introduction. 9.1 Ordinary least squares 9.1.1 Model specification Simple linear regression model has one predictor and its model is mathematically expressed as follows: \\[y = \\alpha + \\beta x + \\varepsilon,\\] where \\(y\\) is dependent or explained variable, response or regressand, \\(\\alpha\\) is the intercept or constant, \\(\\beta\\) is a coefficient of \\(x\\), \\(x\\) is independent or explanatory variable, predictor or regressor, and \\(\\varepsilon\\) is the model error. 9.1.2 Calculation The underlying idea behind (ordinary) least squares regression is the minimization of (squared) residuals. Model parameters are calculated (unlike maximum likelihood estimation based on iterations). To estimate the model \\(Y = \\alpha + \\beta x + \\varepsilon\\) we estimate the parameters \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) as follows: \\(\\hat{\\beta} = \\frac{\\sum{x_{i} y_{i}} - \\frac{1}{n} \\sum{x_{i}}\\sum{y_{i}}}{\\sum{x_{i}^{2}} - \\frac{1}{n} (\\sum{x_{i}})^{2}} = \\frac{Cov[x,y]} {Var[x]}\\) \\(\\hat{\\alpha} = \\overline{y} - \\beta \\overline{x}\\) For a simple model \\(y = \\beta x + \\varepsilon\\) we can simply use matrix algebra on values of \\(x\\) and \\(y\\) to find \\(\\hat{\\beta}\\): \\[\\hat{\\beta} = (X^{T} X)^{-1} X^{T} Y,\\] where \\(X\\) is the matrix of predictor and \\(Y\\) the matrix of response. See Navarro and Foxcroft (2018) section 12.4 for illustrations of the idea. In Jamovi: Regression &gt; Linear regression. In R: lm(y ~ x, data) or summary(lm(y ~ x, data)) 9.2 Elements of (OLS) regression models 9.2.1 Intercept In the equation this is the \\(\\alpha\\) and often referred to as the constant. Intercept is the value of \\(y\\) where regression line crosses the Y-axis, so intercept is the value of \\(y\\) when \\(x\\) is zero ( \\(y|x=0\\) ). Intercept does not need to be theoretically valid but it sometimes is. The statistical significance of the intercept is usually not relevant. Interpretation of the intercept: \\[weight2_{i} = 195.135 + 1.176 * weight1_{i} + \\varepsilon_{i}\\] When weight1 is 0 units, then the value of weight2 is 195.135 units. 9.2.2 Coefficient(s) In the equation the \\(\\beta\\) represents coefficient of \\(x\\). It indicates y how many units \\(y\\) increases when \\(x\\) increases by one unit. We can not be sure if the coefficients are actually significant (when estimation is done on a sample). It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating t-statistic from coefficient and standard error. Coefficients are only relevant if their difference from 0 is statistically significant. 9.2.2.1 Numeric predictors Interpretation of coefficient(s): \\[weight2_{i} = 195.135 + 1.176 * weight1_{i} + \\varepsilon_{i}\\] When weight1 increases by 1 unit, then weight2 increases by 1.176 units. 9.2.2.2 Categorical predictors Interpretation of coefficient(s): Suppose that a categorical variable called diet has the following levels: Low, Medium, High. Then in regression analysis the first level (Low) is considered as the base level. All other factor values are compared against this base value. \\[weight2_{i} = 1019.406 + 1.898dietMedium_{i} + 12.01dietHigh_{i} + \\varepsilon_{i}\\] When diet is Low, then weight2 is 1019.406. When diet is Medium, then weight2 is higher by 1.898 units compared to when diet is Low, i.e. 1021.304. Suppose now that the coefficient for dietHigh is not statistically significant. When diet is High, weight2 is no different compared to when diet is Low. If it was statistically significant, we could say that when diet is High, then weight2 is higher by 12.01 units compared to when diet is Low. 9.2.3 Fitted values These are the values of \\(y\\) calculated using the model for every \\(x\\) in the data. In other words, fitted values are predictions. 9.2.4 Residuals Residuals are model errors, represented by the \\(\\varepsilon\\) in the equation. Residuals are the difference in response between observed and fitted (model predicted) values. We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good. 9.2.5 The \\(R^2\\) This is a way to measure goodness of fit, i.e. how well model fits data. \\(R^2\\) indicates the part of variation in response variable that is explained by the model: \\[R^{2} = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS},\\] where the elements are defined a follows: explained sum of squares, \\(ESS\\); \\(\\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^2\\) residual sum of squares, \\(RSS\\); \\(\\sum_{i = 1}^{n} (y_{i} - \\hat{y}_{i})^2\\) total sum of squares, \\(TSS\\); \\(\\sum_{i = 1}^{n} (y_{i} - \\bar{y})^2.\\) Above \\(\\hat{y}_{i}\\) are fitted values, \\(\\overline{y}\\) is the mean value, and \\(y_{i}\\) are the actual values of \\(y\\). Mathematically, the \\(R^2\\) measures how much better is model at explaining the variance of \\(y\\) compared to just the mean. 9.2.6 The adjusted- \\(R^2\\) The more variables we add, the more the model explains. So \\(R^2\\) can be inflated just by adding variables. To penalize a model for the number of predictors ( \\(k\\) ) while considering the number of observations ( \\(n\\) ), the adjusted \\(R^{2}\\) can also be used, particularly for model comparison: \\[\\overline{R^{2}} = 1 - \\frac{RSS/(n-k)}{TSS/(n-1)}\\] 9.3 Assumptions and diagnostics After the estimation of a regression model it should be diagnosed to make sure that it meets at least the following assumptions: Residuals are normally distributed Residuals have equal variance, i.e. variance of residuals does not depend on the value of \\(x\\) In Jamovi: Regression &gt; Linear regression &gt; Assumption Checks. In R: plot(lm(formula, data)) See Navarro and Foxcroft (2018) section 12.9 for an explanation of assumptions and section 12.10 on model checking. 9.3.1 Gauss-Markov assumptions In addition to the practical considerations outlined above, a theoretical way of expressing the assumptions of OLS is via the Gauss–Markov theorem. It posits the following assumptions: linear in parameters \\(Y = \\alpha + \\beta x + \\varepsilon\\) expected error is zero \\(E(\\varepsilon) = 0\\) homoscedasticity \\(var(\\varepsilon) = E(\\varepsilon^{2})\\) no autocorrelation \\(cov(\\varepsilon_{i}, \\varepsilon_{j}) = 0, i \\neq j\\) independence of predictor(s) and residuals \\(cov(x,\\varepsilon) = 0\\) If these are true, then the model is the best linear unbiased estimator (BLUE). References "],["data-transformations.html", "Section 10 Data transformations 10.1 Log-transformations 10.2 Polynomials", " Section 10 Data transformations Sometimes the response variable is not best expressed as a linear function of predictors. This problem often results in residuals that are then not normally distributed or not constant. In such cases estimating the model on transformed data might improve model fit because many relationships in real world are e.g. “logarithmic” in nature. There are two main ways of transforming data for improving the fit of a linear model: using natural logarithms or polynomials. 10.1 Log-transformations Particularly when the distribution of a variable has a strong right skew, linear model could be improved by using naturally logged (\\(log_ex\\)) values of such variable(s). As a result we are essentially converting the distribution of a variable to normal. Depending on whether response or predictor(s) are log-transformed, the resulting model can be one of the following three explained below. It’s important to note that in such transformations the coefficients represent elasticities, i.e. not absolute values but per cent changes in values of response (log-linear) or both, predictors and response (log-log). To illustrate the interpretation of coefficients, examples are provided of a model where the fuel consumption (mpg) was predicted from horsepower (hp). 10.1.1 Log-linear When we use log-transformation on response variable, then we have a log-linear model: \\[ln(y) = \\alpha + \\beta_{1} x + \\varepsilon.\\] Keep in mind that measures of model fit (e.g. \\(R^2\\), AIC) are only valid for model comparison when models have the same predictor variable. Thus, such measures should not be used to compare log-linear model to a model with untransformed response. Interpretation: \\(ln(mpg_{i}) = 3.46 + -0.003 * hp_{i} + \\varepsilon_{i}\\) When hp increases by 1 unit, mpg increases by -0.003429 * 100 = -0.343 percent. In Jamovi: Regression &gt; Linear regression after transforming response variable. In R: lm(log(y) ~ x, data) 10.1.2 Linear-log When one or more predictors are used in logged form, then the model is referred to as a linear-log model: \\[y = \\alpha + \\beta_{1} ln(x) + \\varepsilon\\] Interpretation: \\(mpg_{i} = 72.64 + -10.764 * ln(hp_{i}) + \\varepsilon_{i}\\) When hp increases by 1 per cent, mpg increases by -10.764 / 100 = -0.108 units. In Jamovi: Regression &gt; Linear regression after transforming predictor variable. In R: lm(y ~ log(x), data) 10.1.3 Log-log When either of the previous transformations have not improved the fit of a linear model, we can also transform both response and predictor(s) as follows: \\[ln(y) = \\alpha + \\beta_{1} ln(x) + \\varepsilon\\] Interpretation: \\(ln(mpg_{i}) = 5.545 + -0.53 * ln(hp_{i}) + \\varepsilon_{i}\\) When hp increases by 1 per cent, mpg increases by -0.53 percent. In Jamovi: Regression &gt; Linear regression after transforming response and predictor variables. In R: lm(log(y) ~ log(x), data) 10.2 Polynomials Application of polynomials usually improves the fit of a model when a curve rather than a straight line describes a relationship. We choose the degree of a polynomial depending on how many curves best express the relationship. For example, if there is a single curve, the relationship is quadratic and we should use the 2nd degree polynomial as follows: \\(Y = \\alpha + \\beta_{1} x + \\beta_{2} x^2 + \\varepsilon.\\) See Crawley (2013) sections 7.1.4 on polynomial functions and 10.3 on polynomial regression. Note that as a result of including polynomials the coefficients for the respective predictor are estimated more than once, so there is no reasonable way to interpret the coefficients. In Jamovi: Regression &gt; Linear regression after transforming predictor variable. In R: lm(y ~ x + I(x^2), data) 10.2.1 Ramsey RESET test The test can be used to determine if adding polynomials to a model improves fit of the model or not. We simply do an F-test to determine whether or not there is a difference between a reduced model \\(y = \\alpha + \\beta x + \\varepsilon\\) and a full model \\(y = \\alpha + \\beta_1 x + \\beta_2 x^2 + \\varepsilon\\). Empirically, we are testing if residuals (\\(RSS\\)) of the two models are different or not: \\(H_0: RSS_1 = RSS_2\\) \\(H_1: RSS_1 \\neq RSS_2.\\) References "],["multiple-linear-regression.html", "Section 11 Multiple linear regression 11.1 Multiple predictors 11.2 Predictor selection 11.3 Multicollinearity", " Section 11 Multiple linear regression In addition to modeling the relationship between two variables, we can use regression to estimate models with multiple predictors. An OLS model with more than one predictor is thus called multiple linear regression. See Navarro and Foxcroft (2018) section 12.5 for an introduction and Crawley (2013) section 10.13 for a more detailed explanations. 11.1 Multiple predictors The estimation of a least squares model with multiple predictors is very similar to single linear regression. The only difference in estimation is that an additional term (\\(\\beta_i x_i\\)) for each predictor is added to the model. A model with two predictors can be expressed as follows: \\(y = \\alpha + \\beta_{1} x_1 + \\beta_{2} x_2 + \\varepsilon,\\) where now \\(\\beta_1\\) is a coefficient of \\(x_1\\) and \\(\\beta_2\\) is a coefficient of \\(x_2\\). The interpretation and diagnostics of the model is now more complicated as now we need to consider both predictors. The addition of a second term means that our model is no longer a line but rather a pane (see Figure 12.14 in Navarro and Foxcroft (2018)). This entire pane is now affected by multiple variables which means that \\(\\beta_1\\) is influenced by the addition of \\(\\beta_2\\). Why use multiple predictors? This allows to control the effects of different variables on the response. Also, adding meaningful predictors improves the fit of the model and thus results in more accurate predictions. 11.2 Predictor selection Constructing a model involves choosing which predictors to include. To get a quick understanding of how variables are related, first take a look at pairwise scatterplots. In Jamovi: Regression &gt; Correlation Matrix | Correlation Matrix. In R: pairs(data) How do we know which predictors to include and which to omit? We should start with estimating a model with all the predictors that we theoretically expect to have an effect on the response and then move on to empirical considerations. While there is no general consensus about what to do with coefficients that are not statistically significant, from an empirical point of view these should be omitted from a model. A traditional approach to testing combinations of predictors would be to examine statistical significance of coefficients but this would lead to the multiple comparisons problem and is not suggested anymore. Instead, we can use a parameter such as the Akaike information criterion (AIC) and apply backward elimination or forward selection to determine which combination of predictors results in the best model. See Hastie, Tibshirani, and Friedman (2017) section 3.3 on subset selection methods. 11.2.1 Akaike information criterion For linear regression the information criterion is calculated as follows: \\(AIC = n \\times log(\\frac{RSS}{n}) + 2 K,\\) where \\(n\\) is the number of observations, \\(RSS\\) the residual sums of squares and \\(K\\) the number of predictors in a model. The model is higher if model performs worse and has more predictors. Thus, lower value of AIC indicates a better model. Note that values of AIC can only be compared for models that have the same response variable. See Navarro and Foxcroft (2018) section 12.11. 11.2.2 Model comparison with F-test Model fit can be estimated in the hypothesis testing framework using F-test. This is often used to estimate a general model fit as it allows us to compare the statistical significance of difference between two models. The F-statistic is calculated as follows: \\[F = \\frac{RSS_1 - RSS_1 / k}{RSS_2/(n-p-1)},\\] where \\(RSS_1\\) is the residual sum of squares of reduced model and \\(RSS_2\\) the same for full model, \\(k\\) the difference in the number of parameters between two models, \\(p\\) the number of parameters and \\(n\\) the number of observations. The test statistic is estimated on F-distribution with \\(df_1 = k\\) and \\(df_2 = n-p-1\\). For example, if our reduced model is \\(y = \\alpha + \\varepsilon\\) and full model \\(y = \\alpha + \\beta_{1} x_1 + \\beta_{2} x_2 + \\varepsilon\\), then we would test whether or not the two terms improve model fit compared to just the intercept (\\(E(y)\\)) and our hypotheses would thus be as follows: \\(H_0: \\beta_1 = \\beta_2 = 0\\) \\(H_1: \\beta_1 \\neq 0 \\text{ or } \\beta_2 \\neq 0\\) 11.3 Multicollinearity In addition the assumptions described in the previous section, multiple predictors introduce the possibility of multicollinearity. The idea is that it should not be possible to linearly predict any of the predictors from others predictors, i.e. predictors should not be (highly) correlated. Otherwise the coefficients are not reliable. Multicollinearity can be detected with variance inflation factor (VIF) by using \\(R^2\\) to estimate for each predictor how much of the variation in one predictor can be predicted from others. \\[VIF_{k} = \\frac{1}{1 - R^{2}_{k-1}},\\] where \\(R^{2}_{k-1}\\) is \\(R^{2}\\) for a model that has predictor \\(k\\) as a response variable and all other predictors as predictor variables. Often VIF value of 5 is considered to be too high and indicate that some predictors should omitted from the model. References "],["logistic-regression.html", "Section 12 Logistic regression 12.1 Generalized linear models 12.2 Maximum likelihood estimation 12.3 Interpretation of coefficients 12.4 Model fit", " Section 12 Logistic regression An important assumption of linear regression models is the normality and constant variance of residuals. With some particular types of response variable these assumptions about model errors can not be met. An example of such variable would be binary variables, i.e. variables that can take only two values (\\(0\\) and \\(1\\)). A common way to model relationships with a binary response variable is logistic regression. 12.1 Generalized linear models The reason why the assumptions regarding the residuals are often not met using ordinary linear model is the structure or errors. Thus, it is necessary to define an error structure prior to estimation. In case of binary data (proportions) we can use binomial distribution to obtain a suitable error structure. Binomial distribution can be used to represent the proportion of successes at a particular number of trials. A distribution of errors can not be applied when estimating a model using the idea of least squares. Instead, the estimation needs to be applied more generally, hence the name general linear models (GLM). See section 13 in Crawley (2013). 12.1.1 Logit link In order to apply a particular distribution on model errors in the framework of GLM, we need to use a link function that relates the mean value of response to predictor variable(s). Link function transforms the mean of response variable. Note that we are not transforming the values themselves but using a function of mean value of response. There is no direct relationship between response and predictor variables, which is why we can not use least squares estimation. The canonical link function for binomial errors is the logit link (in econometrics it is often common that probit link is used instead). The logit link is mathematically expressed as follows: \\[ln(\\frac{p}{1 - p}) = \\alpha + \\beta x,\\] where \\(p\\) is the probability that \\(y = 1\\). In addition to the assumptions of normality and consistency of errors, the logit link also takes into consideration the fact that proportions are bounded to values between 0 and 1. See Crawley (2013) section 13.3 on link function and section 13.4 on proportion data and binomial errors. 12.2 Maximum likelihood estimation The parameters of a GLM can not be simply calculated using the least squares method. Instead, we need to use the maximum likelihood (ML) estimation. See Crawley (2013) section 7.3.3 for an idea behind the ML. It’s a little bit complicated idea but can be summarized as follows: models with various parameters are iteratively fitted until such parameters are found for which the data we have is most likely. While the value of this likelihood can not be directly interpreted, it is used to calculate measures of model fit (see below). In Jamovi: Regression &gt; 2 Outcomes. In R: glm(y ~ x, data, family = binomial(link = 'logit')) 12.3 Interpretation of coefficients Due to the transformation of response variable, the interpretation of coefficients of logistic regression with the logit link is somewhat obscure. Recall that the form of the response variable is \\(log(\\frac{p}{1 - p})\\), where \\(p\\) is the probability that \\(y = 1\\) and hence \\(1-p\\) the probability that \\(y \\neq 1\\). Therefore, the coefficients in logistic regression model represent increase in logged odds of \\(y=1\\) when predictor variable increases by one unit. We can also exponentiate the model equation above. Then \\(\\frac{p}{1-p} = e^{\\alpha + \\beta x}\\) which means that one unit of increase in predictor multiplies the odds of \\(y = 1\\) by \\(e^\\beta\\). As this demonstrates, there isn’t an intuitive way to interpret the coefficients. Still, we can interpret the direction and statistical significance. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Coefficients | Odds ratio. 12.4 Model fit 12.4.1 Deviance When fitted values are determined as a result of ML, using residuals is not straightforward (see Crawley (2013) section 13.11). A ML equivalent of RSS in case of OLS is deviance that can also be used to assess the goodness of fit of a LGM. Deviance expresses the difference in log-likelihood between current and saturated model. For binomial error structure the deviance is expressed as \\[D = 2 \\sum y ln(y/\\mu) + (n - y) ln(n - y) / (n - \\mu),\\] where \\(y\\) are the observed values of response, \\(\\mu\\) the mean value of response and \\(n\\) the number of observations. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Fit | Deviance. 12.4.2 Pseudo \\(R^2\\) Because ML does not involve the minimization of squared residuals, it is not suitable to use residuals via \\(R^2\\) to determine model fit. Still, there are equivalent pseudo-\\(R^2\\) measures for expressing model fit for ML that are calculated using likelihoods. While there are several such measures, most common is the McFadden’s pseduo-\\(R^2\\): \\[R^2 = 1 - \\frac{ln\\theta_{reduced}}{ln\\theta_{full}},\\] where \\(\\theta_{reduced}\\) is the likelihood of model with only intercept and \\(\\theta_{full}\\) likelihood of model with predictors. As such, the measure illustrates how much does the addition of predictors improve the model. The values lie between 0 and 1 and usually values \\(&gt;0.4\\) are considered to indicate good fit. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Fit | McFadden's R^2. 12.4.3 Likelihood ratio test This test can be used to determine if a complex model is different from a simpler one. This can be used to estimate the overall model fit. The test statistic is \\[LR = -2ln(\\frac{\\theta_{reduced}}{\\theta_{full}}),\\] where \\(\\theta_{reduced}\\) is the likelihood of simpler model and \\(\\theta_{full}\\) the likelihood of complex model. The hypotheses are as follows: \\(H_0: L_{reduced} = L_{full}\\) \\(H_1: L_{reduced} \\neq L_{full}.\\) If difference can not be shown, then the simpler model should be preferred. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Coefficients | Likelihood ratio tests. 12.4.4 Overdispersion Unobserved omitted variables inflate residual deviances resulting in difficulties establishing the significance of predictors. See Crawley (2013) section 13.12 for more. 12.4.5 Classification The model fitted values of response variable lie between 0 and 1. These can be interpreted as probabilities of \\(y = 1\\) for each observation. In case we wish to have predictions on original binary scale, we need to choose a cutoff point that determines if a probability should be classified as 0 or 1. The classified predictions can be tabulated against true classes which results in a classification table or a confusion matrix. Such table can then be used to calculate measures such as accuracy, specificity, sensitivity, AUC and others that illustrate how good the model is at predicting. In Jamovi: Regression &gt; 2 Outcomes &gt; Prediction. References "],["principal-component-analysis.html", "Section 13 Principal component analysis 13.1 Intuition 13.2 Finding the PCs 13.3 Calculation 13.4 Number of components 13.5 Interpretation", " Section 13 Principal component analysis Principal component analysis (PCA) is a method in the field of multivariate statistics. In such methods we do not define any causality between variables by distinguishing between response and predictor variables. Instead, methods in multivariate statistics look at the overall structure of data and attempt to identify patterns. PCA allows to find such patterns that could be used to reduce the number of variables without losing too much information. This can be useful e.g. when there are a lot of correlated predictors in a regression model. See Crawley (2013) section 25.1 and Navarro and Foxcroft (2018) section 15.2. 13.1 Intuition The underlying goal of PCA in simple terms is to summarize many variables into one, few or several new variables. More precisely, the aim is to determine a set of standardized linear combinations that would explain a maximum amount of variation in data. Sometimes only one linear combination is found such that it best separates observations. As such, PCA is a dimensionality reduction technique. Each linear combination that summarizes part of variance in data is called a principal component (PC). These linear combinations \\(\\xi_j\\) can be expressed as \\(\\xi_j = b_{j1}x_1 + b_{j2}x_2+ ... a_{jp}x_p\\), where \\(b_{jp}\\) is a weight of variable \\(x_p\\) of a particular \\(\\xi_j\\), i.e. \\(j\\)th PC. PCs are thus essentially weighted sum of variables. The ordering of PCs is fixed: PCs are ordered decreasingly, starting from the PC that explains the most variation in data. The maximum number of PCs we can determine is \\(min(p, n - 1)\\). 13.2 Finding the PCs There are several ways to understand the estimation PCs. An intuitive approach would be to visualize a data cloud from which we incrementally derive the PCs. The first PC should explain the maximum amount of variation in the data cloud. Thus, we choose the first PC1 so that (1) it follows the direction of largest variance in data cloud and (2) is then also a line with smallest orthogonal distance to all data points. As such the idea is similar to least squares estimation, except that in PCA we consider variation in all variables (not just the response) and do not use squared residuals. See the relevant question on Stack Exchange for some visual explanations. Establishing the direction of largest variation requires transforming data points to a new coordinate system. This involves the following steps: (1) centering data points, (2) scaling the axes so that they would be equal, (3) rotating axes into the direction of a PC. In the last step the first PC is rotated so as to maximize variation, whereas each following PC is rotated to be orthogonal to preceding PC. 13.3 Calculation Estimation of PCs requires data to be centered, i.e. for each variable \\(\\mu = 0\\). This is usually done by software. Another aspect to consider is scale of data. If software does calculations on correlation matrix then scale is irrelevant because correlation matrix is scale agnostic. However, if covariance matrix is used then variables with higher variance are given more importance (recall that PCA involves maximizing variation). Whether or not this is preferred depends on data and research problem. In Jamovi: Factor &gt; Principal Component Analysis | Rotation: none. In R: prcomp(data, scale = TRUE) The transformation of data points into a new coordinate system is done according to the eigenvalues and eigenvectors derived from data. There are two methods for obtaining these: (1) using covariance or correlation matrix of data or (2) using raw values. These methods are described below. 13.3.1 Spectral decomposition A data matrix \\(X : n \\times p\\) can be represented by its correlation or covariance matrix \\(\\Sigma\\). This can be decomposed as \\(\\Sigma = V D V^T\\), where \\(D = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_{n-1})\\) is a diagonal matrix of eigenvalues of \\(\\Sigma\\) and matrix \\(V = (v_1, v_2, ..., v_{n-1})\\) the corresponding eigenvectors of \\(\\Sigma\\). This can also be expressed as \\[\\Sigma w_i = \\lambda_i w_i\\] where \\(\\lambda_i\\) is the eigenvalue that corresponds to eigenvector \\(w_i\\) of \\(i\\)th PC. 13.3.2 Singular Value decomposition A data matrix \\(X : n \\times p\\) with \\(rank(X) = k\\) can also be decomposed as \\(X = UDV^T\\), where \\(U = (u_1, u_2, \\dots, u_k)\\) is an \\(n \\times n\\) vector of eigenvectors of \\(XX^T\\), \\(V = (v_1, v_2, ..., v_k)\\) is an \\(p \\times p\\) vector of eigenvectors of \\(X^TX\\), and \\(D = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_k)\\) is a diagonal matrix containing nonzero eigenvalues of \\(X^TX\\) and \\(XX^T\\). Such decomposition of our data matix \\(X\\) then gives us the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\) and corresponding eigenvectors \\(u_1, u_2, \\dots, u_k\\) for \\(X\\). 13.4 Number of components Recall that the aim of PCA is to summarize variables into a smaller number of PCs. The number of PCs that are used is decided by user. Depending on our research problem, we might wish to obtain just one PC or few or more PCs. There also several possible criteria that may help us make the choice: choose a threshold of variation in data that PCs should cumulatively explain, e.g. 80%; use PCs that have eigenvalue above average or above one; use PCs that are above an “elbow” in scree plot. In Jamovi: Factor &gt; Principal Component Analysis | Scree plot. 13.5 Interpretation 13.5.1 Variation explained The proportion of total variation explained by a \\(j\\)th PC can be found from its eigenvalues: it is the proportion of sums of eigenvalues, i.e. \\(\\lambda_j / \\Sigma^p_{j=1} \\lambda\\). The proportion of variation explained by first PCs can be found by cumulative sum of the respective values. This measure illustrates how much information is preserved when summarizing data into a particular number of PCs. In Jamovi: Factor &gt; Principal Component Analysis | Component summary. In R: prcomp(data, scale = TRUE) 13.5.2 Loadings Each PC is defined by the influence that each variable has on it. As such, each variable has a particular loading on each PC which is why the values of this influence are called loadings. Loadings can also be understood as variable weights. When the linear combinations are calculated for each observation using loadings (weights) and initial variables, we obtain principal component scores. These are in essence the new summarized variables. Loadings can be used to interpret PCs to give them a theoretical meaning. This is a rather creative process. In Jamovi: Factor &gt; Principal Component Analysis | Rotation: none &gt; Save | Component scores. In R: prcomp(data, scale = TRUE)$x 13.5.3 Biplots These plots concisely summarize the first two PCs by depicting observations on these two dimensions together with loadings of each variable as arrows. Directions of these arrows indicate the direction of loading and length of arrows indicate the magnitude of loading. Then, the angle between arrows represents correlations between variables. In Jamovi: snowCluster &gt; PCA plot | Biplot. In R: biplot(prcomp(data, scale = TRUE)) References "],["slides.html", "Section 14 Slides", " Section 14 Slides Here is an index of slides used to explain the methods during meetings. The essential information on the slides is covered in the course notes but slides might add some explanations and visuals using example data sets. On slides, click h to see keyboard shortcuts for navigation. Introduction Descriptive statistics Hypothesis testing. Comparing categorical variables Comparing numerical variables Analysis of variance Correlation analysis Regression. Simple linear regression "],["references.html", "Section 15 References", " Section 15 References "]]
