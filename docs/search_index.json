[["index.html", "Research methods Section 1 Syllabus 1.1 Teaching 1.2 Scoring 1.3 Topics", " Research methods Section 1 Syllabus These course notes include information and study materials on the quantitative part of MS.0825 Research methods course of Agri-Food Business Management Master’s program. 1.1 Teaching Flipped classroom approach is used for teaching. This means that students are expected to learn the methods before meetings. During the meetings we revise the theoretical material, address any questions and apply methods in practice. After meetings students apply the methods on their own datasets. 1.1.1 Schedule Meetings take place on Mondays, Wednesdays and Thursdays at 9 o’clock in the morning. Link to meetings is provided under the course page in Moodle. 1.1.2 Aims After completing the course students should be able to … … understand basic concepts in statistics; … know how to describe data, both numerically and visually; … choose an appropriate method to solve a problem; … use a statistical package for data analysis; … communicate results (interpret, explain, present); … learn about statistical methods individually; … find the courage to apply statistical methods. 1.2 Scoring This part of the course gives 50% of total course points. This 50% consists of 50 points that can be obtained by submitting: feedback on reading material for upcoming meeting (10*1=10 points), and report containing the application of methods learned (10*4=40 points). To gain the 40 points students are required to combine the applications of a method into a report. Participation in meetings does not affect points but meetings are not recorded. 1.2.1 Feedback on reading material Submit your questions or thoughts on given reading material before each meeting in Moodle. Feedback can be either an explanation of something you learned, a question on something you don’t understand, or an answer to another student’s question. Compulsory reading is from two books: Navarro and Foxcroft (2018) and Crawley (2013). Each part of feedback is graded as follows: 0 points - Feedback is missing or lacking 1 point - Feedback demonstrates that student has revised relevant material 1.2.2 Research project Students are required to create a research report as follows: Choose at least one data set Apply at least one method from 10 topics on this data set (i.e. you can skip 2 topics) Make changes according to feedback Present the results as a written report Each of the 10 topics is graded as follows: 0 points - Method from a particular topic is not applied 1 point - Method is applied but does not suit data 2 points - Chosen method suits data but is incorrectly applied 3 points - Method is correctly applied but conclusions are incorrect or missing 4 points - Method is correctly applied and conclusions are correct 1.2.2.1 Datasets To apply methods, datasets need to have multiple variables. We mostly learn to explore continuous data but some methods require at least one discrete variable. Students are free to choose a dataset. Possible sources for data: Jamovi: Open &gt; Data Library Goodle dataset search Kaggle datasets Data hub collections Our World in Data World Values Survey European Social Survey 1.3 Topics We cover some common and traditional statistical methods. Descriptive statistics Hypothesis testing. Comparing categorical variables Comparing numerical variables Analysis of variance Correlation analysis Regression. Simple linear regression Regression. Multiple linear regression, factor variables and transformations Regression. Maximum likelihood and binary models Principal component analysis Factor analysis Hierarchical clustering K-means clustering "],["introduction.html", "Section 2 Introduction 2.1 Quantitative methods 2.2 Statistics 2.3 Software 2.4 Data management", " Section 2 Introduction 2.1 Quantitative methods 2.1.1 Quantitative and qualitative methods QuaNTitative QuaLitative Numeric data Semantic data Large-N Small-N, case studies Generalize Explore Measure and test Understand Statistical analysis Interpretation 2.2 Statistics See Navarro and Foxcroft (2018) sections 1.1.1 and 1.5 for a brief introduction. Sections 2.3-2.7 also provide useful background on validity and reliability of statistical methods. Dash (2016) has summarized statistics as follows: Figure 2.1: Flowchart of statistics 2.2.1 Descriptive and inferential statistics Descriptive statistics Inferential statistics Data on entire population Only a sample of a population Simple measures (e.g. mean) Point estimates and intervals Describing Generalizing 2.2.2 Frequentist and bayesian approach to statistics Frequentist Bayesian Traditional Modern What’s the probability of data given some estimate? What’s the probability of an estimate? Prior is not relevant Estimates are conditional on priors Single parametric inference Multiplication of an inference to get a posterior estimate 2.2.3 Statistics and data science, machine learning, artificial intelligence, … Statistics DS, ML, AI, … Less data Big data Clean datasets Untidy data in various formats Traditonal methods Novel methods Mathematics and calculations Programming approach Aim is to explain Aim is to predict 2.3 Software 2.3.1 Commonly used software Spreadsheet vs statistical software Spreadsheet - table management (e.g. Microsoft Excel, Google Sheets, LibreOffice Calc) SPSS - simple GUI-based app with limited functionality R - extensible programming language designed for data analysis Stata - Mostly CLI-based, well suited for econometrics 2.3.2 Jamovi To apply the methods we learn, we use Jamovi. Jamovi is a new “3rd generation” statistical spreadsheet. designed from the ground up to be easy to use, jamovi is a compelling alternative to costly statistical products such as SPSS and SAS. Jamovi is built on top of the R statistical language, giving you access to the best the statistics community has to offer. You can try it online without installing at https://cloud.jamovi.org/. 2.3.3 R language and RStudio This allows us to get under the hood of Jamovi so to speak. R language is based on S language orginating from 1970’s. Developed during 1990’s and became public around 2000. R is a language and environment for statistical computing and graphics. R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, …) and graphical techniques, and is highly extensible. 2.4 Data management 2.4.1 Data structure Usually data is stored in 2-dimensonal tables, i.e. data has rows and columns. To apply statistical procedures, most software requires data to be formatted in a conventional way. In this respect it’s useful to follow “tidy data” principle (Wickham 2014): Every column is a variable. Every row is an observation. Every cell is a single value. This is how data should be formatted before running most statistical procedures. One way to tidy a data table is to use the Pivot table feature in a spreadsheet app. 2.4.2 Scales of measurement Scale of measurement determines the procedures that can be applied to the data. The following typology was first published by Stevens (1946). See Navarro and Foxcroft (2018) section 2.2 for a more detailed explanation. Each scale in the table includes also the properties, operations and measures a on previous rows. Scale Property Operations Central tendency Nominal Classification =, ≠ Mode Ordinal Level &gt;, &lt; Median Interval Difference +, − Arithmetic mean Ratio Magnitude ×, / Geometric mean, harmonic mean 2.4.2.1 Nominal scale Nominal variables contain names (i.e. characters, factors or strings) that do not have a natural order. Such data can be summarized only by counting values or determining the mode. 2.4.2.2 Ordinal scale Ordinal variables have the characteristics of nominal variables with the added possibility of naturally ordering these names. As such, ordinal variables can be said to have levels. It’s important to note that an increase of one level to the next is not necessarily numerically equivalent to an increase of another level to the next. Thus, it is not always meaningful to convert these levels to numbers and do calculations on them. 2.4.2.3 Interval scale Interval variables are expressed numerically. While differences between numbers on interval scale are meaningful, there isn’t a natural zero value. Thus, calculations on interval variables is limited to finding differences and division or multiplication of such values is not reasonable. A classic example of an interval variable is temperature. Difference between 5°C and 15°C is 10°C but 15°C is not 3 times warmer than 5°C. 2.4.2.4 Ratio scale Ratio variables are also numeric but have a natural zero value. This means that division and multiplication is meaningful. 2.4.2.5 Binary scale Binary (or dichotomous) variables do not constitute a distinct scale but can be considered as a subgroup of nominal, ordinal or interval variables. Binary variables can only take two values. Many statistical procedures convert or require the conversion of nominal variables to binary for technical reasons (e.g. “dummy” variables). Binary variables are often coded as 0 for false and 1 for true. As such, calculating the sum or mean of binary variables conveys useful information (think why that is!). 2.4.2.6 Continuous or discrete variables In addition to the previous typology, we can also distinguish between continuous and discrete variables. These are well defined by Navarro and Foxcroft (2018, 20): \"A continuous variable is one in which, for any two values that you can think of, it’s always logically possible to have another value in between. A discrete variable is, in effect, a variable that isn’t continuous. For a discrete variable it’s sometimes the case that there’s nothing in the middle.\" 2.4.2.7 Scales in statistical software The difference between ratio and interval variables is only theoretical. Statistical software treats all numbers as numeric variables and does not distinguish between interval and ratio variables. When binary variables are coded so that they only have values 0 and 1, these are also considered numerical. A lot of statistical software (e.g. Jamovi) distinguish only between three types of variables: nominal, ordinal and numeric. "],["descriptive-statistics.html", "Section 3 Descriptive statistics 3.1 Central tendency 3.2 Variability 3.3 Plots", " Section 3 Descriptive statistics Any data analysis should begin with an exploration of the structure of and values in dataset. Description of values should also be part of the reporting of analysis. This can be done using some simple methods. When we use the methods of descriptive statistics, we need to be aware of that we merely describe the dataset at hand. If observations in the data set constitute a sample, we cannot draw any wider conclusions on population using these methods. Below we are thus defining the statistics for populations and not samples. Description of nominal or ordinal variables is simple and limited to just counting unique values and finding the mode and/or median value. But suppose we have variables that contain sequences of numbers. How can we describe these? 3.1 Central tendency A concise way to represent numeric values is expressing the center of values. This can also be understood the typical value or expected value (expectation of \\(x\\), i.e. \\(E(x)\\)). Note that estimating a central tendency is only meaningful if there is a natural center. i.e. the distribution of values is unimodal. See Navarro and Foxcroft (2018) section 4.1. 3.1.1 Mode Mode is simply the most frequent value in a sequence. This statistic can be useful for nominal and ordinal variables but it’s rarely used for numeric variables, especially on a continuous scale. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Mode 3.1.2 Median When sequence of values is ordered, the middle value is median. In case there are even number of values (\\(N\\)) in a sequence, median is the average of the two values in the middle. Thus, there are equal number of values above and below the median value. When \\(N\\) is an even number, half of the values are above and half below the median value. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Median In R: median(x) 3.1.3 Mean While there are several forms of mean, arithmetic mean is most useful in statistics and also considered here. Mean is in other words the average value: it’s the sum of values (\\(X\\)) divided by number of values: \\[\\bar{X} = \\frac{1}{N} \\sum{^N_{i=1}}{X_i}\\] A feature of mean value is that if you only know the \\(\\bar{X}\\) and \\(N\\), you can still calculate \\(\\sum{^N_{i=1}}{X_i}\\). In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Mean In R: mean(x) 3.2 Variability Central tendency describes values only partially as it tells nothing about the deviation of values from that mean. Estimating also the variability of values provides a more manifold understanding of values. See Navarro and Foxcroft (2018) section 4.2. 3.2.1 Range Most simple way to express variability is to indicate the range of all values, i.e. the difference between minimum and maximum values. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Range, Minimum, Maximum In R: range(x) 3.2.2 Quantiles and interquartile range Quantiles are in essence calculated just like median, because median is the lowest quantile, the 2-quantile. In addition to dividing sequence of ordered values into two halves separated by median, such sequence can be diveded into any number of equal groups as long as the number of groups (\\(k\\)) is smaller than or equal to the number of values, i.e. \\(k \\le N\\). Some common quantiles are quartiles (\\(k = 4\\)), deciles (\\(k = 10\\)) and centiles/percentiles (\\(k = 100\\)). In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Percentile Values In R: quantile(x) Arguably quartile is most frequently used among quantiles because the 2nd and 4th quartiles contain half of all values. This interquartile range (IQR) thus contains the middle half of all values. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | IQR In R: IQR(x) 3.2.3 Variance Although difficult to interpret, variance is used in many statistical calculations due to its mathematical properties. Variance is mean squared difference from the mean value: \\[Var(X) = \\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}\\] In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Variance In R: var(x) 3.2.4 Standard deviation Interpretability issues of variance arise from the fact that the calculation includes squaring and thus the value of the statistic is vastly different to understand regarding initial scale. A solution would be to find a square root of variance. This is what the standard deviation is: the square root of variance. \\[\\sigma = \\sqrt{\\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}}\\] In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Std. deviation In R: sd(x) 3.3 Plots Previously explained measures allow us to only summarize values into fewer numbers. Plotting allows to present these values more intuitively or even depict all of the values at once. 3.3.1 Boxplot Boxplots illustrate the quartiles of values (including interquartile range) and extreme values (outliers). Outliers are usually values that are below the 2nd quartile or above the 4th quartile by 1.5 times the interquartile range. On boxplots, half of all the values lie within the box and another half on the lines, outliers excluded. See Navarro and Foxcroft (2018) section 5.2. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Box Plots In R: boxplot(x) 3.3.2 Histogram and density plot Histograms are essentially barplots where all values are divided between ranges of equal with, i.e. bins. Instead of bins, distribution of values can also be expressed continuously by smoothing the bins. This results in a density plot. See Navarro and Foxcroft (2018) section 5.1. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Histograms In R: histogram(x) and plot(density(x)) "],["hypothesis-testing.html", "Section 4 Hypothesis testing 4.1 Sample and population 4.2 Hypothesis testing", " Section 4 Hypothesis testing 4.1 Sample and population Distinguishing between sample and population is important when we wish to draw inferences about population when we only have data on a sample. 4.1.1 Sample, population, sampling See Navarro and Foxcroft (2018) section 8.1. What is the difference between population, sample and (simple) random sample? 4.1.2 Estimating population parameters 4.1.2.1 Population mean See Navarro and Foxcroft (2018) section 8.4.1. How do you get population mean if you only know the sample mean? 4.1.2.2 Population standard deviation See Navarro and Foxcroft (2018) section 8.4.2. \\[\\hat{\\sigma} = \\sqrt{\\frac{1}{n-1} \\sum{^n_{i=1}{(x_i-\\bar{x})^2}}}\\] Why and how is sample standard deviation different from population standard deviation? 4.1.3 Confidence intervals See Navarro and Foxcroft (2018) section 8.5 A 95% CI of the mean \\(X \\sim N(\\mu,\\sigma^2)\\) is calculated as \\[CI_{95} = \\hat{x} \\pm(1.96\\times\\frac{\\sigma}{\\sqrt{N}})\\] What is the interpretation of confidence interval? 4.2 Hypothesis testing 4.2.1 Null and alternative hypotheses See Navarro and Foxcroft (2018) section 9.1.2. Statistical hypothesis testing involves two hypotheses: null (\\(H_0\\)) and alternative (\\(H_1\\)) hypothesis. These are usually defined as follows: \\(H_0\\): There is no statistically significant difference. \\(H_1\\): There is a statistically significant difference. We always test if we can reject \\(H_0\\). If we reject \\(H_0\\), then we accept \\(H_1\\). If we fail to reject \\(H_0\\), then we accept \\(H_1\\). 4.2.2 Types of errors See Navarro and Foxcroft (2018) section 9.2. We accept \\(H_0\\) We reject \\(H_0\\) \\(H_0\\) is true We are correct We commit type I error \\(H_0\\) is false We commit type II error We are correct The rate of type I error is considered the significance level of a test (denoted as \\(\\alpha\\)). 4.2.3 Test statistic See Navarro and Foxcroft (2018) sections 9.3 and 10.1.6. We use a test statistic to make a decision whether we can or can not reject \\(H_0\\). We reject \\(H_0\\) if the value of a test statistic is sufficiently extreme, i.e. different from 0. 4.2.4 P-value See Navarro and Foxcroft (2018) sections 9.5 and 9.6. P-value is the probability of obtaining at least as extreme value of test statistic if \\(H_0\\) is correct. Thus, it’s the rate of committing a type I error, i.e. the significance level \\(\\alpha\\). If the p-value is below \\(\\alpha\\), we reject \\(H_0\\) and accept \\(H_1\\). \\(p \\ge \\alpha \\implies H_0\\) \\(p \\lt \\alpha \\implies H_1\\) P-value is not the probability of \\(H_0\\) being false or \\(H_{1}\\) being true. Theoretically, a lower p-value does not show a greater difference or vice versa. Only it’s location relative to is what \\(\\alpha\\) matters. 4.2.5 Multiple comparisons problem Every time you find a statistically significant result, there is a possibility that you got this extreme data by chance. In this case, you committed a type 1 error. Therefore, if you do a lot of tests, the possibility that at least one of the statistically significant results is actually not true. Thus, the number of tests you do should be limited. "],["comparing-categorical-data.html", "Section 5 Comparing categorical data 5.1 Contingency tables 5.2 \\(\\chi^2\\)-test 5.3 Degrees of freedom", " Section 5 Comparing categorical data 5.1 Contingency tables For analyzing categorical data we can compare the counts of categories. This gives us contingency tables where cells indicate how many times each value appears in a categorical variable. When we compare more than one variable, contingency tables have more than one dimension. Usually we tabulate two variables against each other. If these variables have \\(m\\) and \\(n\\) unique values (categories), then we obtain a 2-dimensional \\(m*n\\) contingency table. 5.2 \\(\\chi^2\\)-test 5.2.1 Goodness of fit \\(\\chi^2\\)-test For goodness of fit \\(\\chi^2\\)-test see Navarro and Foxcroft (2018) section 10.1.3. We test if frequencies of categories are different from some expected frequencies for a single variable: \\(H_0\\): Frequencies of categories are as expected. \\(H_1\\): Frequencies of categories are different from what is expected. The test statistic is calculated as \\[\\chi^2 = \\sum{^k_{i=1}{\\frac{(O_i-E_i)^2}{E_i}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency of \\(i\\)th category (\\(k\\)). In Jamovi: Frequencies &gt; N Outcomes In R: chisq.test(x) 5.2.2 \\(\\chi^2\\)-test of independence For \\(\\chi^2\\)-test of independence see Navarro and Foxcroft (2018) section 10.2. In this case we test if two variables are associated: \\(H_0\\): Variables are independent. \\(H_1\\): Variables are associated. The test statistic is calculated as \\[\\chi^2 = \\sum{^r_{i=1}\\sum{^c_{j=1}}{\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency in \\(i\\)th row (\\(r\\)) and \\(j\\)th column (\\(c\\)). An assumption of \\(\\chi^2\\)-test is that there are \\(&gt;5\\) observations in each cell of a contingency table. In Jamovi: Frequencies &gt; Independent Samples In R: chisq.test(x, y) 5.3 Degrees of freedom See Navarro and Foxcroft (2018) section 10.1.5 and pages 227-228. Degrees of freedom (DoF) indicates the number of values that are free to vary. This is relevant for evaluating the \\(\\chi^2\\)-test statistic as it indicates the critical value on \\(\\chi^2\\)-distribution. For goodness of fit \\(\\chi^2\\)-test DOF is calculated simply as \\(df = k - 1\\) where \\(k\\) is the number of categories. For \\(\\chi^2\\)-test of independence DoF is calculated as \\[df = (r - 1)(c - 1),\\] where \\(r\\) is the number of rows and \\(c\\) the number of columns in a contingency table. "],["comparing-numerical-data.html", "Section 6 Comparing numerical data 6.1 One or two samples 6.2 Unpaired or paired samples 6.3 One-tailed or two-tailed tests 6.4 Parametric or nonparametric tests 6.5 How to decide which test to use?", " Section 6 Comparing numerical data Statistical testing for numerical data is different from when we have categorical data as we are no longer limited to using frequencies but parameters, ranks and other measures. However, note that nonparametric tests explained at the end can also be used for ordinal variables. This section covers only comparisons with one or two samples. Comparing multiple mean values is done using analysis of variance (Anova) and is considered in the next section. Also, this section only outlines some classic tests and there are many more. 6.1 One or two samples 6.1.1 One sample 6.1.1.1 One sample T-test This test is used to determine if our sample is taken from a population with a given mean. So we test if the sample mean \\(\\bar{x}\\) is equal to a hypothetical population mean \\(\\mu\\). See Navarro and Foxcroft (2018) section 11.2. Hypotheses: \\(H_0: \\bar{x} = \\mu\\) \\(H_1: \\bar{x} \\neq \\mu\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x} - \\mu}{s \\div \\sqrt{n}},\\] where \\(\\bar{x}\\) is the sample mean and \\(\\mu\\) the hypothetical population mean that it is tested against and \\(s\\) sample standard deviation. Test statistic is evaluated on t-distribution with \\(n-1\\) degrees of freedom (df). Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.2.3. In Jamovi: T-tests &gt; One Sample T-test. In R: t.test(x, mu) 6.1.2 Two samples 6.1.2.1 Independent samples T-test assuming equal variances (Student test) This test compares the mean values to determine if these are equal. Equality of means implies that samples come from the same population. The test assumes that variances of the samples are equal. This is true if \\(\\frac{1}{2} &lt; \\frac{s_1}{s_2}&lt;2\\). See Navarro and Foxcroft (2018) section 11.3. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = s_p\\sqrt{\\frac{1}{n_1} +{\\frac{1}{n_2}}},\\] where \\(n_1\\) and \\(n_2\\) are sample sizes and \\(s_p\\) is the pooled standard deviation calculated as \\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}},\\] where \\(s_1\\) and \\(s_2\\) are standard deviations of the samples. Test statistic is evaluated on t-distribution with \\(n_1+n_2-2\\) df. Test assumes normality and independence of data and homogeneity of variance. See Navarro and Foxcroft (2018) section 11.3.7. In Jamovi: T-tests &gt; Independent Samples T-test. In R: t.test(x, y) 6.1.2.2 Independent samples T-test not assuming equal variances (Welch test) This test is equivalent to previously described test but now we don’t assume equal variances. Variances are unequal if \\(s_1 &gt; 2s_2\\) or \\(s_2 &gt; 2s_1\\). See Navarro and Foxcroft (2018) section 11.4. Hypotheses are also the same as in case of Student test \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic is the same as for Student test: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}},\\] where \\(s_1\\) and \\(s_2\\) are unbiased standard deviations of the samples. Test statistic is evaluated on t-distribution where df is calculated as \\[df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(s_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(s_2^2/n_2\\right)^2}{n_2-1}}.\\] Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.4.2 In Jamovi: T-tests &gt; Independent Samples T-test | Welch's. In R: t.test(x, y, var.equal = FALSE) 6.2 Unpaired or paired samples Previous tests assumed independence of samples. This is not true if we have paired values. For instance, if samples contain measurements of same observations in two different points in time, then the values representing same observations are paired. 6.2.0.1 Paired samples T-test See Navarro and Foxcroft (2018) section 11.5. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\hat{D}}{s_\\Delta\\div \\sqrt{n_1 + n_2}},\\] where \\(s_\\Delta\\) is difference in standard deviation expressed as \\(s_\\Delta = s_1 - s_2\\) and \\(\\hat{D}\\) is the mean of differences between paired values calculated as \\[\\hat{D} = \\frac{1}{n} \\sum_{i=1}^i{(X_{i1} - X_{i2})}.\\] Test statistic is evaluated on t-distribution with \\(n-1\\) df where \\(n\\) is the number of pairs. Test assumes normality of data. In Jamovi: T-tests &gt; Paired Samples T-test. In R: t.test(x, y, paired = T) 6.3 One-tailed or two-tailed tests All the tests described above were presented as one-tailed tests. This means that we were not interested in whether the differences are positive or negative. Two-tailed versions of these tests also take this into consideration. See Navarro and Foxcroft (2018) section 11.6. If we wish to test if mean of sample (\\(\\bar{x}\\)) is greater than hypothetical population mean (\\(\\mu\\)), then our hypotheses are the following: \\(H_0: \\bar{x} \\le \\mu\\) \\(H_1: \\bar{x} \\gt \\mu\\) If we wish to test if mean of one sample (\\(\\bar{x}_1\\)) is greater than mean of another sample (\\(\\bar{x}_2\\)) , then our hypotheses are the following: \\(H_0: \\bar{x}_1 \\le \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\gt \\bar{x}_2\\) In Jamovi: T-tests &gt; ... T-test | Group 1 &gt; Group 2 or Group 1 &lt; Group 2. In R: t.test(x, y, alternative = 'less') or t.test(x, y, alternative = 'greater') 6.4 Parametric or nonparametric tests Population parameters (e.g. mean, standard deviance) can be estimated from sample parameters only if we can assume that the distribution of values in population (and thus sample) follows normal distribution. If we can not make assumptions about the distribution or parameters of underlying population values we need to use nonparametric tests. 6.4.1 Normality There are various ways to determine whether or not values are normally distributed or not. Here we look at QQ plots and Shapiro-Wilk test. 6.4.1.1 QQ plot On such plots quantiles of data are plotted against theoretical quantiles representing normal distribution. If quantiles of data are highly correlated to these theoretical quantiles (relationship follows a straight line), then data is normally distributed. Interpretation of QQ plot is thus not precise. See Navarro and Foxcroft (2018) section 11.8.1. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Q-Q. In R: qqnorm(y) 6.4.1.2 Shapiro-Wilk test This test determines if data is normally distributed or not. In other words, it tests if sample comes from a normally distributed population. See Navarro and Foxcroft (2018) section 11.8.2. Test statistic \\(W\\) is calculated as \\[W = \\frac{(\\sum^n_{i=1}{a_ix_i})^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2}\\] The exact explanation of \\(a\\) and thus the logic is complicated but higher value of \\(W\\) indicates non-normality. If the test statistic is statistically significant, then data is not normally distributed: \\(H_0\\): Data is normally distributed \\(H_1\\): Data is not normally distributed In Jamovi: Exploration &gt; Descriptives &gt; Statistics | Shapiro-Wilk. In R: shapiro.test(x) 6.4.2 Nonparametric tests 6.4.2.1 Mann-Whitney U test This is a test to compare distributions (and medians) of two unpaired samples if we can’t assume normality. The test is also known as Wilcoxon rank-sum test. The test is also known as Wilcoxon rank-sum test. See Navarro and Foxcroft (2018) section 11.9.1. Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. Formal definition of \\(H_{0}\\): A randomly selected value from one sample is equally likely to be less than or greater than a randomly selected value from a second sample. Test statistic \\(U\\) is calculated as \\[U = \\sum^n_{i=1} \\sum^m_{j=1} S(X_1, X_2),\\] where \\(n\\) are rows and \\(m\\) columns of a matrix \\(S(X_1, X_2)\\) described as below. \\[S(X_1, X_2) = \\begin{cases} 1 &amp; \\text{if } Y &lt; X\\\\ \\frac12 &amp; \\text{if } Y = X\\\\\\ 1 &amp; \\text{if } Y &gt; X\\ \\end{cases}\\] Basically, we compare all values and count the times when values from one sample are higher than values from another sample. The \\(U\\) is just the count of these differences. For \\(n \\ge 20\\), p-value for \\(U\\) is calculated on a normal distribution. Test assumes independence of samples. In Jamovi: T-tests &gt; Independent Samples T-test | Mann-Whitney U. In R: wilcox.test(x, y) 6.4.2.2 Wilcoxon signed-rank test This is similar to Mann-Whitney U test but used for paired samples. The test is also known as one sample Wilcoxon test. See Navarro and Foxcroft (2018) section 11.9.2 Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. The W statistic is calculated as: \\[W = \\sum^n_{i = 1}(sgn(x_{1i}x_{2i}) \\times R_i),\\] where \\(sgn(x_{1i}x_{2i})\\) is sign function (1 for positive difference, -1 for negative) and \\(R_i\\) is the rank of absolute difference. Basically, we are comparing how different is the ranking of values between two samples. The \\(W\\) is just the sum of ranked differences. For \\(n \\ge 20\\), p-value for \\(W\\) is calculated on normal distribution. Test has no relevant assumptions. In Jamovi: T-tests &gt; Paired Samples T-test | Wilcoxon rank. In R: wilcox.test(x, y) 6.4.2.3 Kolmogorov-Smirnov test This test is equivalent to Mann-Whitney U test, although the calculation is very different. This test compares the overall shape of two distributions using cumulative distribution function. Hypotheses: \\(H_{0}\\): Distributions of both samples are the same. \\(H_{1}\\): Distributions of samples are different. Test statistic \\(D\\) is simply the maximum absolute difference between two cumulative distribution functions. P-value is determined by the extremity of the test statistic \\(D\\) on Kolmogorov distribution. In R: ks.test(x, y) 6.5 How to decide which test to use? One sample Unpaired samples Normally distributed One sample T-test Not normally distributed Wilcoxon signed-rank test Two samples Unpaired samples Normally distributed Equal variance Independent samples T-test assuming equal variances (Student test) Unequal variance Independent samples T-test assuming unequal variances (Welch test) Not normally distributed Mann-Whitney U test Paired samples Normally distributed Paired samples T-test / Wilcoxon rank-sum test Not normally distributed Wilcoxon signed-rank test Three samples Unpaired samples Normally distributed Analysis of Variance "],["analysis-of-variance.html", "Section 7 Analysis of variance 7.1 One-way Anova 7.2 Post-hoc tests 7.3 How to decide which test to use?", " Section 7 Analysis of variance Whereas tests in previous chapters could be used to compare only two parameters we often wish to compare parameters of several samples. We can use Analysis of variance (Anova) to do just that. 7.1 One-way Anova This test is used to compare mean values of three or more samples. Samples are distinguished by a factor variable, so each sample is a group of the factor. See Navarro and Foxcroft (2018) section 13.2. \\(H_0: \\bar{x}_1 = \\bar{x}_2 = \\bar{x}_3\\) \\(H_1: \\bar{x}_1 = \\bar{x}_2 \\neq \\bar{x}_3\\) or \\(\\bar{x}_1 \\neq \\bar{x}_2 \\neq \\bar{x}_3\\) In other words, our \\(H_1\\) is that at least one group mean is different from others. Test statistic \\(F\\) is calculated as \\(F = MSA / MSE,\\) where \\(MSA\\) expresses variation between group and \\(MSE\\) represents random variaton. In order to understand what this means let’s start from the initial measures, the sum of squares of variable \\(y\\) for observations \\(i\\) in groups \\(j\\): within-group sum of squares, i.e. sum of squares of errors (SSE) \\(SSE=\\sum_{j=1}^{k}\\sum_{i=1}^{n} (y_{ij}-\\overline{y_{j}})^{2}\\), \\(df = n - k\\) between group sum of squares, i.e. sum of squares between groups (SSA) \\(SSA=\\sum_{j=1}^{k} (\\overline{y_{j}}-\\overline{y})^{2}\\), \\(df = k - 1\\) total sum of squares (SST=SSE+SSA) \\(SST=\\sum_{j=1}^{k}\\sum_{i=1}^{n} (y_{ij}-\\overline{y})^{2}\\), \\(df = n - 1\\) We can use these variations to find \\(MSA\\) and \\(MSE\\) as follows: mean squares for SSE (MSE) \\(MSE = SSE / (n - k)\\) mean squares for SSA (MSA) \\(MSA = SSA / (k - 1)\\) Test statistic \\(F\\) is evaluated on F-distribution. Test assumes normality and independence of data and homogeneity of variance (only for Welch’s test). In Jamovi: Anova &gt; One-way Anova. In R: summary(aov(x, y)) 7.2 Post-hoc tests The result of Anova does not tell us about which groups are different but only that there is a difference. We can use post hoc tests to compare the means of all groups pariwise. See Navarro and Foxcroft (2018) section 14.8. In Jamovi: Anova &gt; One-way Anova &gt; Post-Hoc test | Games-Howell or Tukey. In R: TukeyHSD(aov(x, y)) 7.3 How to decide which test to use? Depending on the number of factors and whether or not the assumptions are satisfied, there are several types of Anova. These are explained in Navarro and Foxcroft (2018) sections 13 and 14. Two factors Unpaired groups Normally distributed Equal variances Fisher’s one-way Anova Unequal variances Welch’s one-way Anova Not normally distributed Kruskal-Wallis rank-sum test Paired groups (repeated measures) Normally distributed Repeated measures Anova Not normally distributed Friedman test Several factors Unpaired groups Normally distributed Factorial Anova "],["correlation-analysis.html", "Section 8 Correlation analysis 8.1 Scatterplot 8.2 Pearson’s correlation coefficient 8.3 Spearman rank-order correlation coefficient 8.4 Correlation matrix and heatmap 8.5 Causality", " Section 8 Correlation analysis Correlation analysis involves measuring relationships between two continuous variables. A correlation coefficient indicates the direction of relationship on a scale -1 … 1 where 0 means lack of relationship. See Navarro and Foxcroft (2018) section 12.1. 8.1 Scatterplot Scatterplots illustrate where observations are positioned relative to two continuous variables. This allows us to see if and how two variables are related. Conventionally dependent variables are presented on vertical (y) axis and independent variable on horizontal (x) axis. See Navarro and Foxcroft (2018) section 12.2. In Jamovi: Exploration &gt; Scatterplot. In R: plot(x, y) 8.2 Pearson’s correlation coefficient Evaluates linear relationship. Thus, perfect positive correlation implies that when \\(x\\) increases by 1 unit, \\(y\\) also increases by 1 unit. \\[r = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2 \\sum_{i=1}^{n}(y_i-\\bar{y})^2}\\] Essentially, we compare differences from mean value for values of each group. Statistical significance can also be estimated by finding the probability of t-statistic on t-distribution. \\[t=r\\sqrt{n-2}/\\sqrt{1-r^{2}}\\] See Navarro and Foxcroft (2018) section 12.1.3. In Jamovi: Regression &gt; Correlation Matrix &gt; Correlation Coefficients | Pearson. In R: cor(x) 8.3 Spearman rank-order correlation coefficient Evaluation is based on the ranking of values. Evaluates a monotonic relationship. Thus perfect positive correlation implies that when \\(x\\) increases, then \\(y\\) also always increases. \\[r = 1 - \\frac{6\\sum (x_{i}-y_{i})^{2}}{n(n^{2}-1)}\\] Simply put, we compare rankings of values from each group. See Navarro and Foxcroft (2018) section 12.1.6. In Jamovi: Regression &gt; Correlation Matrix &gt; Correlation Coefficients | Spearman. In R: cor(x, method = 'spearman') 8.4 Correlation matrix and heatmap To find relationships among large number of variables, correlation coefficients can be summarized as a correlation matrix. In Jamovi: Regression &gt; Correlation Matrix &gt; Plot | Correlation matrix ​ or Factor &gt; Reliability Analysis &gt; Additional Options | Correlation heatmap. In R: heatmap(cor(x)) 8.5 Causality It’s important to understand that correlation itself does not imply how the causality between variables functions. "],["simple-linear-regression.html", "Section 9 Simple linear regression 9.1 Ordinary least squares 9.2 Elements of (OLS) regression models 9.3 Assumptions and diagnostics", " Section 9 Simple linear regression Regression analysis is a statistical procedure that allows us to model relationships between variables so that the causal relationship between variables is defined. It can be considered as the main statistical technique used in economics, i.e. econometrics. There is a large variety of regression models, depending on estimation method, model specification and assumed distributions. This section introduces the most basic of these, the simple linear regression model, i.e. ordinary least squares (OLS) with one independent variable. See Navarro and Foxcroft (2018) section 12.3 for an introduction. 9.1 Ordinary least squares 9.1.1 Model specification Simple linear regression model has one predictor and its model is mathematically expressed as follows: \\[y = \\alpha + \\beta x + \\varepsilon,\\] where \\(y\\) is dependent or explained variable, response or regressand, \\(\\alpha\\) is the intercept or constant, \\(\\beta\\) is a coefficient of \\(x\\), \\(x\\) is independent or explanatory variable, predictor or regressor, and \\(\\varepsilon\\) is the model error. 9.1.2 Calculation The underlying idea behind (ordinary) least squares regression is the minimization of (squared) residuals. Model parameters are calculated (unlike maximum likelihood estimation based on iterations). To estimate the model \\(Y = \\alpha + \\beta x + \\varepsilon\\) we estimate the parameters \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) as follows: \\(\\hat{\\beta} = \\frac{\\sum{x_{i} y_{i}} - \\frac{1}{n} \\sum{x_{i}}\\sum{y_{i}}}{\\sum{x_{i}^{2}} - \\frac{1}{n} (\\sum{x_{i}})^{2}} = \\frac{Cov[x,y]} {Var[x]}\\) \\(\\hat{\\alpha} = \\overline{y} - \\beta \\overline{x}\\) For a simple model \\(y = \\beta x + \\varepsilon\\) we can simply use matrix algebra on values of \\(x\\) and \\(y\\) to find \\(\\hat{\\beta}\\): \\[\\hat{\\beta} = (X^{T} X)^{-1} X^{T} Y,\\] where \\(X\\) is the matrix of predictor and \\(Y\\) the matrix of response. See Navarro and Foxcroft (2018) section 12.4 or illustrations of the idea. In Jamovi: Regression &gt; Linear regression. In R: lm(formula, data) or summary(lm(formula, data)) 9.2 Elements of (OLS) regression models 9.2.1 Intercept In the equation this is the \\(\\alpha\\) and often referred to as the constant. Intercept is the value of \\(y\\) where regression line crosses the Y-axis, so intercept is the value of \\(y\\) when \\(x\\) is zero ( \\(y|x=0\\) ). Intercept does not need to be theoretically valid but it sometimes is. The statistical significance of the intercept is usually not relevant. 9.2.2 Coefficient(s) In the equation the \\(\\beta\\) represents coefficient of \\(x\\). It indicates y how many units \\(y\\) increases when \\(x\\) increases by one unit. We can not be sure if the coefficients are actually significant (when estimation is done on a sample). It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating t-statistic from coefficient and standard error. Coefficients are only relevant if their difference from 0 is statistically significant. 9.2.3 Fitted values These are the values of \\(y\\) calculated using the model for every \\(x\\) in the data. In other words, fitted values are predictions. 9.2.4 Residuals Residuals are model errors, represented by the \\(\\varepsilon\\) in the equation. Residuals are the difference in response between observed and fitted (model predicted) values. We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good. 9.2.5 The \\(R^2\\) This is a way to measure goodness of fit, i.e. how well model fits data. \\(R^2\\) indicates the part of variation in response variable that is explained by the model: \\[R^{2} = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS},\\] where the elements are defined a follows: explained sum of squares, \\(ESS\\) \\(\\sum_{i = 1}^{n} (\\hat{y}_{i} - \\overline{y})^2\\) residual sum of squares, \\(RSS\\) \\(\\sum_{i = 1}^{n} (y_{i} - \\hat{y}_{i})^2\\) total sum of squares, \\(TSS\\) \\(\\sum_{i = 1}^{n} (y_{i} - \\overline{y}_{i})^2\\) Mathematically, the \\(R^2\\) measures how much better is model at explaining the variance of \\(y\\) compared to just the mean. 9.2.6 The adjusted- \\(R^2\\) The more variables we add, the more the model explains. So \\(R^2\\) can be inflated just by adding variables. To penalize a model for the number of predictors ( \\(k\\) ) while considering the number of observations ( \\(n\\) ), the adjusted \\(R^{2}\\) can also be used, particularly for model comparison: \\[\\overline{R^{2}} = 1 - \\frac{RSS/(n-k)}{TSS/(n-k)}\\] 9.3 Assumptions and diagnostics After the estimation of a regression model it should be diagnosed to make sure that it meets the following assumptions: Residuals are normally distributed Residuals have equal variance, i.e. variance of residuals does not depend on the value of \\(x\\) There is no multicollinearity In Jamovi: Regression &gt; Linear regression &gt; Assumption Checks. In R: plot(lm(formula, data)) 9.3.0.1 Multicollinarity Multicollinearity may be problem when model has multiple predictors. It should not be possible to linearly predict any of the predictors from others predictors, i.e. predictors should not be (highly) correlated. Otherwise the coefficients are not reliable. Multicollinearity can be detected with variance inflation factor (VIF) by using \\(R^2\\) to estimate for each predictor how much of the variation in one predictor can be predicted from others. \\[VIF_{k} = \\frac{1}{1 - R^{2}_{k}},\\] where \\(R^{2}_{k}\\) is \\(R^{2}\\) for a model that has initial predictor \\(k\\) as a response and all other predictors as predictors. 9.3.1 Gauss-Markov assumptions In addition to the practical considerations outlined above, a theoretical way of expressing the assumptions of OLS is via the Gauss–Markov theorem. It posits the following assumptions: linear in parameters \\(Y = \\alpha + \\beta x + \\varepsilon\\) expected error is zero \\(E(\\varepsilon) = 0\\) homoscedasticity \\(var(\\varepsilon) = E(\\varepsilon^{2})\\) no autocorrelation \\(cov(\\varepsilon_{i}, \\varepsilon_{j}) = 0, i \\neq j\\) independence of predictor(s) and residuals \\(cov(x,\\varepsilon) = 0\\) If these are true, then the model is the best linear unbiased estimator (BLUE). "],["slides.html", "Section 10 Slides", " Section 10 Slides Here is an index of slides used to explain the methods during meeting. The essential information on the slides is covered in the course notes. In addition, slides add some explanations and visuals using example data sets. On slides, click h to see keyboard shortcuts for navigation. Introduction Descriptive statistics Hypothesis testing. Comparing categorical variables Comparing numerical variables Analysis of variance Correlation analysis Regression. Simple linear regression Regression. Multiple linear regression, factor variables and transformations Regression. Maximum likelihood and binary models Principal component analysis Factor analysis Hierarchical clustering K-means clustering "],["references.html", "Section 11 References", " Section 11 References Crawley, Michael J. 2013. The r Book. Second edition. Chichester, West Sussex, UK: Wiley. https://www.cs.upc.edu/~robert/teaching/estadistica/TheRBook.pdf. Dash, Ian. 2016. “Flowchart of Statistics for Research.” https://doi.org/10.13140/RG.2.2.12014.41283/1. Navarro, Danielle J, and David R Foxcroft. 2018. Learning Statistics with Jamovi: A Tutorial for Psychology Students and Other Beginners. Danielle J. Navarro; David R. Foxcroft. https://doi.org/10.24384/HGC3-7P15. Stevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677. Wickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (1): 1–23. https://doi.org/10.18637/jss.v059.i10. "]]
