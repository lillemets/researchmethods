---
title: Simple linear regression
subtitle: Research methods
author: Jüri Lillemets
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, dpi = 150, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
# Set colors
Col <- c(red = '#e6457a', green = '#b0e645', blue = '#45cbe6')
```

class: center middle clean

# How can we model a relationship?

---

class: center middle inverse

# Ordinary least squares

---

## Example problem

Suppose we have data on the "Weight gain calves in a feedlot, given three different diets."

```{r}
calves <- agridat::urquhart.feedlot
head(calves, 3)
```

- `animal`, animal ID
- `herd`, herd ID
- `diet`, diet: Low, Medium, High
- `weight1`, initial weight
- `weight2`, slaughter weight

---

.pull-left[
Let's see initial and slaughter weight of calves.

```{r, fig.height = 8}
plotWeights <- function(...) {
  par(bty = 'n', mar = c(4,4,2,0), family = 'RobotoCondensed')
  plot(weight2 ~ weight1, calves, xlab = "1st weight", ylab = "2nd weight", 
       las = 1, ...)
}
par(bty = 'n', mar = c(4,4,0,0), family = 'RobotoCondensed')
plotWeights()
```

]

--

.pull-right[
Does 1st weight have an effect on 2nd weight?

How much of 2nd weight depends on 1st weight?

How much does 2nd weight increase when 1st weight increases?

What could be the 2nd weight when 1st weight is e.g. 700?

Can we quantify this relationship?
]

???
Draw possible lines.

---

We can model this by estimating a regression model!

```{r, fig.height = 4.5}
wtMod <- lm(weight2 ~ weight1, calves)
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
```

---

## Model specification

Simple linear regression model has one predictor.

$$y = \alpha + \beta x + \varepsilon,$$

where

- $y$ is dependent or explained variable, **response** or regressand, 
- $\alpha$ is the intercept or constant, 
- $\beta$ is a coefficient, 
- $x$ is independent or explanatory variable, **predictor** or regressor, and
- $\varepsilon$ is the model error.

--

In our example: $\text{weight2} = \alpha + \beta \text{weight1} + \varepsilon$

---

## Calculation

The idea is to minimize of (squared) residuals. For $Y = \alpha + \beta x + \varepsilon$ :

$\hat{\beta} = \frac{Cov[x,y]} {Var[x]} = \frac{\sum{x_{i} y_{i}} - \frac{1}{n} \sum{x_{i}}\sum{y_{i}}}{\sum{x_{i}^{2}} - \frac{1}{n} (\sum{x_{i}})^{2}}$, $\hat{\alpha} = \overline{y} - \beta \overline{x}$

--

In our example $x$ would be the vector of 1st weights.

```{r}
calves$weight1
```

And $x$ would be the vector of 2nd weights.

```{r}
calves$weight2
```

---

For a model $y = \beta x + \varepsilon$ we can simply use matrix algebra on values of $x$ and $y$:

$$\hat{\beta} = (X^\prime X)^{-1} X^\prime Y$$

--

In our example problem $X$ would be the column matrix of 1st weights.

```{r}
cbind(head(calves$weight1, 3))
```

And $Y$ would be the column matrix of 2nd weights.

```{r}
cbind(head(calves$weight2, 3))
```

---

Why is one variable dependent and other(s) independent?

--

```{r, fig.height = 4.5}
wtMod <- lm(weight2 ~ weight1, calves)
wtModOpposite <- lm(weight1 ~ weight2, calves)
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
lines(predict(wtModOpposite, data.frame(weight2 = calves$weight2)),
      calves$weight2, 
      col = Col['blue'], lwd = 2)
```

???
Because we try to minimize error of using predictor to predict response.

---

class: center middle inverse

# Elements of regression models

---

## Intercept

.pull-left[
In the equation the $\alpha$. Sometimes called constant. The value of $y$ where regression line crosses the Y-axis, so intercept is the value of $y$ when $x$ is zero ( $y|x=0$ ).

In our example 2nd weight is `r round(wtMod$coef[[1]])` if 1st weight is 0. Intercept does not need to be theoretically valid but it sometimes is.
]

.pull-right[

```{r, fig.height = 8}
plotWeights(xlim = c(0, max(calves$weight1)), ylim = c(0, max(calves$weight2)))
lines(c(-100, calves$weight1), 
      predict(wtMod, data.frame(weight1 = c(-100, calves$weight1))), 
      col = Col['red'], lwd = 2)
abline(v = 0)
points(0, wtMod$coef[1], col = Col['blue'], cex = 4, lwd = 5)
```

]

--

> Does the intercept make sense in this example?

---

## Coefficient(s)

.pull-left[
In the equation the $\beta$. Indicates how much y increases when $x$ increases. 

In our example every kg of 1st weight increases 2nd weight by `r round(wtMod$coef[[2]], 3)` kg (on the plot *100).
]

.pull-right[

```{r, , fig.height = 8}
par(pty = 's')
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
predWt <- function(x) predict(wtMod, list(weight1 = x))
lines(c(700, 800), c(predWt(700), predWt(700)))
lines(c(800, 800), c(predWt(700), predWt(800)), col = Col['blue'], lwd = 5)
```

]

---

### Statistical significance of coefficients

Coefficients are only relevant when their difference from 0 is statistically significant.

We can not be sure if the coefficients are actually significant, especially when estimation is done on a sample. 

It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating **t-statistic from coefficient and standard error**.

---

## Fitted values

.pull-left[
These are the values of $y$ calculated using the model for every $x$ in the data. 

In other words, these are predictions.
]

.pull-right[

```{r, fig.height = 8}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
points(calves$weight1, predWt(calves$weight1), pch = 20, col = Col['blue'], lwd = 5)
```
]

---

## Residuals

.pull-left[
In the equation the $\varepsilon$. Residuals are the difference of response between observed and fitted values

We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good.

]

.pull-right[

```{r, , fig.height = 7}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
```

]

--

> What do the sizes of residuals tell us about the model?

---

## The $R^2$ 

A *goodness of fit* measure: measures how well model fits data. Indicates the **part of variation in response variable that is explained by the model**:

$$R^{2} = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$$

- **e**xplained sum of squares, $ESS$
  $\sum_{i = 1}^{n} (\hat{y}_{i} - \overline{y})^2$
- **r**esidual sum of squares, $RSS$
  $\sum_{i = 1}^{n} (y_{i} - \hat{y}_{i})^2$
- **t**otal sum of squares, $TSS$
  $\sum_{i = 1}^{n} (y_{i} - \overline{y}_{i})^2$

---

How much better is model at explaining the variance of $y$ compared to just the mean?

```{r}
par(mfrow = 1:2)
# Model
plotWeights(main = "Unexplained by model")
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
# Mean
plotWeights(main = "Unexplained by mean")
abline(h = mean(calves$weight2), col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], mean(calves$weight2)), 
        col = Col['blue'], lwd = 2)
}
```

???

This is simplification, we're actually using squared distances.

---

## The adjusted- $R^2$ 

The more variables we add, the more the model explains.

To penalize a model for the number of predictors ( $k$ ) while considering the number of observations ( $n$ ), the adjusted $R^{2}$ can also be used, particularly for model comparison:

$$\overline{R^{2}} = 1 - \frac{RSS/(n-k)}{TSS/(n-k)}$$

---

How are the parameters related to the formula?

$$weight2_{i} = `r round(wtMod$coef[[1]])` + `r round(wtMod$coef[[2]], 3)` * weight1_{i} + \varepsilon_{i}$$

```{r, echo = T}
calves$weight2[1]
wtMod
wtMod$coef[[1]] + wtMod$coef[[2]] * calves$weight1[[1]] + wtMod$resid[[1]]
```

---

class: center middle inverse

# Assumptions and diagnostics

---

## Residuals are normally distributed

.pull-left[

``` {r, fig.height = 8}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
```

]

.pull-right[

```{r, fig.height = 8}
par(bty = 'n', mar = c(4,4,1,0), family = 'RobotoCondensed')
hist(wtMod$residuals, 30)
```

``` {r, fig.height = 8, eval = F}
par(bty = 'n', mar = c(4,4,1,0), family = 'RobotoCondensed')
plot(wtMod, 2)
```

]

---

## Residuals have equal variance


.pull-left[

``` {r, fig.height = 8}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
```

]

.pull-right[

```{r, fig.height = 8}
par(bty = 'n', mar = c(4,4,2,0), family = 'RobotoCondensed')
plot(wtMod, 1)
```

]

Here the we have equal variance, e.g. **homo**scedasticity.

???

Variance of residuals does not depend on the value of $x$.

---

Example of **hetero**scedasticity. Here higher values have higher variance than lower.

```{r}
par(bty = 'n', mar = c(4,4,2,0), family = 'RobotoCondensed')
par(mfrow = 1:2)
x <- runif(100, 1, 100)
y <- x + x * rnorm(100, 0, .2)
plot(x, y)
abline(lm(y ~ x), col = 'red')
plot(lm(y ~ x), 1)
```

---

## There is no multicollinearity

It should not be possible to linearly predict any of the predictors from others predictors. Otherwise the **coefficients are not be reliable**.

Multicollinearity can be detected with variance inflation factor (VIF) by using $R^2$ to estimate for each predictor how much of the variation in one predictor can be predicted from others.

$$VIF_{k} = \frac{1}{1 - R^{2}_{k}},$$

where $R^{2}_{k}$ is $R^{2}$ for a model that has initial predictor $k$ as a response and all other predictors as predictors.

---

## Gauss-Markov assumptions

Linear model must fulfill the following conditions according to Gauss–Markov theorem:

- linear in parameters  
  $Y = \alpha + \beta x + \varepsilon$
- expected error is zero  
  $E(\varepsilon) = 0$
- homoscedasticity  
  $var(\varepsilon) = E(\varepsilon^{2})$
- no autocorrelation  
  $cov(\varepsilon_{i}, \varepsilon_{j}) = 0, i \neq j$
- independence of predictor(s) and residuals  
  $cov(x,\varepsilon) = 0$
  
--

If these are true, then the model is the *best linear unbiased estimator* (BLUE).

---

class: inverse
