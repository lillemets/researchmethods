---
title: Factor analysis
subtitle: Research methods
author: JÃ¼ri Lillemets
date: "`r Sys.Date()`"
output_yaml: _output.yml
knit: rmarkdown::render
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, message = F, dpi = 200, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
library('DiagrammeR');library('psych')
# Options
options(scipen = F, digits = 3)
# Set colors
source('/home/jrl/work/resmeth/slides/variables.R')
```

class: center middle clean

# What are the underlying factors behind variables?

---

class: center middle inverse

# Intuition

---

## Factor analysis (FA) and principal component analysis (PCA)

In PCA we attempt to maximize the **total variation in data**. 

In factor analysis we instead try to maximize some **underlying shared variation between variables**.

--

PCs are uniquely defined while **factors depend on estimation and rotation method**.

--

In PCA each PC is a linear combination of variables but **in FA the variables are expressed as linear combinations of factors**.

---

What loads what?

.pull-left[
``` {r}
grViz(
  "digraph pc {
  rankdir=LR
  graph [fontname = RobotoCondensed, fontsize = 8]
  node [shape = box] x_1; x_2; x_3; x_4; x_5
  node [shape = circle] PC_1; PC_2
  
  {x_1 x_2 x_3 x_4 x_5} -> PC_1
  {x_1 x_2 x_3 x_4 x_5} -> PC_2
  }",
  width = 400, height = 400)
```
]
.pull-right[
``` {r}
grViz(
  "digraph {
  rankdir = LR
  graph [fontname = RobotoCondensed, fontsize = 8]
  node [shape = circle] x_1; x_2; x_3; x_4; x_5
  node [shape = box] F_1; F_2; u_1; u_2; u_3; u_4; u_5
  
  {rank = same x_1; x_2; x_3; x_4; x_5}
  {rank = same u_1; u_2; u_3; u_4; u_5}
  
  F_1 -> {x_1 x_2 x_3 x_4 x_5}
  F_2 -> {x_1 x_2 x_3 x_4 x_5}
  
  x_1 -> u_1 [dir=back]
  u_2 -> x_2
  u_3 -> x_3
  u_4 -> x_4
  u_5 -> x_5
  }", 
  width = 400, heigh = 400)
```
]

---

### Should we use PCA or FA?

If our goal would be to **summarize variables** into a smaller set of variables, then we should use PCA. 

If the goal is to **determine some underlying variables**, then we should use FA.

---

## Orthogonal factor model

Variables are loaded by factors.  Each variable can thus be represented as a linear combination of factor loadings as follows:

$$x_j = q_{jl} F_l + U_j +\mu_j,$$

where 

- $x_j$ is a variable $j$, 
- $q_{jl}$ the loading of the $j$th variable on the $l$th common factor, 
- $F_l$ the $l$th common factor, 
- $U_j$ the $j$th specific factor and 
- $\mu_j$ the mean of $x_j$. 

---

class: center middle inverse

# Estimation

---

## Estimating the model

The idea behind FA is to **explain the common variance of varibles by extracting factors from the correlations between variables**. 

--

There are three methods for extracting factors:

- minimum residuals, 
- maximum likelihood and
- principal axis method.

---

A common example of factors are **personality traits**: we can not measure personality directly but we can ask subjects various questions and then use factor analysis to summarize the responses into factors that represent these traits.

--

Let's use data on 25 personality items of 2800 people.

.compact[
``` {r}
Oie <- psych::bfi
Items <- Oie[, 1:25]
#Oie <- Oie[complete.cases(Items), ]
head(Oie) %>% kable
```
]

- ...
- `gender` Males = 1, Females = 2
- `education` 1 = HS, 2 = finished HS, 3 = some college, 4 = college graduate 5 = graduate degree
- `age` age in years

---

The item data were collected using a 6 point response scale where 1 is very inaccurate and 6 is very accurate.

.smaller[
- `A1` Am indifferent to the feelings of others. (q_146)
- `A2` Inquire about others' well-being. (q_1162)
- `A3` Know how to comfort others. (q_1206)
- `A4` Love children. (q_1364)
- `A5` Make people feel at ease. (q_1419)
- `C1` Am exacting in my work. (q_124)
- `C2` Continue until everything is perfect. (q_530)
- `C3` Do things according to a plan. (q_619)
- `C4` Do things in a half-way manner. (q_626)
- `C5` Waste my time. (q_1949)
- `E1` Don't talk a lot. (q_712)
- `E2` Find it difficult to approach others. (q_901)
- `E3` Know how to captivate people. (q_1205)
- `E4` Make friends easily. (q_1410)
- `E5` Take charge. (q_1768)
- `N1` Get angry easily. (q_952)
- `N2` Get irritated easily. (q_974)
- `N3` Have frequent mood swings. (q_1099)
- `N4` Often feel blue. (q_1479)
- `N5` Panic easily. (q_1505)
- `O1` Am full of ideas. (q_128)
- `O2` Avoid difficult reading material.(q_316)
- `O3` Carry the conversation to a higher level. (q_492)
- `O4` Spend time reflecting on things. (q_1738)
- `O5` Will not probe deeply into a subject. (q_1964)
]

``` {r}
Names <- c(
  'Am indifferent to the feelings of others',
  "Inquire about others' well-being", 
  'Know how to comfort others', 
  'Love children', 
  'Make people feel at ease', 
  'Am exacting in my work', 
  'Continue until everything is perfect', 
  'Do things according to a plan', 
  'Do things in a half-way manner', 
  'Waste my time', 
  "Don't talk a lot", 
  'Find it difficult to approach others', 
  'Know how to captivate people', 
  'Make friends easily', 
  'Take charge', 
  'Get angry easily', 
  'Get irritated easily', 
  'Have frequent mood swings', 
  'Often feel blue', 
  'Panic easily', 
  'Am full of ideas', 
  'Avoid difficult reading material', 
  'Carry the conversation to a higher level', 
  'Spend time reflecting on things', 
  'Will not probe deeply into a subject'
)
```

---

In psychology *Big Five personality traits* is a suggested taxonomy of psychological attributes that includes the following traits: 

- agreeableness (friendly/compassionate vs. critical/rational)
- conscientiousness(efficient/organized vs. extravagant/careless)
- extraversion(outgoing/energetic vs. solitary/reserved)
- neuroticism (sensitive/nervous vs. resilient/confident)
- openness (inventive/curious vs. consistent/cautious)

--

So we assume that there are 5 latent factors hidden behind the accuracy of statements.

---

Let's see what are the loadings if we estimate the factor model without rotation.

.compact[
``` {r}
faItems <- fa(Items, nfactors = 5, rotate = 'none')
tabLoadings <- function(x) {
  Tab <- x$loadings %>% as.data.frame.matrix %>% round(2)
  rownames(Tab) <- Names
  Tab[abs(Tab) < .3] <- NA
  Tab
}
tabLoadings(faItems) %>% kable
```
]

---

## Rotations

We can apply a rotation on factors **to better distinguish factors**.

The result of rotation is an **adjusted loadings matrix, where factors are more distinct and thus more intuitive to interpret**. 

While rotation introduces some subjectivity as there are no rules about the choice of rotation, it often **eases description of factors**.

There are many rotations to choose from but we look at three: varimax, oblimin and quartimax.

---

### Varimax

This rotation maximizes variances of squared loadings **within factors**. **Each factor has few variables with high loadings**. Factors are **uncorrelated**.

.compact[
``` {r}
fa(Items, nfactors = 5, rotate = 'varimax') %>% tabLoadings %>% kable
```
]

---

### Oblimin

This rotation also involves maximization of squared loading variances **within factors** but with possibly **correlated** factors.

.compact[
``` {r}
fa(Items, nfactors = 5, rotate = 'oblimin') %>% tabLoadings %>% kable
```
]

---

### Quartimax

This rotation aims to maximize variances of factor loadings **within variables**. The result is a factor structure in which **each variable is loaded by only few factors**.

.compact[
``` {r}
fa(Items, nfactors = 5, rotate = 'quartimax') %>% tabLoadings %>% kable
```
]

???

The difference from Varimax rotation is that factors are not necessarily orthogonal but oblique, thus can be correlated. The resulting factors may be more similar to each other than in case of Varimax but the variances of loadings within factors are likely to be higher.

---

How to think about rotations?

``` {r, fig.height = 5}
par(bty = 'n', family = 'RobotoCondensed')
faItems <- fa(Items, nfactors = 5, rotate = 'none')
plot(faItems$loadings, xlim = c(-1,1), ylim = c(-1,1))
abline(v = 0, h = 0, lty = 2)
```

---

class: center middle inverse

# Assumptions

---

### Sphericity

**Variables need to be correleated** to a certain extent for us to able to summarize them into a smaller set of factors. 

If data cloud on scatter plot is spherical, the correlations in data are random and factor analysis is thus not accurate. 

Whether or not correlation matrix is statistically significantly different from zero (identity matrix) can be tested using *Bartlett's test*.

$H_0:$ Correlation matrix is zero  
$H_1:$ Correlation matrix is different from zero

If the test returns $p \ge 0.05$, then we should not trust the results of factor analysis.

---

.compact[
``` {r}
cor(Items, use = 'pairwise.complete.obs') %>% round(1) %>% kable
```
]

P-value here is `r cortest.bartlett(cor(Items, use = 'pairwise.complete.obs'), n = nrow(Items))$p.value`.

> Are the correlations significantly different from zero?

---

### Sampling adequacy

In addition to correlations it is necessary that **each factor is not related to only few variables**. We can use Kaiser-Meyer-Olkin (KMO) measure to determine if sum of partial correlations is higher than sum of correlations or not. The value of the measure is interpreted as follows regarding the suitability of FA.

- 0 ... 0.6 - Not suitable
- 6 ... 0.7 - Suitable
- 0.7 ... 0.9 - Adequate
- 0.9 ... 1 - Excellent

--

.compact[
``` {r}
KMO(Items)$MSAi %>% round(2) %>% t %>% kable
```
]

> Is a factor analysis with these varables adequate?

---

class: center middle inverse

# Interpretation

---

## Loadings

Loadings represent the **influence factors have on variables.** 

.compact[
``` {r}
faItems <- fa(Items, nfactors = 5, rotate = 'varimax') 
faItems %>% tabLoadings %>% kable
```
]

---

Let's now see the result with interpretations for each factor.

.compact[
``` {r}
Traits <- c('Agreeableness', 'Conscientiousness', 'Extraversion', 
            'Neuroticism', 'Opennness')
Ordering <- c(4, 3, 2, 1, 5)
faItems %>% tabLoadings %>% kable(col.names = Traits[Ordering])
```
]

---

We can visualize the loadings as well.

``` {r}
fa.diagram(faItems)
```

---

And we can add some variables to the visualization.

``` {r}
fa.extend(Oie, 5, ov = 1:25, ev = 26:28) %>% extension.diagram
```

---

## Uniqueness and communality

These measures characterize **each variable** in FA.

*Uniqueness* is a measure of proportion of variation **explained by a particular variable and not by factors**. 

*Communality* is the inverse of this, i.e. the **variation of a variable that is shared with other variables**.

???
Communality is the variation of a variable that is shared with other variables.

---

Recall the factor model $x_j = q_{jl} F_l + U_j +\mu_j$.

.compact[
``` {r}
faItems %>% tabLoadings %>% 
  cbind(Uniq. = faItems$uniquenesses, mu = colMeans(Items, na.rm = T)) %>% 
  round(2) %>% 
  kable(col.names = c(Traits[Ordering], 'Uniqueness', 'Mean'))
```
]

---

## Complexity

This can be calculated for **each variable** and it expresses the **number of factors which load the variable**.

.compact[
``` {r}
faItems %>% tabLoadings %>% 
  cbind(Comp. = faItems$complexity) %>% round(2) %>% 
  kable(col.names = c(Traits[Ordering], 'Complexity'))
```
]

---

## Scores

As for PCA, we can calculate a score of each factor for each observation.

.compact[
``` {r}
cbind(Oie[26:28], faItems$scores) %>% head(20) %>% kable
```
]

--

We can use these scores, for example, to see how social status is related to personality.

---

Gender

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,3,0), 
    mfrow = c(1,5))
for (i in 1:5) {
  boxplot(faItems$scores[, i] ~ Oie$gender == 1, 
          xlab = "Male", ylab = "Factor score", main = Traits[Ordering][i])
}
```

---

Education

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,3,0), 
    mfrow = c(1,5))
for (i in 1:5) {
  boxplot(faItems$scores[, i] ~ Oie$education, 
          xlab = "Education", ylab = "Factor score", main = Traits[Ordering][i])
}
```

---

Age

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,3,0), 
    mfrow = c(1,5))
for (i in 1:5) {
  plot(faItems$scores[, i] ~ Oie$age, 
          xlab = "Age", ylab = "Factor score", main = Traits[Ordering][i], 
       pch = 20, cex = .01, col = Col['clear'])
  #abline(lm(faItems$scores[, i] ~ Oie$age), col = Col['red'])
}
```

---

## FA is a controversial method

We can never be sure if the factors actually exist and were correctly estimated. 

This is due to the different options a researcher to choose

- the number of factors, 
- method of estimating the model, 
- rotation technique and
- interpreting factors.

---

Factor analysis is an exploratory method.

![](../img/madeupnumbers.gif)

---

class: center middle inverse

# Practical application

---

Use the data set `gss_spending`.

>Find the hidden factors behind respondents' attitudes toward public spending. Save the factor scores as new variables.

--

>Explain the factors by respondents' background.

--

> If you find a strong association test its statistical significance.

---

class: inverse
