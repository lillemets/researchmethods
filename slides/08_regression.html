<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Simple linear regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jüri Lillemets" />
    <meta name="date" content="2021-05-09" />
    <script src="08_regression_files/header-attrs-2.7/header-attrs.js"></script>
    <link rel="stylesheet" href="minimal.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Simple linear regression
## Research methods
### Jüri Lillemets
### 2021-05-09

---




class: center middle clean

# How can we model a relationship?

---

class: center middle inverse

# Ordinary least squares

---

## Example problem

Suppose we have data on the "Weight gain calves in a feedlot, given three different diets."


```
##   animal herd diet weight1 weight2
## 1      1    9  Low     575     826
## 2      2    9  Low     605     816
## 3      3    9  Low     640     902
```

- `animal`, animal ID
- `herd`, herd ID
- `diet`, diet: Low, Medium, High
- `weight1`, initial weight
- `weight2`, slaughter weight

---

.pull-left[
Let's see initial and slaughter weight of calves.

![](08_regression_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;

]

--

.pull-right[
Does 1st weight have an effect on 2nd weight?

How much of 2nd weight depends on 1st weight?

How much does 2nd weight increase when 1st weight increases?

What could be the 2nd weight when 1st weight is e.g. 700?

--

Can we quantify this relationship?
]

???
Draw possible lines.

---

We can model this by estimating a regression model!

![](08_regression_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

## Model specification

Simple linear regression model has one predictor.

`$$y = \alpha + \beta x + \varepsilon,$$`

where

- `\(y\)` is dependent or explained variable, **response** or regressand, 
- `\(\alpha\)` is the intercept or constant, 
- `\(\beta\)` is a coefficient, 
- `\(x\)` is independent or explanatory variable, **predictor** or regressor, and
- `\(\varepsilon\)` is the model error.

--

In our example: `\(\text{weight2} = \alpha + \beta \text{weight1} + \varepsilon\)`

---

## Calculation

The idea is to minimize of (squared) residuals. For `\(Y = \alpha + \beta x + \varepsilon\)` :

`\(\hat{\beta} = \frac{Cov[x,y]} {Var[x]} = \frac{\sum{x_{i} y_{i}} - \frac{1}{n} \sum{x_{i}}\sum{y_{i}}}{\sum{x_{i}^{2}} - \frac{1}{n} (\sum{x_{i}})^{2}}\)`, `\(\hat{\alpha} = \overline{y} - \beta \overline{x}\)`

In our example `\(x\)` would be the vector of 1st weights.


```
##  [1] 575 605 640 600 610 575 730 670 690 685 690 670 755 655 725 750 705 785 685
## [20] 655 750 715 785 795 640 680 620 765 750 645 730 795 605 570 730 670 700 665
## [39] 635 700 715 675 770 800 685 715 865 845 705 725 840 755 725 770 755 530 680
## [58] 605 665 765 720 780 675 705 755 755 750
```

And `\(x\)` would be the vector of 2nd weights.


```
##  [1]  826  816  902  931  854  960 1104  922 1046 1027 1018  864 1008  979 1085
## [16] 1037 1123 1171 1018 1027 1152 1142 1104 1152  941  979  931 1133  989  970
## [31] 1162 1248  874  854 1046 1056 1037  874  874  998  960  979 1056 1133 1027
## [46] 1037 1219  998 1037 1075 1190 1104  883 1123 1056  883 1018  874  970 1114
## [61] 1018 1152 1056 1066 1104 1114 1008
```

---

For a model `\(y = \beta x + \varepsilon\)` we can simply use matrix algebra on values of `\(x\)` and `\(y\)`:

`$$\hat{\beta} = (X^\prime X)^{-1} X^\prime Y$$`

In our example problem `\(X\)` would be the column matrix of 1st weights.


```
##      [,1]
## [1,]  575
## [2,]  605
## [3,]  640
```

And `\(Y\)` would be the column matrix of 2nd weights.


```
##      [,1]
## [1,]  826
## [2,]  816
## [3,]  902
```

---

Why is one variable dependent and other(s) independent?

--

![](08_regression_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

???
Because we try to minimize error of using predictor to predict response.

---

class: center middle inverse

# Elements of regression models

---

## Intercept

.pull-left[
In the equation the `\(\alpha\)`. Sometimes called constant. The value of `\(y\)` where regression line crosses the Y-axis, so intercept is the value of `\(y\)` when `\(x\)` is zero ( `\(y|x=0\)` ).

In our example 2nd weight is 195 if 1st weight is 0. Intercept does not need to be theoretically valid but it sometimes is.
]

.pull-right[

![](08_regression_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

]

--

&gt; Does the intercept make sense in this example?

---

## Coefficient(s)

.pull-left[
In the equation the `\(\beta\)`. Indicates how much y increases when `\(x\)` increases. 

In our example every kg of 1st weight increases 2nd weight by 1.176 kg (on the plot *100).
]

.pull-right[

![](08_regression_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

]

---

### Statistical significance of coefficients

Coefficients are only relevant when their difference from 0 is statistically significant.

We can not be sure if the coefficients are actually significant, especially when estimation is done on a sample. 

It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating **t-statistic from coefficient and standard error**.

---

## Fitted values

.pull-left[
These are the values of `\(y\)` calculated using the model for every `\(x\)` in the data. 

In other words, these are predictions.
]

.pull-right[

![](08_regression_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
]

---

## Residuals

.pull-left[
In the equation the `\(\varepsilon\)`. Residuals are the difference of response between observed and fitted values

We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good.

]

.pull-right[

![](08_regression_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

]

--

&gt; What do the sizes of residuals tell us about the model?

---

## The `\(R^2\)` 

A *goodness of fit* measure: measures how well model fits data. Indicates the **part of variation in response variable that is explained by the model**:

`$$R^{2} = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$$`

- **e**xplained sum of squares, `\(ESS\)`
  `\(\sum_{i = 1}^{n} (\hat{y}_{i} - \overline{y})^2\)`
- **r**esidual sum of squares, `\(RSS\)`
  `\(\sum_{i = 1}^{n} (y_{i} - \hat{y}_{i})^2\)`
- **t**otal sum of squares, `\(TSS\)`
  `\(\sum_{i = 1}^{n} (y_{i} - \overline{y}_{i})^2\)`

---

How much better is model at explaining the variance of `\(y\)` compared to just the mean?

![](08_regression_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

???

This is simplification, we're actually using squared distances.

---

## The adjusted- `\(R^2\)` 

The more variables we add, the more the model explains.

To penalize a model for the number of predictors ( `\(k\)` ) while considering the number of observations ( `\(n\)` ), the adjusted `\(R^{2}\)` can also be used, particularly for model comparison:

`$$\overline{R^{2}} = 1 - \frac{RSS/(n-k)}{TSS/(n-k)}$$`

---

How are the parameters related to the formula?

`$$weight2_{i} = 195 + 1.176 * weight1_{i} + \varepsilon_{i}$$`


```r
calves$weight2[1]
```

```
## [1] 826
```

```r
wtMod
```

```
## 
## Call:
## lm(formula = weight2 ~ weight1, data = calves)
## 
## Coefficients:
## (Intercept)      weight1  
##     195.135        1.176
```

```r
wtMod$coef[[1]] + wtMod$coef[[2]] * calves$weight1[[1]] + wtMod$resid[[1]]
```

```
## [1] 826
```

---

class: center middle inverse

# Assumptions and diagnostics

---

## Residuals are normally distributed

.pull-left[

![](08_regression_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

]

.pull-right[

![](08_regression_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;



]

---

## Residuals have equal variance


.pull-left[

![](08_regression_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;

]

.pull-right[

![](08_regression_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;

]

Here the we have equal variance, e.g. **homo**scedasticity.

???

Variance of residuals does not depend on the value of `\(x\)`.

---

Example of **hetero**scedasticity. Here higher values have higher variance than lower.

![](08_regression_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

---

## There is no multicollinearity

It should not be possible to linearly predict any of the predictors from others predictors. Otherwise the **coefficients are not be reliable**.

Multicollinearity can be detected with variance inflation factor (VIF) by using `\(R^2\)` to estimate for each predictor how much of the variation in one predictor can be predicted from others.

`$$VIF_{k} = \frac{1}{1 - R^{2}_{k}},$$`

where `\(R^{2}_{k}\)` is `\(R^{2}\)` for a model that has initial predictor `\(k\)` as a response and all other predictors as predictors.

---

## Gauss-Markov assumptions

Linear model must fulfill the following conditions according to Gauss–Markov theorem:

- linear in parameters  
  `\(Y = \alpha + \beta x + \varepsilon\)`
- expected error is zero  
  `\(E(\varepsilon) = 0\)`
- homoscedasticity  
  `\(var(\varepsilon) = E(\varepsilon^{2})\)`
- no autocorrelation  
  `\(cov(\varepsilon_{i}, \varepsilon_{j}) = 0, i \neq j\)`
- independence of predictor(s) and residuals  
  `\(cov(x,\varepsilon) = 0\)`

If these are true, then the model is the *best linear unbiased estimator* (BLUE).

---

class: inverse
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
