---
title: Simple linear regression
subtitle: Research methods
author: Jüri Lillemets
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, dpi = 150, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
# Set colors
Col <- c(red = '#e6457a', green = '#b0e645', blue = '#45cbe6')
```

class: center middle clean

# How can we model a relationship?

---

class: center middle inverse

# Ordinary least squares

---

## Example problem

Suppose we have data on the "Weight gain calves in a feedlot, given three different diets."

```{r}
calves <- agridat::urquhart.feedlot
head(calves, 3)
```

- `animal`, animal ID
- `herd`, herd ID
- `diet`, diet: Low, Medium, High
- `weight1`, initial weight
- `weight2`, slaughter weight

---

.pull-left[
Let's see initial and slaughter weight of calves.

```{r, fig.height = 8}
plotWeights <- function(...) {
  par(bty = 'n', mar = c(4,4,2,0), family = 'RobotoCondensed')
  plot(weight2 ~ weight1, calves, xlab = "1st weight", ylab = "2nd weight", 
       las = 1, ...)
}
plotWeights()
```

]

--

.pull-right[
Does 1st weight have an effect on 2nd weight?

How much of 2nd weight depends on 1st weight?

How much does 2nd weight increase when 1st weight increases?

What could be the 2nd weight when 1st weight is e.g. 700?

Can we quantify this relationship?
]

???
Draw possible lines.

---

We can model this by estimating a regression model!

```{r, fig.height = 4.5}
wtMod <- lm(weight2 ~ weight1, calves)
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
```

---

## Model specification

Simple linear regression model has one predictor.

$$y = \alpha + \beta x + \varepsilon,$$

where

- $y$ is dependent or explained variable, **response** or regressand, 
- $\alpha$ ( $\beta_0$ ) is the intercept or constant, 
- $\beta$ ( $\beta_k$ ) is a coefficient, 
- $x$ is independent or explanatory variable, **predictor** or regressor, and
- $\varepsilon$ is the model error.

---

In our example the formula can be expressed as

$$\text{weight2} = \alpha + \beta \text{weight1} + \varepsilon_i$$

or in case we just look at 3 observations as

$$\begin{matrix} 826 \\ 816 \\ 902 \end{matrix} = \alpha + \beta \times \begin{matrix} 575 \\ 605 \\ 640 \end{matrix} + \begin{matrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \end{matrix}.$$

--

Residuals, $\varepsilon_i$ can be calculated after we have found $\alpha$ and $\beta$. 

But how do we find the $\alpha$ and $\beta$?

---

## Calculation

The idea is to minimize of (squared) residuals. For $Y = \alpha + \beta x + \varepsilon$ :

$\hat{\beta} = \frac{Cov[x,y]} {Var[x]} = \frac{\sum{x_{i} y_{i}} - \frac{1}{n} \sum{x_{i}}\sum{y_{i}}}{\sum{x_{i}^{2}} - \frac{1}{n} (\sum{x_{i}})^{2}}$, $\hat{\alpha} = \overline{y} - \hat\beta \overline{x}$

--

In our example $x$ would be the vector of 1st weights.

```{r}
calves$weight1
```

And $x$ would be the vector of 2nd weights.

```{r}
calves$weight2
```

---

For a model $y = \beta x + \varepsilon$ we can simply use matrix algebra on values of $x$ and $y$:

$$\hat{\beta} = (X^\prime X)^{-1} X^\prime Y$$

--

In our example problem $X$ would be the column vector of 1st weights.

```{r}
cbind(head(calves$weight1, 3))
```

And $Y$ would be the column vector of 2nd weights.

```{r}
cbind(head(calves$weight2, 3))
```

---

Why is one variable dependent and other(s) independent?

--

```{r, fig.height = 4.5}
wtMod <- lm(weight2 ~ weight1, calves)
wtModOpposite <- lm(weight1 ~ weight2, calves)
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
lines(predict(wtModOpposite, data.frame(weight2 = calves$weight2)),
      calves$weight2, 
      col = Col['blue'], lwd = 2)
```

???
Because we try to minimize error of using predictor to predict response.

---

class: center middle inverse

# Elements of regression models

---

## Intercept

.pull-left[
In the equation the $\alpha$. Sometimes called constant. The value of $y$ where regression line crosses the Y-axis, so intercept is the value of $y$ when $x$ is zero ( $y|x=0$ ).

Intercept is usually irrelevant when interpreting the model.
]

.pull-right[
```{r, fig.height = 8}
plotWeights(xlim = c(0, max(calves$weight1)), ylim = c(0, max(calves$weight2)))
lines(c(-100, calves$weight1), 
      predict(wtMod, data.frame(weight1 = c(-100, calves$weight1))), 
      col = Col['red'], lwd = 2)
abline(v = 0)
points(0, wtMod$coef[1], col = Col['blue'], cex = 4, lwd = 5)
```
]

--

> Does the intercept make sense in this example?

---

In our example 2nd weight is `r round(wtMod$coef[[1]])` if 1st weight is 0. Intercept does not need to be theoretically valid but it sometimes is.

``` {r}
summary(wtMod)
```

---

class: middle

.center[![extrapolating](/home/jrl/pics/stats/extrapolating.png)]

.footnote[Source: XKCD, extrapolating]

---

## Coefficient(s)

.pull-left[
In the equation the $\beta$. Indicates how much y increases when $x$ increases. 
]

.pull-right[

```{r, , fig.height = 8}
par(pty = 's')
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
predWt <- function(x) predict(wtMod, list(weight1 = x))
lines(c(700, 800), c(predWt(700), predWt(700)))
lines(c(800, 800), c(predWt(700), predWt(800)), col = Col['blue'], lwd = 5)
```

]

---

In our example every kg of 1st weight increases 2nd weight by `r round(wtMod$coef[[2]], 3)` kg (on the plot $\times 100$).

``` {r}
summary(wtMod)
```

---

### Statistical significance of coefficients

Coefficients are only relevant for generalization when their difference from 0 is statistically significant. 

We can not be sure if the coefficients are actually significant, especially when estimation is done on a sample. So we again need to test this.

---

It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating **t-statistic from coefficient and standard error**.

``` {r}
summary(wtMod)
```

---

## Fitted values

.pull-left[
These are the values of $y$ calculated using the model for every $x$ in the data. 

In other words, these are predictions.
]

.pull-right[

```{r, fig.height = 8}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
points(calves$weight1, predWt(calves$weight1), pch = 20, col = Col['blue'], lwd = 5)
```
]

--

> What will be the approximate 2nd weight for a calf whose 1st weight is 700?

---

### Predictions

We can use regression models to predict the values of $y$ for any value of $x$.

$$\text{weight2} = \alpha + \beta \text{weight1} + \varepsilon$$

As prediction does not have a residual $\varepsilon = 0$.

--

What is the exact 2nd weight if 1st weight is 700?

$$`r wtMod$coef[[1]]` + `r wtMod$coef[[2]]` \times 700 = `r predict(wtMod, data.frame(weight1 = 700))`$$

---

## Residuals

.pull-left[
In the equation the $\varepsilon$. Residuals are the difference of response between observed and fitted values

We use residuals to evaluate how well model fits data. 

]

.pull-right[

```{r, , fig.height = 7}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
```

]

--

> What do the sizes of residuals tell us about the model?

???

If residuals are large, the model is not very good.

---

## The $R^2$ (coefficient of determination)

A *goodness of fit* measure: measures how well model fits data. Indicates the **part of variation in response variable that is explained by the model**:

$$R^{2} = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}, $$

where the elements are defined a follows:

- **e**xplained sum of squares, $ESS$;  
  $\sum_{i = 1}^{n} (\hat{y}_{i} - \bar{y})^2$
- **r**esidual sum of squares, $RSS$;  
  $\sum_{i = 1}^{n} (y_{i} - \hat{y}_{i})^2$
- **t**otal sum of squares, $TSS$;  
  $\sum_{i = 1}^{n} (y_{i} - \bar{y})^2.$

---

How much better is model at explaining the variance of $y$ compared to just the mean?

```{r}
par(mfrow = 1:2)
# Model
plotWeights(main = "Unexplained by model")
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
# Mean
plotWeights(main = "Unexplained by mean")
abline(h = mean(calves$weight2), col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], mean(calves$weight2)), 
        col = Col['blue'], lwd = 2)
}
```

???

This is simplification, we're actually using squared distances.

---

class: middle

.center[![linear_regression](../img/linear_regression.png)]

.footnote[Source: XKCD, linear regression]

---

## The adjusted- $R^2$ 

The more variables we add, the more the model explains.

To penalize a model for the number of predictors ( $k$ ) while considering the number of observations ( $n$ ), the adjusted $R^{2}$ can also be used, particularly for model comparison:

$$\overline{R^{2}} = 1 - \frac{RSS/(n-k)}{TSS/(n-1)}$$

---

``` {r}
summary(wtMod)
```

---

How are the parameters related to the formula?

$$weight2_{i} = `r round(wtMod$coef[[1]])` + `r round(wtMod$coef[[2]], 3)` * weight1_{i} + \varepsilon_{i}$$

```{r, echo = T}
c(wtMod$coef[[1]], wtMod$coef[[2]])
calves$weight1[1]
wtMod$residuals[[1]]
wtMod$coef[[1]] + wtMod$coef[[2]] * calves$weight1[[1]] + wtMod$resid[[1]]
calves$weight2[1]
```

---

class: center middle inverse

# Assumptions and diagnostics

---

## Residuals are normally distributed

.pull-left[

``` {r, fig.height = 8}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
```

]

.pull-right[

```{r, fig.height = 8}
par(bty = 'n', mar = c(4,4,1,0), family = 'RobotoCondensed')
hist(wtMod$residuals, 30)
```

``` {r, fig.height = 8, eval = F}
par(bty = 'n', mar = c(4,4,1,0), family = 'RobotoCondensed')
plot(wtMod, 2)
```

]

---

## Residuals have equal variance


.pull-left[

``` {r, fig.height = 8}
plotWeights()
lines(calves$weight1, 
      predict(wtMod, data.frame(weight1 = calves$weight1)), 
      col = Col['red'], lwd = 2)
for (i in 1:nrow(calves)) {
  lines(c(calves$weight1[i], calves$weight1[i]), 
        c(calves$weight2[i], predWt(calves$weight1[i])), 
        col = Col['blue'], lwd = 2)
}
```

]

.pull-right[

```{r, fig.height = 8}
par(bty = 'n', mar = c(4,4,2,0), family = 'RobotoCondensed')
plot(wtMod, 1)
```

]

Here the we have equal variance, e.g. **homo**scedasticity.

???

Variance of residuals does not depend on the value of $x$.

---

Example of **hetero**scedasticity. Here higher values have higher variance than lower.

```{r}
par(bty = 'n', mar = c(4,4,2,0), family = 'RobotoCondensed')
par(mfrow = 1:2)
x <- runif(100, 1, 100)
y <- x + x * rnorm(100, 0, .2)
plot(x, y)
abline(lm(y ~ x), col = 'red')
plot(lm(y ~ x), 1)
```

---

## There is no multicollinearity

It should not be possible to linearly predict any of the predictors from others predictors. Otherwise the **coefficients are not be reliable**.

Multicollinearity can be detected with variance inflation factor (VIF) by using $R^2$ to estimate for each predictor how much of the variation in one predictor can be predicted from others.

$$VIF_{k} = \frac{1}{1 - R^{2}_{k}},$$

where $R^{2}_{k}$ is $R^{2}$ for a model that has initial predictor $k$ as a response and all other predictors as predictors.

---

## Gauss-Markov assumptions

Linear model must fulfill the following conditions according to Gauss–Markov theorem:

- linear in parameters  
  $Y = \alpha + \beta x + \varepsilon$
- expected error is zero  
  $E(\varepsilon) = 0$
- homoscedasticity  
  $var(\varepsilon) = E(\varepsilon^{2})$
- no autocorrelation  
  $cov(\varepsilon_{i}, \varepsilon_{j}) = 0, i \neq j$
- independence of predictor(s) and residuals  
  $cov(x,\varepsilon) = 0$

--

If these are true, then the model is the *best linear unbiased estimator* (BLUE).

---

class: inverse

???

Use some data to predict house prices (`AER::HousePrices`).
