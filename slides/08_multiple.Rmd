---
title: Multiple linear regression
subtitle: Research methods
author: JÃ¼ri Lillemets
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, dpi = 150, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
# Set colors
Col <- c(red = '#e6457a', green = '#b0e645', blue = '#45cbe6')
```

class: center middle clean

# How to model a relationship with multiple predictors?

---

class: center middle inverse

# Linear regression with multiple predictors

---

## What does a multiple linear regression model look like?

A model with two predictors can be expressed as follows:

$$y = \alpha + \beta_{1} x_1  + \beta_{2} x_2 + \varepsilon$$

where 

- $\beta_1$ is a coefficient of $x_1$ and
- $\beta_2$ is a coefficient of $x_2$.

---

``` {r}
GrowthDJ <- read.csv('/home/jrl/work/resmeth/data/GrowthDJ.csv', row.names = 1)
GrowthDJ[, sapply(GrowthDJ, is.character)] %<>% `==`('yes')
GrowthDJ <- GrowthDJ[complete.cases(GrowthDJ), ]
```

Let's illustrate it with data on "Determinants of Economic Growth".

.small[
- `oil` factor. Is the country an oil-producing country?
- `inter` factor. Does the country have better quality data?
- `oecd` factor. Is the country a member of the OECD?
- `gdp60` Per capita GDP in 1960.
- `gdp85` Per capita GDP in 1985.
- `gdpgrowth` Average growth rate of per capita GDP from 1960 to 1985 (in percent).
- `popgrowth` Average growth rate of working-age population 1960 to 1985 (in percent).
- `invest` Average ratio of investment (including Government Investment) to GDP from 1960 to 1985 (in percent).
- `school` Average fraction of working-age population enrolled in secondary school from 1960 to 1985 (in percent).
- `literacy60` Fraction of the population over 15 years old that is able to read and write in 1960 (in percent).
]

--

Suppose that we wish to know how do population growth and investments impact economic growth.

---

What does the data look like? Here are first 20 rows:

``` {r}
head(GrowthDJ, 20)
```

---

Start with examining (plotting) relationships.

``` {r, fig.height = 5}
par(family = 'RobotoCondensed')
pairs(gdpgrowth ~ ., GrowthDJ[, sapply(GrowthDJ, is.numeric)], 
      gap = .2, cex = .6)
```

???
Can you see something wrong here? An outlier!
Move this point to first regression meeting.

---

Suppose we have a reason to exclude the outlier. 

``` {r, fig.height = 5}
par(family = 'RobotoCondensed')
GrowthDJ <- GrowthDJ[GrowthDJ$gdp60 < 2e4, ]
pairs(gdpgrowth ~ ., GrowthDJ[, sapply(GrowthDJ, is.numeric)], 
      gap = .2, cex = .6)
```

---

Our model is thus

$$gdpgrowth = \alpha + \beta_{1} popgrowth  + \beta_{2} invest + \varepsilon.$$

``` {r}
Model <- lm(gdpgrowth ~ popgrowth + invest, GrowthDJ)
summary(Model)
```

---

What does the model it look like visually?

``` {r}
Popgrowth <- unique(GrowthDJ$popgrowth)
Invest <- unique(GrowthDJ$invest)
grid <- with(GrowthDJ, expand.grid(popgrowth = Popgrowth, invest = Invest))
vals <- predict(Model, newdata = grid)
Model <- matrix(vals, 
            nrow = length(Popgrowth), 
            ncol = length(Invest))
widgetframe::frameWidget(
  plotly::plot_ly() %>% 
    plotly::add_surface(x = ~Invest, y = ~Popgrowth, z = ~Model), 
  height = 600)
```

???
More predictors, more dimensions. If one side is changed, the other also changes: coefficients are often interdependent.

---

## Why use predictors?

### To control for effects

.small[
.pull-left[
``` {r}
Model <- lm(gdpgrowth ~ school, GrowthDJ)
summary(Model)
```
]
.pull-right[
``` {r}
Model <- lm(gdpgrowth ~ school + invest, GrowthDJ)
summary(Model)
```
]
]

Studies often include *control variables*.

???
Common control variables used: gender, education, SES

---

Why did secondary school enrolment alone have an effect but not when controlled for investment?

``` {r, fig.height = 4.5}
par(family = 'RobotoCondensed')
pairs(GrowthDJ[, c('gdpgrowth', 'school', 'invest')], 
      labels = c("Economic growth", "Secondary school enrolment",
                 "Invenstment"))
```

???
Actually, investments have the effect.

---

An effect may only become apparent in combination with other predictors.

.small[
.pull-left[
``` {r}
Model <- lm(gdpgrowth ~ literacy60, GrowthDJ)
summary(Model)
```
]
.pull-right[
``` {r}
Model <- lm(gdpgrowth ~ literacy60 + popgrowth, GrowthDJ)
summary(Model)
```
]
]

???
Coefficients change, too.

---

Why does literacy have no effect alone but does so with population growth?

``` {r, fig.height = 4.5}
par(family = 'RobotoCondensed')
pairs(GrowthDJ[, c('gdpgrowth', 'literacy60', 'popgrowth')], 
      labels = c("Economic growth", "Literacy", "Population growth"))
```

---

### Model fit is improved

.small[
.pull-left[
``` {r}
Model <- lm(gdpgrowth ~ popgrowth, GrowthDJ)
summary(Model)
```
]
.pull-right[
``` {r}
Model <- lm(gdpgrowth ~ ., GrowthDJ)
summary(Model)
```
]
]

---

class: center middle inverse

# Selection of predictors

---

We can start with adding all other variables as predictors.

``` {r}
Model <- lm(gdpgrowth ~ ., GrowthDJ)
summary(Model)
```

---

## Statistical significance

We could remove statistically insignificant variables.

.small[
.pull-left[
``` {r}
Model <- lm(gdpgrowth ~ ., GrowthDJ)
summary(Model)
```
]
.pull-right[
``` {r}
Model <- update(Model, ~inter + gdp60 + gdp85 + popgrowth + invest)
summary(Model)
```
]
]

---

## F-test

This is a test of overall model fit.

F-statistic is calculated as follows:

$$F = \frac{RSS_1 - RSS_1 / k}{RSS_2/(n-p-1)},$$

where $RSS_1$ is the residual sum of squares of reduced model and $RSS_2$ the same for full model, $k$ the difference in the number of parameters between two models, $p$ the number of parameters and $n$ the number of observations.

$H_0: \beta_1 = \beta_2 = 0$  
$H_1: \beta_1 \neq 0 \text{ or } \beta_2 \neq 0$  

---

Are oil producing and OECD membership together significant predictors of economic growth?

``` {r}
Model <- lm(gdpgrowth ~ oil + oecd, GrowthDJ)
summary(Model)
```

--

> How would you interpret the p-value for F-statistic here?

---

## Akaike Information Criterion (AIC)

It's a measure of model goodness of fit.

For linear regression the information criterion is calculated as follows:

$$AIC = n \times log(\frac{RSS}{n}) + 2 K,$$

where $n$ is the number of observations, $RSS$ the residual sums of squares and $K$ the number of predictors in a model.

**Lower** value indicates better fit.

---

Stepwise backwards selection according to AIC

.scroll[
``` {r}
Model <- lm(gdpgrowth ~ ., GrowthDJ)
step(Model)
```
]

---

## Multicollinearity

It should not be possible to linearly predict any of the predictors from others predictors. Predictors should not be (highly) correlated.

--

Let's look at Pearson's correlation coefficients pairwise.

``` {r}
options(width = 100)
sapply(GrowthDJ, as.numeric) %>% cor %>% round(2)
```

> Can you see anything that could result in collinearity?

---

## There is no multicollinearity

It should not be possible to linearly predict any of the predictors from others predictors. Otherwise the **coefficients are not be reliable**.

Multicollinearity can be detected with variance inflation factor (VIF) by using $R^2$ to estimate for each predictor how much of the variation in one predictor can be predicted from others.

$$VIF_{k} = \frac{1}{1 - R^{2}_{k}},$$

where $R^{2}_{k}$ is $R^{2}$ for a model that has initial predictor $k$ as a response and all other predictors as predictors.

---

### Variance inflation factor (VIF)

Multicollinearity can be detected with variance inflation factor by using $R^2$ to estimate for each predictor how much of the variation in one predictor can be predicted from others.

$$VIF_{k} = \frac{1}{1 - R^{2}_{k-1}},$$

where $R^{2}_{k-1}$ is $R^{2}$ for a model that has predictor $k$ as a response variable and all other predictors as predictor variables.

Remove predictors that have VIF >5.

---

What about our predictors that were selected using backwards selection with AIC?

``` {r, echo = T}
Model <- lm(gdpgrowth ~ oil + inter + gdp60 + gdp85 + popgrowth + invest, 
            GrowthDJ)
car::vif(Model)
```

> Would you exclude anything?

--

``` {r, echo = T}
Model <- update(Model, ~  oil + inter + gdp60 + popgrowth + invest)
car::vif(Model)
```

---

What happens when we exclude influential predictors?

.small[
.pull-left[
``` {r}
Model <- lm(gdpgrowth ~ oil + inter + gdp60 + gdp85 + popgrowth + invest, 
            GrowthDJ)
summary(Model)
```
]
.pull-right[
``` {r}
Model <- update(Model, ~  oil + inter + gdp60 + popgrowth + invest)
summary(Model)
```
]
]

---

Our final model.

``` {r}
summary(Model)
```

> How to interpret this?

---

class: center middle inverse

# Presenting the results

---

.compact[
``` {r, results = 'asis'}
stargazer::stargazer(
  lm(gdpgrowth ~ popgrowth + invest, GrowthDJ), 
  lm(gdpgrowth ~ school + invest, GrowthDJ), 
  lm(gdpgrowth ~ literacy60 + popgrowth, GrowthDJ), 
  lm(gdpgrowth ~ ., GrowthDJ), 
  lm(gdpgrowth ~ oil + inter + gdp60 + gdp85 + popgrowth + invest, GrowthDJ), 
  type = 'html')
```
]

---

class: inverse

