---
title: Cluster analysis
subtitle: Research methods
author: JÃ¼ri Lillemets
date: "`r Sys.Date()`"
output_yaml: _output.yml
knit: rmarkdown::render
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, message = F, dpi = 200, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
library('cluster')
library('rgl')
# Options
options(scipen = F, digits = 3)
# Set colors
source('/home/jrl/work/resmeth/slides/variables.R')
setPar <- function() {
  par(bty = 'n', mar = c(4,4,2,0), family = 'RobotoCondensed', 
      pch = 20, cex = 1, col = Col['clear'])
}
```

class: center middle clean

# How to group objects?

---

class: center middle inverse

# What is clustering?

---

## The idea behind clustering

The purpose of cluster analysis is to **categorize objects into some homogeneous groups** 

--

so that **objects within the same groups are more closely related than objects in different groups**.

--

The catogorization is based on **similarities between objects** according to a set of variables.

---

There are various methods for categorizing objects:

- classification, 
- **clustering**, 
  - model-based (e.g. mixture models) methods, 
  - **distance-based (combinatorial) methods**, 
    - **hierarchical clustering**, 
    - **partitioning (K-means clustering)**.

---

### Clustering and classification

In clustering we do not have any information on possible existing classes and we can not compare clusters to classes.

In classification we already know the classes and use that information to determine classification rules.

--

We used classification after estimating a logistic regression model.

---

### Model- and distance-based clustering

Model-based methods assume that an underlying model can explain clusters in data (e.g. mixture models).

Distance-based methods apply distances between objects and clusters to separate objects into clusters.

---

Mixture models assume that objects follow mixture of distributions. Thus objects can be clustered by locating the densities in data. Distance-based methods use distances.

``` {r}
setPar();par(mfrow = c(1,3))
hist(faithful$eruptions, 20, freq = F, main = NA, xlab = "eruptions")
density(faithful$eruptions) %>% lines(col = Col['blue'])
hist(faithful$waiting, 20, freq = F, main = NA, xlab = "waiting")
density(faithful$waiting) %>% lines(col = Col['red'])
plot(faithful)
```

---

### Hierarchical clustering and partitioning

In hierarchical clustering clusters are constructed **incrementally** according to similarities between objects and clusters.

In partitioning objects are assigned to a particular number of groups and the optimal clustering is determined **iteratively**. 

--

While partitioning is intrinsically a *divisive* process, *hierachical clustering* can be applied *agglomeratively* as well as *divisively*. 

---

## Why objects?

It is more common to assign observations to clusters.

However, we can cluster either observations or variables. 

That's why we refer to **objects** as the phenomena to be assigned to clusters.

---

## Standardization

Variables with higher variances have a higher influence on how the objects are clustered. 

If this is not desired, variables should be standardized prior to clustering.

Conversely, sometimes it might be desirable to give more weight to particular variables.

---

Why should we standardize?

``` {r}
data(cloth, package = 'boot')
setPar(); par(mfrow = 1:2)
plot(cloth, main = "Raw data", 
     xlab = "The length of the roll of cloth", 
     ylab = "The number of flaws found in the roll", 
     pch = 20, asp = 1)
plot(scale(cloth), main = "Scaled data", 
     xlab = "The length of the roll of cloth", 
     ylab = "The number of flaws found in the roll", 
     pch = 20, asp = 1)
```

--

The distances between objects are scale-dependent.

---

## Application

Clustering can be applied in practice for various purposes.

Marketing and sales - find homogeneous groups of customers so that promotional campaigns could be addresses more accurately and thus more efficiently.

Medicine - cluster patients with similar symptoms or predispositions for treatment or discovery of risk.

Finance - categorize enterprises into different types based on some financial or other characteristics.

Biology - assign plants to species depending on characteristics they share.

--

We can cluster anything we want.

---

## Distance measures

The assignment of objects into groups should be such that observations are more similar within groups than between groups. 

--

.pull-left[
We need to somehow measure the distances between all objects.
]


.pull-right.small[
``` {r}
head(cloth, 10) %>% kable(col.names = c("Length", "Flaws"))
```
]

---

### Distance matrix

For data matrix $X : n \times p$ with $n$ observations and $p$ variables the distances $d$ between objects $i$ and $j$ can be described as a *proximity matrix* or a *distance or matrix* $D : n \times n$ where $d_{ij} = d(x_i,x_j)$. 

--

We usually need to calculate this.

---

A distance matrix contains pairwise distances between all objects.

``` {r}
dist(cloth) %>% as.matrix %>% .[1:10, 1:10] %>% kable
```

---

We look at the most common measures for continuous variables, the Euclidean and Manhattan distances: 

$$d_{Euclidean}(x_i,x_j) = [\sum^p_{k=1}(x_{ik} - x_{jk})^2]^{1/2},$$

$$d_{Manhattan}(x_i,x_j) = \sum^p_{k=1} |x_{ik} - x_{jk}|.$$

--

Distance measures for ordinal and nominal variables also exist but are not explained here.

---

Example data on the number of flaws in cloth for 32 pieces of cloth.

``` {r}
# Visualizatin of Euclidean and Manhattan distances
x <- cloth[c(6,27), 1]; y <- cloth[c(6,27), 2]
setPar();par(mfrow = 1:2)
# Euclidean
plot(cloth, main = "Euclidean distance", 
     xlab = "The length of the roll of cloth", 
     ylab = "The number of flaws found in the roll", pch = 20)
lines(x[1:2], y[1:2], col = Col['red'], lwd = 2)
# Manhattan
plot(cloth, main = "Manhattan distance", 
     xlab = "The length of the roll of cloth", 
     ylab = "The number of flaws found in the roll", pch = 20)
lines(c(x[1], x[2]), c(y[1], y[1]), col = Col['red'], lwd = 2)
lines(c(x[2], x[2]), c(y[1], y[2]), col = Col['red'], lwd = 2)
```

---

class: center middle inverse

# Hierarchical clustering

---

Clusters are constructed **incrementally**. This can be done

- **divisive**ly ("top-down") where we begin with a single cluster and divide it into smaller clusters and eventually into objects or
- **agglomerative**ly ("bottom-up") where we start from combining objects into clusters and eventually have a single cluster.

We will explore the agglomerative hierarchical clustering (*Agnes* - agglomerative nesting).

---

The example data we use contains on 908 measurements on three hawk species.

.small[
``` {r}
data('Hawks', package = 'Stat2Data')
tail(Hawks[10:19], 10) %>% kable
```
]

---

We will use the following variables to cluster the hawks.

- `Wing`	Length (in mm) of primary wing feather from tip to wrist it attaches to
- `Weight`	Body weight (in gm)
- `Culmen`	Length (in mm) of the upper bill from the tip to where it bumps into the fleshy part of the bird
- `Hallux`	Length (in mm) of the killing talon
- `Tail`	Measurement (in mm) related to the length of the tail (invented at the MacBride Raptor Center)
- `StandardTail`	Standard measurement of tail length (in mm)
- `Tarsus`	Length of the basic foot bone (in mm)
- `WingPitFat`	Amount of fat in the wing pit
- `KeelFat`	Amount of fat on the breastbone (measured by feel
- `Crop`	Amount of material in the crop, coded from 1=full to 0=empty

---


## Process

Agglomerative hierarchical clustering has the following steps.

1. Initial number of clusters is $n$, so each cluster contains one object. 
2. Calculate distance matrix $D$ that expresses pairwise distances between clusters (objects).
3. Find the smallest distance and merge the objects with smallest distance into a single cluster.
4. Calculate a new distance matrix $D$ that now includes distance between the new cluster and all other clusters using a linkage method (see below).
5. Repeat the previous two steps until all objects are in a single cluster.

---

Let's illustrate the process with two variables and 5 hawks.

``` {r}
Sample <- Hawks[6:11, c(10,14)]
setPar();par(mar = c(4,4,0,0))
plot(Sample, 
     xlab = "Wing length, mm", ylab = "Tail length, mm", 
     pch = 20, cex = 2,  asp = 1)
text(Sample, rownames(Sample), 0, 2, col = 'black')
```

---

Here's the part of the respective initial Euclidean distance matrix $D$ that represent pairwise distances between hawks.

``` {r}
dist(Sample) %>% as.matrix %>% kable
```

---

What if we have more than two variables? For example three?

``` {r}
setPar()
plot3d(Hawks[6:11, c(10,11,14)], 
            col = Col['clear'], size = 2, type = 's', pch = 20, asp = 1)
rglwidget(width = 800, height = 450)
```

---

## Linkage methods

How to calculate distance between cluster and a point?

- single linkage: $d_{IJ,k} = min(d_{i,K}, d_{j,K})$;
- complete linkage: $d_{IJ,k} = max(d_{I,K}, d_{J,K})$; 
- average linkage: $d_{IJ,k} = \sum_{i \in IJ} \sum_{k \in K} d_{ik} / (n_{ij}n_k)$; 
- Ward's method: compares the within-cluster and between-cluster squared distances.

---

How to think about single, complete and average linkage?

``` {r}
setPar();par(mar = c(4,4,0,0))
plot(Sample, 
     xlab = "Wing length, mm", ylab = "Tail length, mm", 
     pch = 20, cex = 2,  asp = 1)
text(Sample, rownames(Sample), 0, 2, col = 'black')
```

???
Draw linkage results.

---

Single linkage tends to link objects serially, resulting in **clusters with large diameter** where objects within a cluster are not similar. 

Complete linkage has the tendency to produce **clusters with small diameter** and as a result, an object can be closer to members of another cluster. 

Average linkage is a **compromise between the two** but is sensitive to the scale on which distances are measured.

---

There is no correct linkage method.

![](../img/wrongdata.gif)

---

## Dendrogram

The clusters in case of hierarchical clustering are estimated incrementally, resulting in a **nested structure**. 

This tree-shaped structure can be visualized by a **dendrogram**.

Dendrogam is highly intrepretative and provides complete description of the clustering process.

---

``` {r} 
setPar();par(mfrow = 1:2)
i <- 'ward.D2'
dist(Sample) %>% hclust(method = i) %>% 
  plot(main = paste("Linkage:", toupper(i)), col = 'black')
plot(Sample, 
     xlab = "Wing length, mm", ylab = "Tail length, mm", 
     pch = 20, cex = 2,  asp = 1)
text(Sample, rownames(Sample), 0, 2, col = 'black')
```

---

Dendrogram allows us to also illustrate differences between linkage methods.

``` {r} 
setPar();par(mfrow = c(1,4), mar = c(0,4,2,0))
for (i in c('single', 'complete', 'average', 'ward.D2')) {
  dist(Sample) %>% hclust(method = i) %>% 
    plot(main = paste("Linkage:", toupper(i)), col = 'black')
}
```

---

## Number of clusters

In hierarchical clustering we can decide the suitable number of clusters after the clustering procedure.

The descision can be made by examining dendrogram.

We can find the longest consecutive height and cut between the ends of that at the point where another heights are also the longest.

---

To determine the clusters we can choose either the **height of cut** or **number of clusters**. 

``` {r} 
setPar();par(mar = c(0,4,4,0))
Clust <- dist(Hawks[1:100, 10:19]) %>% hclust(method = 'ward.D2')
plot(Clust, cex = .5)
```

---

If we would cut at 1000 then we would obtain `r max(cutree(Clust, h = 1000))` clusters.

``` {r} 
setPar();par(mar = c(0,4,4,0))
Clust <- dist(Hawks[1:100, 10:19]) %>% hclust(method = 'ward.D2')
plot(Clust, cex = .5)
abline(h = 1000, col = Col['red'])
```

--

> At what height would we have to cut if we wished to obtain 3 clusters?

---

Actually, the species for each Hawk is already determined. How does it coincide with our clusters?

``` {r}
table(dist(Hawks[10:19]) %>% hclust(method = 'ward.D2') %>% cutree(3), 
      Hawks$Species) %>% 
  kable(row.names = T)
```

---

class: center middle inverse

# K-means clustering

---

Clusters are constructed by *partitioning* objects *iteratively*.

The **number of clusters $K$ has to be defined before estimation**. 

The goal is to partition objects $x$ into $K$ clusters so that distances between objects within cluster are small compared to distances to points outside the cluster. 

We can achieve this by assigning each object to the closest **centroid**, i.e. cluster mean.

---

We thus need optimal cluster means. The optimal mean vector $\bar x_1,  ... \bar x_K$ can be found by minimizing the following function:

$$ESS = \sum^K_{k = 1} \sum_{c(i)=k} (x_i - \bar  x_k)^T(x_i - \bar x_k),$$

where $c(i)$ is the cluster containing $x_i$. 

--

An alternative is **K-medoids clustering** in which case the centroids are not some mean values but represented by actual objects.

???
Number of clusters has to be defined before clustering.
We attempt to minimize the sum of distances within all clusters

---

Let's attempt to cluster 392 vehicles.

``` {r}
data('Auto', package = 'ISLR')
head(Auto[, 1:7], 4) %>% kable
```

--

We should scale the variables.

``` {r}
Oie <- scale(Auto[, 1:7])
head(Oie, 4) %>% kable
```

---

We have 7 variables, so 7 dimensions. Let's use PCs to represent the data.

``` {r}
setPar()
Auto[, 1:7] %>% pairs(pch = 20, cex = .5, oma = c(2,2,2,2))
```

---

What do we mean by "partitioning"?

``` {r}
PCA <- prcomp(Oie)
PCs <- PCA$x[, 1:2]
setPar();par(pty = 's')
plot(PCs, pch = 20)
```

---

## Process

K-means clustering involves the following steps:

1. We start with a distance matrix $D$ based on 
   - random assignment of objects to $K$ clusters with cluster means, or
   - some (random) cluster means.
2. Calculate squared Euclidean distance between each object and each cluster mean. Reassign each item to its nearest cluster mean, resulting in decreased $ESS$.
3. Update cluster means.
4. Repeat the previous two steps until objects can not be reassigned, so each object is closest to its own cluster mean.

---

Can we see any clusters if we summarize the data into PCs?

``` {r}
setPar();par(pty = 's')
plot(PCs, pch = 20)
```

--

> How many clusters would you distinguish?

---

## Number of clusters (Gap statistic)

The Gap statistic is a technique used to determine the optimal number of clusters.

The measure compares the sum of average distances within cluster to the same sum obtained from uniformly distributed data. 

The optimal number of clusters is at the value of the Gap statistic $k$ where $k + se(k)$ is higher or equal to the estimate $k$ of next number of clusters.

---

``` {r}
kmFun <- function(x, k) kmeans(x, k, iter.max = 50, nstart = 10)
kmG <- clusGap(Oie, kmFun, K.max = 8, B = 10)
par(bty = 'n', family = 'RobotoCondensed')
plot(kmG)
```

> How many clusters should we estimate?

---

We get a better picture the more dimensons we look at.

``` {r}
setPar()
plot3d(prcomp(Oie)$x[, 1:3], 
       col = Col['clear'], size = 1, type = 's', pch = 20, asp = 1)
rglwidget(width = 800, height = 450)
```

---

Let's estimate 4 clusters as suggested by the Gap statistic. Depending on the centers we choose, we obtain different clusters.

``` {r}
setPar();par(pty = 's', mfrow = c(2,3))
replicate(6, {
  Kmeans <- kmeans(PCs, centers = 4, iter.max = 1, nstart = 1)
  plot(PCs, pch = 20, col = rainbow(4)[Kmeans$cluster])
  Kmeans$centers %>% points(col = 'black', pch = 4, lwd = 2)
  }) %>% invisible # To hide printing from replicate
```

???
Cluster number is arbitrary. It should not converge to illustrate that we may get different results!

---

We actually have 9 dimensions instead of 2. But we can only depict 3 at most.

``` {r}
setPar()
Kmeans <- kmeans(PCs, 4, iter.max = 5, nstart = 1)
plot3d(prcomp(Oie)$x[, 1:3], 
       col = rainbow(4)[Kmeans$cluster], size = 1, type = 's', pch = 20, asp = 1)
rglwidget(width = 800, height = 450)
```

---

## Cluster plot

When we apply K-means clustering we do not obtain a nested structure and therefore can not express the clustering as a dendrogram. 

We can create a plot that shows the locations of objects and cluster centroids on a two-dimensional plot.

The dimensions can be found via PCA or multidimensional scaling.

---

Here is the final estimate if we run 10 iterations and 10 different starting values.

``` {r}
par(bty = 'n', family = 'RobotoCondensed', pty = 's', mar = c(4,4,0,0))
Kmeans <- kmeans(PCs, 4, iter.max = 10, nstart = 10)
plot(PCs, pch = 20, col = rainbow(4)[Kmeans$cluster])
Kmeans$centers %>% points(col = 'black', pch = 4, lwd = 2)
```

---

class: center middle inverse

# How to use clusters?

---

Save clusters as a new variable.

.smaller[
``` {r}
cbind(Auto, Cluster = Kmeans$cluster) %>% head %>% kable
```
]

---

You can describe clusters using descriptive statistics, e.g. mean values as below

.small[
``` {r}
aggregate(Auto[, 1:7], by = list(Cluster = Kmeans$cluster), FUN = mean) %>% kable
```
]

---

Or create some visualizations.

``` {r, fig.height = 4.5}
par(bty = 'n', family = 'RobotoCondensed', mfrow = c(2,3), mar = c(4,4,2,2))
for (i in c(1,3:7)) {
  boxplot(Auto[, i] ~ Kmeans$cluster, xlab = "Cluster", ylab = names(Auto)[i], 
          col = rainbow(4)[unique(Kmeans$cluster)])
}
```

---

class: inverse
