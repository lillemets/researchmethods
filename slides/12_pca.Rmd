---
title: Principal component analysis
subtitle: Research methods
author: Jüri Lillemets
date: "`r Sys.Date()`"
output_yaml: _output.yml
editor_options: 
  chunk_output_type: console
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, dpi = 200, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
# Options
options(scipen = F, digits = 3)
# Set colors
Col <- c(red = '#e6457a', green = '#b0e645', blue = '#45cbe6')
```

class: center middle clean

# How to summarize variables?

---

class: center middle inverse

# Intuition

---

## Multivariate statistics

There are no response or predictor variables. 

Response variable is not predicted from a model.

Instead we examine the overall structure of data to find patterns. 

Principal component analysis (PCA) is one of the methods in the field of multivariate statistics. 

---

## Principal component analysis

The aim is to summarize many variables into one, few or several new variables.

This is done by determining a set of standardized **linear combinations** that would explain a maximum amount of variation in data.

A **dimensionality reduction** technique.

The maximum number of PCs we can determine is $min(p, n - 1)$. 

---

Let's use data on top performances in the *Decathlon* from 1985 to 2006 and examine results for each event in 2000.

``` {r}
Decathlon <- GDAdata::Decathlon
Decathlon <- Decathlon[Decathlon$yearEvent == 2000, ]
Oie <- Decathlon[, 4:13]
names(Oie) <- c('run100', 'longjump', 'shotput', 'highjump', 'run400', 'run110hurdle', 'polevault', 'discus', 'javelin', 'run1500')
Oie[, c(1,5,6,10)] <- Oie[, c(1,5,6,10)] * -1
rownames(Oie) <- Decathlon$DecathleteName
```

Below are first rows.

.smaller[
``` {r}
kable(head(Oie))
```
]

--

> How could we summarise all of these events into a single score?

---

### Linear combinations

Each linear combination is called a principal component (PC). A $j$th PC can be expressed as linear combination $\xi_j$ as:

$$\xi_j = b_{j1}x_1 + b_{j2}x_2+ ... a_{jp}x_p,$$ 

where $b_{jp}$ is a weight of variable $x_p$ of a particular $\xi_j$. 

.smaller[
``` {r}
kable(head(Oie))
```
]

???
PCs are thus essentially weighted sum of variables.
We need to give each sports event some weight. 

---

## Estimation of PCs

We can think of the estimation as a process starting from estimation of first PC.

The first PC is chosen so that

1. it follows the direction of largest variance in data cloud and
2. it is a line with smallest orthogonal distance to all data points.

Each following PC is chosen so that it is orthogonal to preceding PC.

---

Suppose we have two variables:

```{r, fig.height = 4.5}
Vars <- Oie[, c('longjump', 'highjump')]
plotEst <- function() {
  par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), pty = 's')
  plot(Vars[[1]] ~ Vars[[2]], xlab = "Long jump", ylab = "High jump",
       cex = .6)
}
plotEst()
```

---

The line that maximizes variation and has smallest orthogonal distance to data points is the first PC.

``` {r}
par(mfrow = 1:2)
# Data cloud
plotEst()
# First PC
Vars <- Vars %>% lapply(scale) %>% data.frame
eigHiLo <- cov(Vars) %>% eigen
plotEst()
eigHiLo <- cov(Vars) %>% eigen
lines(Vars[[2]], (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * (Vars[[2]] - mean(Vars[[2]])) + mean(Vars[[1]])), 
      col = Col['red'])
```

--

> What does this line remind you of?

---

It's theoretically similar to least squares estimation, but mathematically different.

```{r}
# First PC and linear regression
plotEst()
lines(Vars[[2]], (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * (Vars[[2]] - mean(Vars[[2]])) + mean(Vars[[1]])), 
      col = Col['red'])
lmHiLo <- lm(Vars[[1]] ~ Vars[[2]])
abline(lmHiLo, col = Col['blue'])
```

???

Except that in PCA we consider variation in all variables (not just the response)

---

Each following PC is orthogonal (perpendicular) to the first PC.

``` {r}
par(mfrow = 1:2)
# First PC
plotEst()
lines(Vars[[2]], (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * (Vars[[2]] - mean(Vars[[2]])) + mean(Vars[[1]])), 
      col = Col['red'])
# First and second PC
plotEst()
lines(Vars[[2]], (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * (Vars[[2]] - mean(Vars[[2]])) + mean(Vars[[1]])), 
      col = Col['red'])
lines(Vars[[1]], (eigHiLo$vec[1,2]/eigHiLo$vec[2,2] * (Vars[[1]] - mean(Vars[[1]])) + mean(Vars[[2]])), 
      col = Col['blue'])
```

---

The process involves transforming data points to a new coordinate system: 

1. centering data points, 
2. scaling the axes so that they would be equal, 
3. rotating axes into the direction of a PC.

---

This is what transforming data to a new coordinate system means:

``` {r}
# First and second PC
par(mfrow = 1:2)
plotEst()
lines(Vars[[2]], (eigHiLo$vec[2,1]/eigHiLo$vec[1,1] * (Vars[[2]] - mean(Vars[[2]])) + mean(Vars[[1]])), 
      col = Col['red'])
lines(Vars[[1]], (eigHiLo$vec[1,2]/eigHiLo$vec[2,2] * (Vars[[1]] - mean(Vars[[1]])) + mean(Vars[[2]])), 
      col = Col['blue'])
# First and second PC rotated
pcaHiLo <- prcomp(Vars, scale = T)
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), pty = 's')
plot(pcaHiLo$x, cex = .6)
abline(h = 0, col = Col['red'])
abline(v = 0, col = Col['blue'])
```

???
How to interpret?

---

How do we know how exactly to scale and rotate axes? 

We use some parameters derived from data:

- for scaling we use **eigenvalues** and 
- for rotating we use **eigenvectors**.

---

## Calculation

There are two methods for calculating eigenvalues and eigenvectors from data, depending on whether we use 

1. covariance or correlation matrix $\Sigma$ of data matrix $X : n \times p$ or
2. the raw data matrix $X : n \times p$.

### Spectral decomposition

This calculation uses correlation or covariance matrix $\Sigma$.

Because $\Sigma = V D V^T$, we can find eigenvalues from $D$ and corresponding eigenvectors from $V$.

Computationally more simple

---

### Singular value decomposition

This calculation uses data matrix with raw values $X : n \times p$.

Because $X = UDV^T$, we can find eigenvalues from $D$ and corresponding eigenvectors from $U$.

Numerically more stable

---

Because PCA involves maximizing variation, variables with higher variance are given more importance. Consider scaling variables: $x\prime = (x - \mu) / \sigma$ prior to estimation.

``` {r}
# Example of scales
par(bty = 'n', family = 'RobotoCondensed', mar = c(6,4,1,0), mfrow = c(1,3), cex = .8)
boxplot(Oie, main = "Raw data", las = 2)
boxplot(lapply(Oie, function(x) x - mean(x)), main = "Normalized data", las = 2)
boxplot(scale(Oie), main = "Scaled data", las = 2)
```

---

## Number of PCs

Aside from theoretical considerations, there also several possible criteria for determining the number of estimated PCs:

1. choose a threshold of variation in data that PCs should cumulatively explain, e.g. 50%; 
2. use PCs that have eigenvalue above average (one); 
3. use PCs that are above an "elbow" in scree plot.

---

The choice can be made by looking at scree plots.

``` {r}
pcDec <- prcomp(Oie, scale = T)
```

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mfrow = c(1,3))
plot(pcDec$sdev^2 * 10, type = 'b', 
     xlab = "Principal component", ylab = "Variation explained, %")
plot(cumsum(pcDec$sdev^2 * 10), type = 'b', 
     xlab = "Principal component", ylab = "Cumulative variation explained, %")
plot(pcDec$sdev^2, type = 'p', 
     xlab = "Principal component", ylab = "Eigenvalue")
abline(h = 1)
```

---

class: center middle inverse

# Interpretation

---

## Variation explained

Total variation explained by PCs illustrates how much information is preserved when summarizing data into a particular number of PCs.

``` {r}
kable(t(cbind(`Variation explained, %` = pcDec$sdev^2 * 10, 
              `Cumulative variation, %` = cumsum(pcDec$sdev^2 * 10))), 
      col.names = attributes(pcDec$x)$dimnames[[2]],
      digits = 0)
```

--

> How much of total variation in data do the first two PCs explain? 

---

## Loadings

Variables "load" PCs and thus each variable has a particular loading on each PC. These can be understood as weights.

.smaller[
``` {r}
kable(pcDec$rotation)
```
]

--

> How would you describe these PCs?

???
Loadings can be used to give a theoretical meaning to each PC. This is creative.

---

### Scores

Principal component scores are the original variables weighted by PCs.

Recall the representation of linear combinations $\xi_j = b_{j1}x_1 + b_{j2}x_2+ ... a_{jp}x_p$.

.smaller[
``` {r}
kable(head(pcDec$x, 10))
```
]

---

## Biplot

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,2,4))
biplot(pcDec, xlabs = rep('·', nrow(Oie)), cex = .8)
```

--

> How would you interpret these first two PCs?

---

How are our results related to actual points given to athletes?

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), mfrow = c(1,3))
for (i in 1:3) {
  plot(pcDec$x[, i], Decathlon$Totalpoints, 
       xlab = paste0("PC", i), ylab = "Points awarded")
}
```

???
It seems like technical skills are given higher importance. Methodology for total points calculation is from 1985.

---

class: inverse
