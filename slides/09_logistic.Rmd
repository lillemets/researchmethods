---
title: Logistic regression
subtitle: Research methods
author: JÃ¼ri Lillemets
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, dpi = 200, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
# Set colors
Col <- c(red = '#e6457a', green = '#b0e645', blue = '#45cbe6')
```

class: center middle clean

# How to model a binary response variable?

---

class: center middle inverse

# Linear regression with binary response

---

Let's use a data set on spam email.

``` {r}
Oie <- DAAG::spam7
Oie$spam <- as.numeric(Oie$yesno == 'y')
#Oie <- Oie[!(Oie$spam == 1 & Oie$dollar == 0), ] # Remove when spam but no dollar signs
Oie <- Oie[Oie$dollar < quantile(Oie$dollar, .99), ] # Include only lowest 99%
```

- `crl.tot` total length of words in capitals
- `dollar` number of occurrences of the "$" symbol
- `bang` number of occurrences of the "!" symbol
- `money` number of occurrences of the word "money"
- `n000` number of occurrences of the string "000"
- `make` number of occurrences of the word "make"
- `yesno` outcome variable, a factor with levels n not spam, y spam

---

Do occurrences of dollar symbol have an effect on categorizing an email as spam? Can we predict spam from dollar signs?

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0))
boxplot(dollar ~ spam, Oie, 
        xlab = "Number of occurrences of the $ symbol", ylab = "Spam", 
        horizontal = T)
```

---

Let's estimate a linear model where response `spam` is predicted from `dollar`.

``` {r}
modLin <- lm(spam ~ dollar, Oie)
summary(modLin)
```

---

This is how an ordinary linear model fits.

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0))
plotSpam <- function() {
  plot(jitter(spam, amount = .05) ~ dollar, Oie, 
     xlab = "Number of occurrences of the $ symbol", ylab = "Spam",
     yaxt = 'n', cex = .2)
  axis(2, 0:1, 0:1)
}
plotSpam()
abline(modLin, col = Col['red'], lwd = 2)
```

> Is this a good fit?

---

What about the assumptions?

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0), mfrow = 1:2)
plot(modLin, 1:2)
```

--

> Are the assumptions of normality and constant variance of residuals satisfied?

---

How are the errors distributed?

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0))
plot(density(modLin$resid),
     main = NA, xlab = "Residuals", ylab = '')
```

--

Assumptions of linear regression model can not be met when response is binary.

---

class: center middle inverse

# Estimation of generalized linear models

---

What to do if the distribution of errors is such that it can not meet assumptions of linear regression?

--

Specify a distribution of errors! For binary data we shoult use binomial distribution.

``` {r, fig.height = 3.5}
par(bty = 'n', family = 'RobotoCondensed', mfrow = 1:2)
hist(Oie$spam,
     main = "Original distribution", xlab = "Spam", ylab = '')
hist(rbinom(nrow(Oie), nrow(Oie), mean(Oie$spam)),
     main = paste0("Binomial distribution\n (p = ", round(mean(Oie$spam), 3), " , n = ", nrow(Oie), ")"), 
     xlab = "Spam (number of spam)", ylab = '')
```

---

## Generalized linear models (GLM)

Least squares requires normal distribution of errors. So we can not use this. 

Generalized linear models allow us to specify an error distribution prior to estimation.

--

Generalized linear models can also be used for other than binary response variable, e.g.

- normal data, 
- data with gamma distribution, 
- count data (incl. zero-inflated or zero-truncated response), 
- truncated or censored data.

???

Application of models is creative.

---

Let's estimate the same model now as a generalized linear model.

``` {r}
modLog <- glm(spam ~ dollar, Oie, family = binomial(link = 'logit'))
summary(modLog)
```

---

## Logit link

Link function relates the mean value of response to predictor variable(s), allowing us to apply a particular distribution of errors.

Link function transforms the mean of response variable.

--

The logit link is mathematically expressed as follows:

$$ln(\frac{p}{1 - p}) = \alpha + \beta x,$$

where $p$ is the probability that $y = 1$.

---

What does the generalized version of the model look like?

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0))
plotSpam()
abline(modLin, col = Col['red'], lwd = 2)
curve(predict(modLog, list(dollar = x), type = 'response'), 
      min(Oie$dollar), max(Oie$dollar), 
      add = T, col = Col['blue'], lwd = 2)
```

--

> Why is it better than linear model?

---

A better illustration: predict transmission of a car from its weight.

``` {r}
par(bty = 'n', family = 'RobotoCondensed', mar = c(4,4,0,0))
plot(jitter(am, amount = .05) ~ wt, mtcars, 
     xlab = "Weight (1000 lbs)", ylab = "Transmission (0 = automatic, 1 = manual)",
     yaxt = 'n')
axis(2, 0:1, 0:1)
carMod <- glm(am ~ wt, data = mtcars, family = binomial(link = 'logit'))
curve(predict(carMod, list(wt = x), type = 'response'), 
      min(mtcars$wt), max(mtcars$wt), 
      add = T, col = Col['blue'], lwd = 2)
```

---

## Maximum likelihood estimation (MLE)

The parameters of a GLM can not be simply calculated using the least squares method. 

--

Models with various parameters are **iteratively** fitted until we find such parameters for which the data we have is most likely.

In case of OLS: Data -> Parameters  
For MLE: Parameters -> Data

---

### Likelihood

Likelihood expresses how likely is the data we have given the parameters.

It can not be directly interpreted but it is used to calculate measures of model fit.

---

class: center middle inverse

# Interpretation and diagnostics

---

## Coefficients

The $\beta$ coefficient is the increase in logged odds that $y = 1$.

Recall the logit link $ln(\frac{p}{1 - p}) = \alpha + \beta x$, where $p$ is the probability that $y = 1$.

We are estimating log odds ratio: $\frac{p}{1-p} = e^{\alpha + \beta x}$.

Exponent of a coefficient is thus odds ratio: $\frac{p}{1-p} = \frac{p(y = 1)}{(y \neq 1)}$.

If odds ratio is $\frac{1}{2} = 0.5$, then a unit increase of predictor **decreases** the odds of $y = 1$ twice. If odds ratio is $\frac{2}{1} = 2$, then a unit increase of predictor **increases** the odds of $y = 1$ twice.

---

``` {r}
summary(modLog)
```

--

> Does the number of dollar symbols have an effect of email categorized as spam?

--

> Does an increase in the number of dollar symbols decrease the odds of email categorized as spam?

---

class: middle

.center(![increased_risk](../img/increased_risk.png))

.footnote[Source: XKCD,  
increased risk]

---

## Model fit

### Pseudo $R^2$

An alternative of $R^2$ for linear regression, although calculation and interpretation is different:

$$R^2 = 1 - \frac{ln\theta_{reduced}}{ln\theta_{full}},$$

where $\theta_{reduced}$ is the likelihood of model with only intercept and $\theta_{full}$ likelihood of model with predictors. 

The values lie between 0 and 1 and usually values  $>0.4$ are considered to indicate good fit.

???

How much does the addition of predictors improve the model.

---

### Likelihood ratio test

This test can be used to determine if a complex model is different from a simpler one:

$$LR = -2ln(\frac{\theta_1}{\theta_2}),$$

where $\theta_1$ is the likelihood of simpler model and $\theta_2$ the likelihood of complex model. The hypotheses are as follows:

$H_0: L_1 = L_2$  
$H_1: L_1 \neq L_2.$

If difference can not be shown, then the simpler model should be preferred. 

---

## Classification

### Probabilities

Fitte values of logistic regression model are $p$, i.e. the probability that $y = 1$:

$$p = \frac{e^{\alpha + \beta x}}{1 + e^{\alpha + \beta x}}$$

Value of $p$ lies between 0 and 1. 

For classification we thus need to choose a cutoff point.

---

If cutoff point is .5

``` {r}
Dimensions <- c("Observed", "Classified")
table(Oie$spam, as.numeric(modLog$fitted > .3), dnn = Dimensions)
```

If cutoff point is .2

``` {r}
table(Oie$spam, as.numeric(modLog$fitted > .5), dnn = Dimensions)
```

If cutoff point is .1

``` {r}
table(Oie$spam, as.numeric(modLog$fitted > .7), dnn = Dimensions)
```

---

### Confusion matrix

Classification table or a confusion matrix demonstrates the classification results.

``` {r}
addmargins(table(Oie$spam, as.numeric(modLog$fitted > .5), dnn = Dimensions))
```

We can calculate various measures from this table:

- accuracy: $(2676 + 884) / 4554 = 0.78$; 
- sensitivity: $883 / 993 = 0.89$; 
- specifity: $2676 / 3561 = 0.75$;
- ...

---

class: inverse

???

Illustration of maximizing likelihood. Explanation of odds from a contingency table. ROC and AUC. Deviance and other residuals.

# Labor Force Participation Data. Cross-section data originating from the 1976 Panel Study of Income Dynamics (PSID), based on data for the previous year, 1975.

.small[
- `participation` Factor. Did the individual participate in the labor force in 1975? ...
- `hours` Wife's hours of work in 1975.
- `youngkids` Number of children less than 6 years old in household.
- `oldkids` Number of children between ages 6 and 18 in household.
- `age` Wife's age in years.
- `education` Wife's education in years.
- `wage` Wife's average hourly wage, in 1975 dollars.
- `repwage` Wife's wage reported at the time of the 1976 interview ...
- `hhours` Husband's hours worked in 1975.
- `hage` Husband's age in years.
- `heducation` Husband's education in years.
- `hwage` Husband's wage, in 1975 dollars.
- `fincome` Family income, in 1975 dollars. ...
- `tax` Marginal tax rate facing the wife, ...
- `meducation` Wife's mother's educational attainment, in years.
- `feducation` Wife's father's educational attainment, in years.
- `unemp` Unemployment rate in county of residence, in percentage points. ...
- `city` Factor. Does the individual live in a large city?
- `experience` Actual years of wife's previous labor market experience.
- `college` Factor. Did the individual attend college?
- `hcollege` Factor. Did the individual's husband attend college?
]

# Airbag and other influences on accident fatalities

``` {r}
Oie <- DAAG::nassCDS
Oie$dead <- as.numeric(Oie$dead == 'dead')
?DAAG::nassCDS
```
.small[
- `dvcat` ordered factor with levels (estimated impact speeds) 1-9km/h, 10-24, 25-39, 40-54, 55+
- `weight` Observation weights, albeit of uncertain accuracy, designed to account for varying sampling probabilities.
- `dead` factor with levels alive dead
- `airbag` a factor with levels none airbag
- `seatbelt` a factor with levels none belted
- `frontal` a numeric vector; 0 = non-frontal, 1=frontal impact
- `sex` a factor with levels f m
- `ageOFocc` age of occupant in years
- `yearacc` year of accident
- `yearVeh` Year of model of vehicle; a numeric vector
- `abcat` Did one or more (driver or passenger) airbag(s) deploy? This factor has levels deploy nodeploy unavail
- `occRole` a factor with levels driver pass
- `deploy` a numeric vector: 0 if an airbag was unavailable or did not deploy; 1 if one or more bags deployed.
- `injSeverity` a numeric vector; 0:none, 1:possible injury, 2:no incapacity, 3:incapacity, 4:killed; 5:unknown, 6:prior death
- `caseid` character, created by pasting together the populations sampling unit, the case number, and the vehicle number. Within each year, use this to uniquely identify the vehicle.
]
