---
title: Comparing numerical data
subtitle: Research methods
author: JÃ¼ri Lillemets
date: "`r Sys.Date()`"
knit: rmarkdown::render
---

``` {r setup, include = F}
# Settings
knitr::opts_chunk$set(echo = F, warning = F, dpi = 150, fig.height = 4)
# Load packages
library('magrittr');library('knitr')
# Set colors
source('/home/jrl/work/resmeth/slides/variables.R')
# TODO: Add some explanation on why this is relevant and also explain randomized controlled trials.
```

class: center middle clean

# Are the numbers different enough?

---

class: center middle inverse

# The idea behind comparing numbers

---

## One sample T-test

Assumptions:

- normality
- independence

---

Let's use "Diameter, Height and Volume for Black Cherry Trees". The mean height of trees in our sample is `r mean(trees$Height)`. Is this different from some hypothetical population mean value?

``` {r}
makePlot <- function() {
  par(mar = c(4,0,0,0), bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
  plot(density(trees$Height), 
       main = NA, xlab = 'Tree height, ft', ylab = '', 
       ylim = c(0,.07))
}
makePlot()
abline(v = mean(trees$Height), lwd = 2, col = Col['red'])
```

---

It would be reasonable to guess that mean tree height in population is be 75 ft.

```{r}
makePlot()
abline(v = mean(trees$Height), lwd = 2, col = Col['red'])
abline(v = 75, lwd = 2, col = Col['blue'])
```

---

However, if we would be much more uncertain to estimate that the mean tree height in population is 74 ft.

```{r}
makePlot()
abline(v = mean(trees$Height), lwd = 2, col = Col['red'])
abline(v = 74, lwd = 2, col = Col['blue'])
```

---

Another way to ask this: does the samples come from a population with a mean tree height of 74 ft?

```{r}
makePlot()
abline(v = mean(trees$Height), lwd = 2, col = Col['red'])
sdTree <- sqrt((length(trees$Height)-1)/length(trees$Height)) * sd(trees$Height)
curve(dnorm(x, 74, sdTree), add = T, col = Col['blue'])
abline(v = 74, lwd = 2, col = Col['blue'])
```

---

So our statistical hypothesis would be

$H_0: \bar{x} = \mu$  
$H_1: \bar{x} \neq \mu$

or

$H_0: \mu = 74$  
$H_1: \mu \neq 74.$

Let's set significance level $\alpha = 0.05$. 

---

We can see that there are several parameters that influence our guess:

.pull-left[
- mean value of sample, $\overline{x}$;
- our guess of sample mean, $\mu$; 
- standard deviation of the sample mean, $s$; 
- number of observations, $n$; 
]

.pull-right[
```{r}
makePlot()
abline(v = mean(trees$Height), lwd = 2, col = Col['red'])
sdTree <- sqrt((length(trees$Height)-1)/length(trees$Height)) * sd(trees$Height)
curve(dnorm(x, 74, sdTree), add = T, col = Col['blue'])
abline(v = 74, lwd = 2, col = Col['blue'])
```
]

--

> How can we take all of these parameters into account?

---

We combine these parameters into a test statistic!

--

For this test the test statistic is called $t$ and it's calculated as

$$t = \frac{\bar{x} - \mu}{s \div \sqrt{n}},$$

where $\bar{x}$ is the sample mean and $\mu$ the hypothetical population mean that it is tested against and $s$ sample standard deviation.

>  Does higher value of $t$ indicate higher or smaller difference?

---

Let's calculate the value of test statistic.

``` {r}
Test <- t.test(trees$Height, mu = 74)
```

$$t = \frac{76 - 74}{6.37 \div \sqrt{31}}$$

$$t = 1.748$$

---

### Test statistic $t$ on t-distribution

Test statistic is evaluated on t-distribution with $n-1$ degrees of freedom (df).

``` {r}
par(bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
curve(dt(x, nrow(trees)-1), -3, 3, 
      main = "T-distribution (df = 30)", xlab = "Test statistic", ylab = "Density")
abline(v = c(-Test$statistic, Test$statistic), col = Col['red']) # Empirical
abline(v = c(-1.97, 1.97), col = Col['blue']) # Critical
```

>  Is the value of our test statistic above critical value?

---

Probability of test statistic with value of `r Test$statistic %>% round(3)` is `r Test$p.value %>% round(3)`.

This `r Test$p.value %>% round(3)` is our p-value and we chose significance level to be $\alpha = 0.05$.

$$\text{p-value} \ge 0.05 \implies H_{0}: \bar{x} = \mu$$

$$\text{p-value} < 0.05 \implies H_{1}: \bar{x} \neq \mu$$

> So could the sample with mean tree height of 76 ft come from a population with a mean tree height of 74 ft?

---

class: middle

## So...

The more extreme the difference...

...the higher the value of test statistic...

...the lower the p-value...

...the lower the probability that the difference is coincidental.

---

class: center middle inverse

# Two samples

What if we wish to compare two samples?

---

## Independent samples t-test assuming equal variances

Assumptions:

- normality
- independence
- homogeneity of variance  
  $\frac{1}{2} < \frac{s_1}{s_2}<2$  

---

Example data from "Edgar Anderson's Iris Data" dataset.

```` {r}
makePlot <- function() {
  par(mar = c(4,0,0,0), bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
  plot(density(iris$Petal.Length), type = 'n', 
       main = NA, xlab = 'Petal length, cm', ylab = '', 
       ylim = c(0,1))
  lines(density(iris$Petal.Length[iris$Species == 'versicolor']), col = Col['red'])
  lines(density(iris$Petal.Length[iris$Species == 'virginica']), col = Col['blue'])
}
makePlot()
abline(v = mean(iris$Petal.Length[iris$Species == 'versicolor']), col = Col['red'])
abline(v = mean(iris$Petal.Length[iris$Species == 'virginica']), col = Col['blue'])
````

In this case $\frac{s_1}{s_2} =$ `r var(iris$Petal.Length[iris$Species == 'virginica'])/ var(iris$Petal.Length[iris$Species == 'versicolor'])`.

---

### Hypotheses

Are the mean values equal? Do these samples come from the same population?

$H_0: \bar{x}_1 = \bar{x}_2$  
$H_1: \bar{x}_1 \neq \bar{x}_2$

Let's set significance level $\alpha = 0.05$. 

---

### Test statistic $t$

Test statistic $t$ is calculated as

$$t = \frac{\bar{x}_1 - \bar{x}_2}{se_{\bar{x}_1 - \bar{x}_2}},$$

where $\bar{x}_1$ and $\bar{x}_2$ are the means of samples and $se_{\bar{x}_1 - \bar{x}_2}$ is the standard error of the difference of means that is calculated as follows:

$$se_{\bar{x}_1 - \bar{x}_2} = s_p \times \sqrt{\frac{1}{n_1} +{\frac{1}{n_2}}},$$

where $n_1$ and $n_2$ are sample sizes and $s_p$ is the pooled standard deviation calculated as ...

---

### Test statistic $t$ on t-distribution

``` {r}
Test <- t.test(iris$Petal.Length[iris$Species == 'versicolor'], iris$Petal.Length[iris$Species == 'virginica'], var.equal = T)
```

Value of test statistic is `r Test$statistic %>% round(3)`. Test statistic is evaluated on t-distribution with $n_1+n_2-2$ df.

``` {r}
par(bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
curve(dt(x, nrow(iris)+nrow(iris)-2), -3, 3, 
      main = "T-distribution (df = 30)", xlab = "Test statistic", ylab = "Density")
abline(v = c(-Test$statistic, Test$statistic), col = Col['red']) # Empirical
abline(v = c(-1.97, 1.97), col = Col['blue']) # Critical
```

---

P-value is `r Test$p.value %>% round(3)` and we chose significance level to be $\alpha = 0.05$.

> Do these petal samples come from the same population?

???
Different species: versicolor and virginica.

---

## Independent samples t-test not assuming equal variances

Assumptions:

- normality
- independence

Samples do not need to have homogeneous variance. The test is reliable if $s_1 > 2s_2$ or $s_2 > 2s_1$.

---

Example data from "Edgar Anderson's Iris Data" dataset.

```` {r}
makePlot <- function() {
  par(mar = c(4,0,0,0), bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
  plot(density(iris$Petal.Length), type = 'n', 
       main = NA, xlab = 'Petal length, cm', ylab = '', 
       ylim = c(0,3))
  lines(density(iris$Petal.Length[iris$Species == 'versicolor']), col = Col['red'])
  lines(density(iris$Petal.Length[iris$Species == 'setosa']), col = Col['blue'])
}
makePlot()
abline(v = mean(iris$Petal.Length[iris$Species == 'versicolor']), col = Col['red'])
abline(v = mean(iris$Petal.Length[iris$Species == 'setosa']), col = Col['blue'])
````

In this case $\frac{s_1}{s_2} =$ `r var(iris$Petal.Length[iris$Species == 'versicolor'])/ var(iris$Petal.Length[iris$Species == 'setosa'])`.

---

### Hypotheses

Are the mean values equal? Do these samples come from the same population?

$H_0: \bar{x}_1 = \bar{x}_2$  
$H_1: \bar{x}_1 \neq \bar{x}_2$

Let's set significance level $\alpha = 0.05$. 

---

### Test statistic $t$

Test statistic is the same as for Student test:

$$t = \frac{\bar{x}_1 - \bar{x}_2}{se_{\bar{x}_1 - \bar{x}_2}},$$

where $\bar{x}_1$ and $\bar{x}_2$ are the means of samples and $se_{\bar{x}_1 - \bar{x}_2}$ is the standard error of the difference of means that is calculated as follows:

$$se_{\bar{x}_1 - \bar{x}_2} = \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}},$$

where $s_1$ and $s_2$ are unbiased standard deviations of the samples.

---

### Test statistic $t$ on t-distribution

``` {r}
Test <- t.test(iris$Petal.Length[iris$Species == 'versicolor'], iris$Petal.Length[iris$Species == 'setosa'])
```

Value of test statistic is `r Test$statistic %>% round(3)`. Test statistic is evaluated on t-distribution where df is calculated using a complicated formula.

``` {r}
par(bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
curve(dt(x, nrow(iris)+nrow(iris)-2), -3, 3, 
      main = "T-distribution (df = 30)", xlab = "Test statistic", ylab = "Density")
abline(v = c(-Test$statistic, Test$statistic), col = Col['red']) # Empirical
abline(v = c(-1.97, 1.97), col = Col['blue']) # Critical
```

---

P-value is `r Test$p.value %>% round(3)` and we chose significance level to be $\alpha = 0.05$.

> Do these petal samples come from the same population?

???
Different species: versicolor and setosa.

---

class: middle center inverse

# Paired samples

What if our samples are not independent?

---

## Paired-samples t-test

Assumptions: 

- normality

Let's set significance level $\alpha = 0.05$. 

---

What is paired data?

``` {r}
kable(cbind(sleep[sleep$group == 1, ], sleep[sleep$group == 2, ]))
```

---

Let's see the effect of different drugs on increase in hours of sleep. Data is from dataset "Student's Sleep Data". 

``` {r}
makePlot <- function() {
  par(mar = c(4,0,0,0), bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
  plot(density(sleep$extra), type = 'n', 
       main = NA, xlab = 'Increase in hours of sleep, hour', ylab = '', 
       ylim = c(0,.25))
  lines(density(sleep$extra[sleep$group == 1]), col = Col['red'])
  lines(density(sleep$extra[sleep$group == 2]), col = Col['blue'])
  }
makePlot()
abline(v = mean(sleep$extra[sleep$group == 1]), col = Col['red'])
abline(v = mean(sleep$extra[sleep$group == 2]), col = Col['blue'])
```

???
Effect of different drugs

---

### Hypotheses

Are the mean values equal? Do these samples come from the same population?

Would we also observe the increase in hours of sleep in population?

$H_0: \bar{x}_1 = \bar{x}_2$  
$H_1: \bar{x}_1 \neq \bar{x}_2$

Let's set significance level $\alpha = 0.05$. 

???
If different populations, then increase also occurs in population. Draw!

---

### Test statistic $t$

Test statistic $t$ is calculated as

$$t = \frac{\hat{D}}{s_\Delta\div \sqrt{n_1 + n_2}},$$

where $s_\Delta$ is difference in standard deviation expressed as $s_\Delta = s_1 - s_2$ and  $\hat{D}$  is the mean of differences between paired values calculated as

$$\hat{D} = \frac{1}{n} \sum_{i=1}^i{(X_{i1} - X_{i2})}.$$

???
We're essentially testing if the difference or change is extreme enough.

---

### Test statistic $t$ on t-distribution

``` {r}
Test <- t.test(sleep$extra[sleep$group == 1], sleep$extra[sleep$group == 2],
               paired = T)
```

Value of test statistic is `r Test$statistic %>% round(3)`. Test statistic is evaluated on t-distribution with $n-1$ df where $n$ is the number of pairs.

``` {r}
par(bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
curve(dt(x, Test$parameter), -5, 5, 
      main = "T-distribution (df = 9)", xlab = "Test statistic", ylab = "Density")
abline(v = c(-Test$statistic, Test$statistic), col = Col['red']) # Empirical
abline(v = c(-1.97, 1.97), col = Col['blue']) # Critical
```

---

P-value is `r Test$p.value %>% round(3)` and we chose significance level to be $\alpha = 0.05$.

> Did the hours of sleep increase only in our sample or also in population?

---

class: middle center inverse

# One-tailed tests

What if we are not interested in difference but wish to know if one parameter is larger or smaller than another?

---

### Hypotheses

If we wish to test if mean of sample ( $\bar{x}$ ) is greater than hypothetical population mean ( $\mu$ ), then our hypotheses are the following:

$H_0: \bar{x} \le \mu$  
$H_1: \bar{x} \gt \mu$

If we wish to test if mean of one sample ( $\bar{x}_1$ ) is greater than mean of another sample ( $\bar{x}_2$ ) , then our hypotheses are the following:

$H_0: \bar{x}_1 \le \bar{x}_2$  
$H_1: \bar{x}_1 \gt \bar{x}_2$

---

### Test statistic $t$ on t-distribution (one-tailed)

``` {r}
Test <- t.test(sleep$extra[sleep$group == 1], sleep$extra[sleep$group == 2], 
               paired = T, alternative = 'less')
```

Value of test statistic is `r Test$statistic %>% round(3)`. 

``` {r}
par(bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
curve(dt(x, Test$parameter), -5, 5, 
      main = "T-distribution (df = 9)", xlab = "Test statistic", ylab = "Density")
abline(v = c(Test$statistic), col = Col['red']) # Empirical
abline(v = c(1.645), col = Col['blue']) # Critical
```

Now we observe only one side of the distribution. Is t-statistic above the critical value?

???
For \alpha = 0.05 we need to use critical value of -1.645 for lower-tailed test.

---

P-value is `r Test$p.value %>% round(3)` and we chose significance level to be $\alpha = 0.05$.

$H_0: \bar{x} \le \mu$  
$H_1: \bar{x} \gt \mu$

> Hours of sleep increased in our sample. Did the hours of sleep in population actually decrease?

???
P-value is high. So the probability that the difference is random is high.

---

class: center middle inverse

# Nonparametric tests

What if our data is not normally distributed?

---

## Normal distribution

Commonly used continuous probability distribution due to *central limit theorem*. Many methods assume random variables to be normally distributed.

Symmetric and bell-shaped. Characterized by parameters mean ( $\mu$ ) and variance ( $\sigma^2$ ). ( ${\mathcal{N}}(\mu,\sigma^{2})$ ). For standard normal distribution mean is 0 and variance 1 ( ${\mathcal{N}}(0,1)$.

---

```{r}
par(mfrow = c(1,3), bty = 'na', yaxt = 'n')
curve(dnorm(x, 0, 1), -3,3, main = "N(0,1)");
curve(dnorm(x, 0, .2), -3,3, main = "N(0,.2)");
curve(dnorm(x, 0, 2), -3,3, main = "N(0,2)")
```

---

### How to test normality?

#### QQ plot

On these scatterplots empirical quantiles of data are plotted against theoretical quantiles that represent normal distribution.

Let's use some data on rocks from "Measurements on Petroleum Rock Samples" to test normality.

---

``` {r}
par(mfrow = 1:2, bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
plot(density(rock$area), 
       main = NA, xlab = 'Area of pores space, pixels', ylab = '')
par(yaxt = 's')
curve(dnorm(x, mean(rock$area), sd(rock$area)), min(density(rock$area)$x), max(density(rock$area)$x), 
      add = T, col = Col['red'])
qqnorm(rock$area)
```

--

> Your conclusion?

---

``` {r}
par(mfrow = 1:2, bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
plot(density(rock$shape), 
       main = NA, xlab = 'Perimeter / sqrt(area)', ylab = '')
curve(dnorm(x, mean(rock$shape), sd(rock$shape)), min(density(rock$shape)$x), max(density(rock$shape)$x), 
      add = T, col = Col['red'])
qqnorm(rock$shape)
```

--

> Conclusion?

---

#### Shapiro-Wilk test

Let's set significance level $\alpha = 0.05$. 

We test if the distribution of values is different from normal distribution. So the hypotheses are the following:

$H_0$: Data is normally distributed  
$H_1$: Data is not normally distributed

---

For test statistic $W$ higher value and thus p-value below $\alpha$ indicates non-normality.

--

``` {r, echo = T}
shapiro.test(rock$area)
```

> Conclusion?

--

``` {r, echo = T}
shapiro.test(rock$shape)
```

> Conclusion?

---

## Mann-Whitney U test

Compare two independent samples that are not necessarily normally distributed.

Test assumes independence of samples. It also means that observations need to be **unpaired**.

---

Data on number of breaks and type of wool from "The Number of Breaks in Yarn during Weaving".

The median number of breaks for wool A is `r median(warpbreaks$breaks[warpbreaks$wool == 'A'])` and for wool B `r median(warpbreaks$breaks[warpbreaks$wool == 'B'])`. 

--

.pull-left[
``` {r}
makePlot <- function() {
  par(mar = c(4,0,0,0), bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
  plot(density(warpbreaks$breaks), type = 'n', 
       main = NA, xlab = 'Number of breaks', ylab = '', 
       ylim = c(0,.05))
  lines(density(warpbreaks$breaks[warpbreaks$wool == 'A']), col = Col['red'])
  lines(density(warpbreaks$breaks[warpbreaks$wool == 'B']), col = Col['blue'])
}
makePlot()
```
]

--

.pull-right[
``` {r}
shapiro.test(warpbreaks$breaks[warpbreaks$wool == 'A'])
shapiro.test(warpbreaks$breaks[warpbreaks$wool == 'B'])
```
]

--

> Are these distributions different?

???
Different tests

---


### Hypotheses

Are the mean values equal? Do these samples come from the same population?

Does the number of breaks in yarn depend on the type of wool?

$H_{0}$: Distributions (medians) of both samples are the same.  
$H_{1}$: Distributions (medians) of samples are different.

Formal definition of $H_{0}$: A randomly selected value from one sample is equally likely to be less than or greater than a randomly selected value from a second sample.

Let's set significance level $\alpha = 0.05$. 

???
Draw popluation A and population B.

---

### Test statistic $U$

Test statistic $U$ is calculated as

$$U = \sum^n_{i=1} \sum^m_{j=1} S(X_1, X_2),$$

where $n$ are rows and $m$ columns of a matrix $S(X_1, X_2)$ described as below.

$$S(X_1, X_2) = \begin{cases}
      1 & \text{if } Y < X\\
      \frac12 & \text{if } Y = X\\\
      1 & \text{if } Y > X\
    \end{cases}$$

???

Basically, we compare all values and count the times when values from one sample are higher than values from another sample. The $U$ is just the count of these differences.

---

### Test statistic $U$ on normal distribution

``` {r}
Test <- wilcox.test(breaks ~ wool, warpbreaks)
```

Value of test statistic is `r Test$statistic %>% round(3)`. Test statistic is evaluated on Wilcoxon distribution that approaches normal distribution if $n>20$.

``` {r, eval = F}
par(bty = 'n', family = 'RobotoCondensed')
curve(dwilcox(x, 
              length(warpbreaks$breaks[warpbreaks$wool == 'A']), 
              length(warpbreaks$breaks[warpbreaks$wool == 'B'])), 
      -3, 3, 
      main = "Wilcoxon distribution (m = 27, n = 27)", xlab = "Test statistic", ylab = "Density")
abline(v = c(-Test$statistic, Test$statistic), col = Col['red']) # Empirical
abline(v = c(-1.97, 1.97), col = Col['blue']) # Critical
```

--

P-value is `r Test$p.value %>% round(3)` and we chose significance level to be $\alpha = 0.05$.

> Does the number of breaks in yarn depend on the type of wool?

---

## Wilcoxon signed-rank test

This is similar to Mann-Whitney U test but used for **paired** samples. 

Test assumes that observations are **paired**, but pairs are independent.

---

Let's explore "Weight versus age of chicks on different diets" and compare weights on day 2 and 4.

``` {r}
ChickWeight <- ChickWeight[ChickWeight$Chick %in%
  intersect(ChickWeight$Chick[ChickWeight$Time == 2], 
            ChickWeight$Chick[ChickWeight$Time == 4]), ]
```

``` {r}
makePlot <- function() {
  par(mar = c(4,0,0,0), bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
  plot(density(ChickWeight$weight[ChickWeight$Time <= 4]), type = 'n', 
       main = NA, xlab = "Weight of a chick, g", ylab = '', 
       ylim = c(0,.2))
  lines(density(ChickWeight$weight[ChickWeight$Time == 2]), col = Col['red'])
  lines(density(ChickWeight$weight[ChickWeight$Time == 4]), col = Col['blue'])
}
makePlot()
```

???
Two different time periods but same chicken.

---

### Hypotheses

Did the weight increase only in our sample or also in a population? Is the increase in weight significant?

$H_{0}$: Distributions (medians) of both samples are the same.  
$H_{1}$: Distributions (medians) of samples are different.

Let's set significance level $\alpha = 0.05$. 

---

### Test statistic $W$

The W statistic is calculated as:

$$W = \sum^n_{i = 1}(sgn(x_{1i}x_{2i}) \times R_i),$$

where $sgn(x_{1i}x_{2i})$ is sign function (1 for positive difference, -1 for negative) and $R_i$ is the rank of absolute difference.

???

Basically, we are comparing how different is the ranking of values between two samples. The $W$ is just the sum of ranked differences.

---

### Test statistic $U$ on normal distribution

``` {r}
Test <- wilcox.test(ChickWeight$weight[ChickWeight$Time == 2], 
                    ChickWeight$weight[ChickWeight$Time == 4], paired = T)
```

Value of test statistic is `r Test$statistic %>% round(3)`. Test statistic is evaluated on Wilcoxon distribution that approaches normal distribution if $n > 20$.

``` {r, eval = F}
par(bty = 'n', family = 'RobotoCondensed')
curve(dwilcox(x, 
              length(ChickWeight$weight[ChickWeight$Time == 2]), 
              length(ChickWeight$weight[ChickWeight$Time == 4])), 
      -.1, .1, 
      main = "Wilcoxon distribution (m = 41, n = 41)", xlab = "Test statistic", ylab = "Density")
abline(v = c(-Test$statistic, Test$statistic), col = Col['red']) # Empirical
abline(v = c(-1.97, 1.97), col = Col['blue']) # Critical
```

--

P-value is `r Test$p.value %>% round(3)` and we chose significance level to be $\alpha = 0.05$.

> Did the weight increase only in our sample or also in a population?

---

## Kolmogorov-Smirnov test

This test is equivalent to Mann-Whitney U test, although the calculation is very different.

---

Example data on "The Effect of Vitamin C on Tooth Growth in Guinea Pigs". Let's compare two supplement types.

``` {r}
makePlot <- function() {
  par(mar = c(4,0,0,0), bty = 'n', yaxt = 'n', family = 'RobotoCondensed')
  plot(density(ToothGrowth$len), type = 'n', 
       main = NA, xlab = 'Tooth length, ?', ylab = '', 
       ylim = c(0,.08))
  lines(density(ToothGrowth$len[ToothGrowth$supp == 'VC']), col = Col['red'])
  lines(density(ToothGrowth$len[ToothGrowth$supp == 'OJ']), col = Col['blue'])
}
makePlot()
```

---

### Hypotheses

Is the tooth length different for different types of supplements? Do the supplements have a different effect? 

$H_{0}$: Distributions of both samples are the same.  
$H_{1}$: Distributions of samples are different.

Let's set significance level $\alpha = 0.05$. 

---

## Cumulative distribution function (CDF)

```{r, echo = F}
par(mfrow = 1:2, bty = 'n', family = 'RobotoCondensed')
plot(density(ToothGrowth$len[ToothGrowth$supp == 'VC']), main = "Probability density function (PDF)")
plot(ecdf(ToothGrowth$len[ToothGrowth$supp == 'VC']), main = "Cumulative distribution function (CDF)")
```

---

### Test statistic $D$

``` {r}
Test <- ks.test(ToothGrowth$len[ToothGrowth$supp == 'VC'], 
                ToothGrowth$len[ToothGrowth$supp == 'OJ'])
```

This is simply the maximum absolute difference between two cumulative distribution functions. Value of test statistic is `r Test$statistic %>% round(3)`.

``` {r}
par(mar = c(4,2,0,0), bty = 'n', family = 'RobotoCondensed')
plot(c(0, 40), 0:1, type = 'n', xlab = "Percent infected")
ToothGrowth$len[ToothGrowth$supp == 'VC'] %>% ecdf %>% lines(col = Col['red'])
ToothGrowth$len[ToothGrowth$supp == 'OJ'] %>% ecdf %>% lines(col = Col['blue'])
```

---

P-value is determined by the extremity of the D-statistic on Kolmogorov distribution.

P-value is `r Test$p.value %>% round(3)` and we chose significance level to be $\alpha = 0.05$.

> Do the supplements have a different effect? 

---

class: center middle inverse

# Practical application

---

Load data as follows:

1. Download the data set "barley" from the course notes.
2. Go to cloud.jamovi.org.
3. Open the data set in Jamovi.
4. Use only rows for year 1932 (filter `year == 1932`).

Conduct tests about the yield in population.

1. Are the weights normally distributed?
2. Are the measurements of weights independent?
3. Which test should we use?

> Could the yield in population be greater than 30?

???

If we had kept both years, then the data would not be independent.

---

Use data set "Griliches" from the course notes.

Conduct tests to see if wages for individuals changed.

1. Are wages normally distributed?
2. Are the measurements of wages independent?
3. Which test should we use?

> Is the mean of wage during first observation different from wage in 1980?
>
> Could wages actually have decreased?

???

Paired samples test

---

Use data set "computers" from the course notes.

Conduct tests to see what determines the price of computers.

1. Are the prices normally distributed?
2. Are the prices independent?
3. Which test should we use?

> Does the price depend on the presence of CD-ROM?
>
> Does the incusion multimedia kit influence the price?
>
> Are computers manufactured by premium firms more expensive?

???

Non-parametric test

---

class: inverse
