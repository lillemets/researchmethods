[["index.html", "Research methods Section 1 Syllabus 1.1 Teaching 1.2 Scoring 1.3 Meetings and topics", " Research methods Section 1 Syllabus These course notes include information and study materials on the quantitative part of MS.0825 Research methods course of Agri-Food Business Management Master’s program. 1.1 Teaching Flipped classroom approach is used for teaching. This means that students are expected to learn the methods before meetings. During the meetings we revise the theoretical material, address any questions and apply methods in practice. In order to discuss the topics during meetings it is necessary that students are already acquainted to the reading material. After meetings students apply the methods on their own datasets. 1.1.1 Schedule See meeting times on Moode under course MS.0825 Research methods. 1.1.2 Aims After completing the course students should be able to: understand basic concepts in statistics; know how to describe data, both numerically and visually; choose an appropriate method to solve a problem; use a statistical package for data analysis; communicate the results of an analysis (interpret, explain, present); learn about statistical methods individually; find the courage to apply statistical methods. 1.2 Scoring This part of the course gives 50% of total course points. This 50% consists of 50 points that can be obtained by submitting: answers to tests on the methods based on reading material (\\(10 \\times 2 = 20\\) points), and a report containing the application of methods learned (\\(10 \\times 3 = 30\\) points). Thus, students are required to acquaint themselves with the reading material to understand the theory behind the methods and also to demonstrate the ability to apply the methods in practice. 1.2.1 Tests on reading material From the third meeting onwards, students will complete a test during the beginning of each meeting to demonstrate their understanding of the reading material. Compulsory reading is from the following books: Navarro and Foxcroft (2018), Crawley (2013) and Hastie, Tibshirani, and Friedman (2017). Tests consist of four questions, each contributing half of a point towards the final score. 1.2.2 Research project Students are required to create a research report as follows: Choose at least one data set Apply at least one method from 10 topics on this data set (i.e. you can skip 2 topics) Make changes according to feedback Present the results as a written report In case of each topic, one point is assigned for each of the following: method is suitable for the data, method is correctly applied, interpretation of the results is correct. 1.2.2.1 Datasets To apply methods, datasets need to have multiple variables. We mostly learn to explore continuous data but some methods require at least one discrete variable. Students are free to choose a dataset. Possible sources for data: Jamovi: Open &gt; Data Library Goodle dataset search Kaggle datasets Data hub collections Our World in Data World Values Survey European Social Survey 1.3 Meetings and topics We have a total of 12 meetings that involve the topics as outlined below. We will cover some common statistical methods. Introduction. Descriptive statistics Hypothesis testing. Comparing categorical variables Comparing numerical variables Analysis of variance Correlation analysis Simple linear regression Data transformations Multiple linear regression Logistic regression Principal component analysis Factor analysis Clustering Note that two topics are covered during the first meetings. References "],["introduction.html", "Section 2 Introduction 2.1 Quantitative methods 2.2 Statistics 2.3 Software 2.4 Notation", " Section 2 Introduction 2.1 Quantitative methods The methods introduced in this part of the course differ from the qualitative methods that are explained separately. As the name itself indicates, here the focus is on quantity of observations rather than some quality of a particular observation. The dichotomy of the two distinct kind of methods is summarized as a table below. QuaNTitative QuaLitative Numeric data Semantic data Large-N Small-N, case studies Generalize Explore Measure and test Understand Statistical analysis Interpretation Whether we should apply a quantitative or qualitative method usually depends on the data we are seeking to analyze. If our data includes a large number of observations, it is feasible to express each of them numerically. If we only have data on one or several observations, we can and need to treat them semantically, i.e. find meanings rather than attempt to summarize them. As a result, qualitative methods can only be used to explore the observations that are available to us. If our aim is to generalize findings to an unobserved population, we need to use quantitative methods. Thus, in the latter case we measure our observations and test the differences between them, rather than attempt to understand or interpret each of them separately. 2.2 Statistics Quantitative research methods can be summarized with the term “statistics”. This can refer to the numeric nature of data or to the techniques applied to analyze such data. See Navarro and Foxcroft (2018) sections 1.1 and 1.5 for a brief explanation of why we use and need statistics. Sections 2.3-2.7 are more lengthy but also provide useful background on validity and reliability of statistical methods. Dash (2016) has summarized statistics as follows: Figure 2.1: Flowchart of statistics Some ways to think about different approaches to statistics are explained as follows. 2.2.1 Descriptive and inferential statistics Statistics can not only be used to describe the data we have but also to infer wider conclusions from that data with a certain degree of certainty. While descriptive statistics methods use simple measures, inferential methods involve mathematically and conceptually more complex procedures. These allow us to derive point estimates and intervals that we may also apply to observations outside our data set. Most of the methods introduced during this course will be inferential. 2.2.2 Frequentist and Bayesian approach In this course we only examine the more common Frequentist methods that are based on the work of Ronald Fisher (1890-1962) and dominate the research to this day. In contrast to Bayesian approach, the methods we will learn do not condition estimates on some prior knowledge and only apply a single inference rather than multiply them for a posterior estimate. Not very intuitively, the Frequentist analysis tells us about the probability of data given an estimate, and not directly about probability of an estimate as in case of Bayesian approach. For a more thorough explanation of Bayesian statistics you can read Navarro and Foxcroft (2018) section 16. 2.2.3 Statistics and data science, machine learning, artificial intelligence, … This course will focus on the research methods applied in social sciences. Because in academic research the aim is commonly to explain phenomena, we will not discuss solely predictive methods applied in fields such as data science, machine learning and artificial intelligence. Thus, we will only learn methods that assume small and clean data sets and the use of mathematics and simple calculations rather than programming and complex algorithms. 2.3 Software In case of software for statistics, the most basic distinction can be made between spreadsheet software and statistical packages. The former are mostly used to manage two-dimensional tables of data and run simple calculations on them. For more sophisticated statistical analysis, it is necessary to use a dedicated application or a programming language. Some commonly used software for statistics includes the following: Spreadsheet - table management (e.g. Microsoft Excel, Google Sheets, LibreOffice Calc) SPSS - simple GUI-based app with limited functionality R - extensible programming language designed for data analysis Stata - Mostly CLI-based, well suited for econometrics SAS, Matlab, Statistica, … 2.3.1 Jamovi During this course we use Jamovi app to implement the methods in practice. Jamovi is a new “3rd generation” statistical spreadsheet. designed from the ground up to be easy to use, jamovi is a compelling alternative to costly statistical products such as SPSS and SAS. Jamovi is built on top of the R statistical language, giving you access to the best the statistics community has to offer. You can try it online without installing at https://cloud.jamovi.org/. 2.3.2 R language and RStudio As an alternative to Jamovi, students are free to use R programming language. Due to its steep learning curve, it will not be taught during meetings. R is a language and environment for statistical computing and graphics. R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, …) and graphical techniques, and is highly extensible. 2.4 Notation The notation below is used in these notes. \\(n\\) - sample size (number of observations in sample) \\(N\\) - population size \\(\\bar{x}\\) - sample mean \\(\\mu\\) - population mean \\(\\hat{\\mu}\\) - estimate of the population mean \\(s\\) - sample standard deviation \\(\\sigma\\) - popuation standard deviation \\(\\hat{\\sigma}\\) - estimate of the population standard deviation \\(x\\), \\(y\\) - random variables References "],["data-management.html", "Section 3 Data management 3.1 Data structure 3.2 Scales of measurement", " Section 3 Data management 3.1 Data structure Usually data is stored in 2-dimensonal tables, i.e. data has rows and columns. To apply statistical procedures, most software requires data to be formatted in a conventional way. In this respect it’s useful to follow “tidy data” principle (Wickham 2014): Every column is a variable. Every row is an observation. Every cell is a single value. This is how data should be formatted before running most statistical procedures. One way to tidy a data table is to use the Pivot table feature in a spreadsheet app. 3.2 Scales of measurement Scale of measurement determines the procedures that can be applied to the data. 3.2.1 Steven’s operational theory of measurement The following typology was first published by Stevens (1946). See Navarro and Foxcroft (2018) section 2.2 for a more detailed explanation. Each scale in the table includes also the properties, operations and measures a on previous rows. Scale Property Operations Central tendency Nominal Classification =, ≠ Mode Ordinal Level &gt;, &lt; Median Interval Difference +, − Arithmetic mean Ratio Magnitude ×, / Geometric mean, harmonic mean 3.2.1.1 Nominal scale Nominal variables contain names (i.e. characters, factors or strings) that do not have a natural order. Such data can be summarized only by counting values or determining the mode. 3.2.1.2 Ordinal scale Ordinal variables have the characteristics of nominal variables with the added possibility of naturally ordering these names. As such, ordinal variables can be said to have levels. It’s important to note that an increase of one level to the next is not necessarily numerically equivalent to an increase of another level to the next. Thus, it is not always meaningful to convert these levels to numbers and do calculations on them. 3.2.1.3 Interval scale Interval variables are expressed numerically. While differences between numbers on interval scale are meaningful, there isn’t a natural zero value. Thus, calculations on interval variables is limited to finding differences and division or multiplication of such values is not reasonable. A classic example of an interval variable is temperature. Difference between 5°C and 15°C is 10°C but 15°C is not 3 times warmer than 5°C. 3.2.1.4 Ratio scale Ratio variables are also numeric but have a natural zero value. This means that division and multiplication is meaningful. 3.2.2 Binary scale Binary (or dichotomous) variables do not constitute a distinct scale but can be considered as a subgroup of nominal, ordinal or interval variables. Binary variables can only take two values. Many statistical procedures convert or require the conversion of nominal variables to binary for technical reasons (e.g. “dummy” variables). Binary variables are often coded as 0 for false and 1 for true. As such, calculating the sum or mean of binary variables conveys useful information (think why that is!). 3.2.3 Continuous and discrete variables In addition to the previous typology, we can also distinguish between continuous and discrete variables. These are well defined by Navarro and Foxcroft (2018, 20): “A continuous variable is one in which, for any two values that you can think of, it’s always logically possible to have another value in between. A discrete variable is, in effect, a variable that isn’t continuous. For a discrete variable it’s sometimes the case that there’s nothing in the middle.” 3.2.4 Scales in statistical software A lot of statistical software (e.g. Jamovi) distinguishes between numbers, text and ordered text. The difference between ratio and interval variables is only theoretical. Statistical software treats all numbers as integers or floating-point numbers and does not distinguish between interval and ratio variables. When binary variables are coded so that they only have values 0 and 1, these are also considered numerical. References "],["descriptive-statistics.html", "Section 4 Descriptive statistics 4.1 Central tendency 4.2 Variability 4.3 Plots 4.4 Scatterplot", " Section 4 Descriptive statistics Any data analysis should begin with an exploration of the structure of and values in dataset. Description of values should also be part of the reporting of analysis. This can be done using some simple methods. When we use the methods of descriptive statistics, we need to be aware of that we merely describe the dataset at hand. If observations in the data set constitute a sample, we cannot draw any wider conclusions on population using these methods. Below we are thus defining the statistics for populations and not samples. Description of nominal or ordinal variables is simple and limited to just counting unique values and finding the mode and/or median value. But suppose we have variables that contain sequences of numbers. How can we describe these? 4.1 Central tendency A concise way to represent numeric values is expressing the center of values. This can also be understood the typical value or expected value (expectation of \\(x\\), i.e. \\(E(x)\\)). Note that estimating a central tendency is only meaningful if there is a natural center. i.e. the distribution of values is unimodal. See Navarro and Foxcroft (2018) section 4.1. 4.1.1 Mode Mode is simply the most frequent value in a sequence. This statistic can be useful for nominal and ordinal variables but it’s rarely used for numeric variables, especially on a continuous scale. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Mode 4.1.2 Median When sequence of values is ordered, the middle value is median. In case there are even number of values (\\(N\\)) in a sequence, median is the average of the two values in the middle. Thus, there are equal number of values above and below the median value. When \\(N\\) is an even number, half of the values are above and half below the median value. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Median In R: median(x) 4.1.3 Mean While there are several forms of mean, arithmetic mean is most useful in statistics and also considered here. Mean is in other words the average value: it’s the sum of values (\\(X\\)) divided by number of values: \\[\\bar{X} = \\frac{1}{N} \\sum{^N_{i=1}}{X_i}\\] A feature of mean value is that if you only know the \\(\\bar{X}\\) and \\(N\\), you can still calculate \\(\\sum{^N_{i=1}}{X_i}\\). In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Central Tendency | Mean In R: mean(x) 4.2 Variability Central tendency describes values only partially as it tells nothing about the deviation of values from that mean. Estimating also the variability of values provides a more manifold understanding of values. See Navarro and Foxcroft (2018) section 4.2. 4.2.1 Range Most simple way to express variability is to indicate the range of all values, i.e. the difference between minimum and maximum values. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Range, Minimum, Maximum In R: range(x) 4.2.2 Quantiles and interquartile range Quantiles are in essence calculated just like median, because median is the lowest quantile, the 2-quantile. In addition to dividing sequence of ordered values into two halves separated by median, such sequence can be diveded into any number of equal groups as long as the number of groups (\\(k\\)) is smaller than or equal to the number of values, i.e. \\(k \\le N\\). Some common quantiles are quartiles (\\(k = 4\\)), deciles (\\(k = 10\\)) and centiles/percentiles (\\(k = 100\\)). In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Percentile Values In R: quantile(x) Arguably quartile is most frequently used among quantiles because the 2nd and 4th quartiles contain half of all values. This interquartile range (IQR) thus contains the middle half of all values. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | IQR In R: IQR(x) 4.2.3 Variance Although difficult to interpret, variance is used in many statistical calculations due to its mathematical properties. Variance is mean squared difference from the mean value: \\[Var(X) = \\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}\\], where \\(X_i\\) is the value of observation \\(i\\) and \\(\\bar{X}\\) the mean value. In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Variance In R: var(x) 4.2.4 Standard deviation Interpretability issues of variance arise from the fact that the calculation includes squaring and thus the value of the statistic is not comparable to the initial scale. A solution would be to find a square root of variance. This is what the standard deviation is: the square root of variance. To simplify, standard deviation is the average difference from the mean. \\[\\sigma = \\sqrt{\\frac{1}{N} \\sum{^N_{i=1}{(X_i-\\bar{X})^2}}}\\] In Jamovi: Exploration &gt; Descriptives &gt; Statistics &gt; Dispersion | Std. deviation In R: sd(x) 4.3 Plots Previously explained measures allow us to only summarize values into fewer numbers. Plotting allows to present these values more intuitively or even depict all of the values at once. 4.4 Scatterplot A simple way to examine a relationship between two continuous variables is a scatterplot. It simply depicts all data points on a two-dimensional space where location of each points is determined by it’s value of each variable. If the relationship between the two variables is causal, then it is conventional to use have predictor variable on the horizontal (x-)axis and response variable on vertical (y-)axis. Additional variables can be shown by alternating the size, shape or color of data points. In Jamovi: Regression &gt; Correlation Matrix | Plot In R: plot(x) 4.4.1 Boxplot Boxplots illustrate the quartiles of values (including interquartile range) and extreme values (outliers). Outliers are usually values that are below the 2nd quartile or above the 4th quartile by 1.5 times the interquartile range. On boxplots, half of all the values lie within the box and another half on the lines, outliers excluded. See Navarro and Foxcroft (2018) section 5.2. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Box Plots In R: boxplot(x) 4.4.2 Histogram and density plot Histograms are essentially barplots where all values are divided between ranges of equal with, i.e. bins. Instead of bins, distribution of values can also be expressed continuously by smoothing the bins. This results in a density plot. See Navarro and Foxcroft (2018) section 5.1. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Histograms In R: histogram(x) and plot(density(x)) References "],["hypothesis-testing.html", "Section 5 Hypothesis testing 5.1 Sample and population 5.2 Statistical hypothesis testing", " Section 5 Hypothesis testing Statistical hypothesis testing is a procedure of determining the statistical significance of a given difference. More precisely, it allows to demonstrate whether some difference observed in data can be generalized. As such, it is perhaps the most basic procedure in inferential statistics. 5.1 Sample and population Distinguishing between sample and population is important when we wish to draw inferences about population when we only have data on a sample. 5.1.1 Sample, population, sampling See Navarro and Foxcroft (2018) section 8.1. When we have data on entire population we can simply describe it using descriptive statistics. But often we only have information about part of a population, i.e. we only have a sample of the population. Since we only have data on sample, we can’t be sure if the observed differences in our sample also hold in the population. Hence, in order to estimate population parameters from our sample data, we need to make inferences. In order to draw accurate inferences about population, sample should not be biased toward particular type of observations. Bias in sample can be mitigated by including observations into sample from the population randomly. In such case we have a simple random sample. A non-mathematical definition of a random sample can be that every value in the population has an equal chance of being included in sample. However, samples are very often not random. In addition to a simple random sample, some examples of sampling process are stratified sampling, snowball sampling and convenience sampling. In case of a small population, it is also relevant to consider if observations are drawn from the sample with or without replacement. 5.1.2 The law of large numbers See Navarro and Foxcroft (2018) section 8.2 and figure 8.8. One might guess that the more observations we have in our sample, the better the sample describes the population. This is true and can also be expressed in more statistically sounding terms: as sample size increases, sample parameters tend to approach population parameters in their values. So the more observations there are in a sample, the more accurate are the inferences drawn about population using that sample. 5.1.3 The central limit theorem See Navarro and Foxcroft (2018) section 8.3.3. Due to the law of large numbers we can make certain assumptions about how sample characterizes population. The theorem has been shown to prove the following Navarro and Foxcroft (2018, 165): “The mean of the sampling distribution is the same as the mean of the population The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases The shape of the sampling distribution becomes normal as the sample size increases” 5.1.4 Estimating population parameters We can estimate population mean and standard deviation of a variable from the values in sample. These two parameters are relevant in the context of parametric tests. 5.1.4.1 Population mean See Navarro and Foxcroft (2018) section 8.4.1. Our best estimate of population mean is sample mean, i.e.  \\[\\hat{\\mu} = \\bar{x}.\\] 5.1.4.2 Population standard deviation See Navarro and Foxcroft (2018) section 8.4.2. Standard deviation in a sample tends to be lower than population standard deviation. Thus, unlike the estimate of population mean, estimate of population standard error needs to be corrected for bias as follows: \\[\\hat{\\sigma} = \\sqrt{\\frac{1}{n-1} \\sum{^n_{i=1}{(x_i-\\bar{x})^2}}}.\\] , where \\(x_i\\) is the value of variable \\(x\\) for observation \\(i\\), \\(\\hat{x}\\) is the sample mean and \\(n\\) sample size. This estimate of population standard error is also called standard error of the mean. 5.1.5 Confidence intervals See Navarro and Foxcroft (2018) section 8.5 Whenever we draw an inference about population from our sample, there is always a degree of uncertainty. Confidence intervals (CI) is a measure of this uncertaintly. A 95% CI of the population mean \\(X \\sim N(\\mu,\\sigma^2)\\) is calculated as \\[CI_{95} = \\hat{x} \\pm(1.96\\times\\frac{s}{\\sqrt{n}})\\], where \\(\\hat{x}\\) is the sample mean, \\(s\\) sample standard deviation and \\(n\\) sample size. CI is not the 95% probility that the true value lies between the intervals. CI indicates that if we take many samples from the population, then in case of 95% of these samples the means will lie between the intervals. There is no intuitive interpretation other than the following rather inaccurate explanation: we are just pretty sure that the true value is somewhere in the interval. 5.2 Statistical hypothesis testing The essence of hypothesis testing is to check if some difference in our sample is large enough to reject the idea that there is no difference in population. We are interested in the difference but hypothesis testing actually involves verifying the opposite: lack of a difference. It is not intuitive but it’s just the way it is in Frequentist approach. The actual process of hypothesis testing involves several steps. 5.2.1 Null and alternative hypotheses See Navarro and Foxcroft (2018) section 9.1.2. Statistical hypothesis testing involves two hypotheses: null (\\(H_0\\)) and alternative (\\(H_1\\)) hypothesis. These are usually defined as follows: \\(H_0\\): There is no statistically significant difference. \\(H_1\\): There is a statistically significant difference. We always test if we can reject \\(H_0\\): If we reject \\(H_0\\), then we accept \\(H_1\\). If we fail to reject \\(H_0\\), then we accept \\(H_1\\). 5.2.2 Types of errors See Navarro and Foxcroft (2018) section 9.2. We can make two types of errors when making a conclusion about \\(H_0\\): We retain \\(H_0\\) We reject \\(H_0\\) \\(H_0\\) is true We are correct We commit type I error \\(H_0\\) is false We commit type II error We are correct The rate of type I error is considered the significance level of a test (denoted as \\(\\alpha\\)). Conventionally, \\(\\alpha = 0.05\\) , so we are willing to accept that type I error occurs 5% of times. 5.2.3 Test statistic and the critical region See Navarro and Foxcroft (2018) sections 9.3 and 9.4.1. To make a decision whether or not we can reject \\(H_0\\) we use a test statistic. We reject \\(H_0\\) if the value of a test statistic is at or above its critical value. Or more simply, if it is sufficiently extreme, i.e. different from 0. The critical value depends on \\(\\alpha\\) and is obtained from the critical region of the sampling distribution of the test statistic. If \\(\\alpha = 0.05\\), then the critical region covers 5% of the sampling distribution of test statistic. Test statistic summarizes how extreme is the difference that we are testing. If the difference is high, the value of the test statistic is also high and it is thus likely that we can reject \\(H_0\\). The particular parameters that we use to calculate test statistic depend on test we use. For instance, in case of \\(\\chi^2\\)-tests (Comparing categorical data), test statistic sums up the difference in observed and expected values. If this difference is large in our sample, then the difference is also likely to exist in population, hence we are more likely to reject \\(H_0\\). 5.2.4 P-value See Navarro and Foxcroft (2018) sections 9.5 and table 9.1 in section 9.6. P-value is the probability of obtaining at least as extreme value of test statistic if \\(H_0\\) is correct. In other words, it is the probability that the difference in our sample does not occur in population . Thus, it’s the rate of committing a type I error. Another useful definition has been provided by Navarro and Foxcroft (2018, 193): “\\(p\\) is defined to be the smallest Type I error rate (\\(\\alpha\\)) that you have to be willing to tolerate if you want to reject the null hypothesis.” If the p-value is below \\(\\alpha\\), we reject \\(H_0\\) and accept \\(H_1\\). \\(p \\ge \\alpha \\implies H_0\\) \\(p \\lt \\alpha \\implies H_1\\) P-value is not the probability of \\(H_0\\) being false or \\(H_{1}\\) being true. From theoretical perspective, a lower p-value does not show a greater difference or vice versa. 5.2.5 Multiple comparisons problem Every time you find a statistically significant result, there is a possibility that you got this extreme data by chance. In this case, you committed a type 1 error. Therefore, if you do a lot of tests, the possibility that at least one of the statistically significant results is actually not true. Thus, the number of tests you do should be limited. 5.2.6 Steps in testing a statistical hypothesis Statistical hypothesis testing can be summarized as follows: Decide the appropriate type I error rate, i.e. or \\(\\alpha\\) (significance level). Find an appropriate test and test statistic. Calculate the critical region of the sampling distribution of the relevant test statistic according to \\(\\alpha\\). Calculate the value of the test statistic. Reject \\(H_0\\) if the value of the test statistic is in the critical region and respective p-value is lower than \\(alpha\\). All of the steps to calculate the p-value are actually done by software. Thus, in practice it is only necessary to set \\(\\alpha\\) (conventionally \\(\\alpha = 0.05\\) ) and see if the p-value printed by software is higher or lower than this \\(\\alpha\\) (0.05). 5.2.7 The trial of the null hypothesis This analogy is also described at the end of Navarro and Foxcroft (2018) section 9.1.2. The procedure of hypothesis testing might be confusing and not very intuitive at first. In such case it might help to think about hypothesis testing as a trial with the aim of establishing if there is enough evidence to convict a person. The hypotheses would be the following: \\(H_0\\): Person is innocent, \\(H_1\\): Person is guilty. Person is innocent until proven guilty. Thus, we begin with assuming that \\(H_0\\) is true (there is no difference) and make a conclusion after analyzing the evidence (data): if there is enough evidence (data) that demonstrates the crime (difference), we can reject \\(H_0\\) and must accept \\(H_1\\) that the person is guilty (there is a difference); if there is not enough evidence (data) to demonstrate the crime (difference), we can not reject \\(H_0\\) and must assume that the person is innocent (there is no difference). Note that in the latter case we are still not certain if the person is actually innocent (there is no difference), we might just not have enough evidence (data) to prove it. Similarly, retaining \\(H_0\\) in hypothesis testing does not show that there is no difference but only that our data did not indicate a difference. The analogy applies to errors as well. A type I error would be convicting an innocent person while a type II error would be letting a guilty person free. References "],["comparing-categorical-data.html", "Section 6 Comparing categorical data 6.1 Contingency tables 6.2 \\(\\chi^2\\)-test 6.3 Degrees of freedom", " Section 6 Comparing categorical data 6.1 Contingency tables To test whether or not categorical values of a variable are random, we can compare the counts of categories. This gives us contingency tables where cells indicate how many times each value appears in a categorical variable. When we compare more than one variable, contingency tables have more than one dimension. Usually we tabulate two variables against each other. If one variable has \\(m\\) and another variable has \\(n\\) unique values (categories), then we obtain a 2-dimensional \\(m*n\\) contingency table. 6.2 \\(\\chi^2\\)-test 6.2.1 Goodness of fit \\(\\chi^2\\)-test For goodness of fit \\(\\chi^2\\)-test see Navarro and Foxcroft (2018) section 10.1.3. We test if frequencies of categories are different from some expected frequencies for a single categorical variable: \\(H_0\\): Frequencies of categories are as expected. \\(H_1\\): Frequencies of categories are different from what is expected. The test statistic is calculated as \\[\\chi^2 = \\sum{^k_{i=1}{\\frac{(O_i-E_i)^2}{E_i}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency of \\(i\\)th category (\\(k\\)). In Jamovi: Frequencies &gt; N Outcomes In R: chisq.test(x) 6.2.2 \\(\\chi^2\\)-test of independence For \\(\\chi^2\\)-test of independence see Navarro and Foxcroft (2018) section 10.2. In this case we test if two categorical variables are associated: \\(H_0\\): Variables are independent. \\(H_1\\): Variables are associated. The test statistic is calculated as \\[\\chi^2 = \\sum{^r_{i=1}\\sum{^c_{j=1}}{\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}}},\\] where O is the observed (empirical) frequency and E the expected (theoretical) frequency in \\(i\\)th row (\\(r\\)) and \\(j\\)th column (\\(c\\)). An assumption of \\(\\chi^2\\)-test is that there are \\(&gt;5\\) observations in each cell of a contingency table. If there are less, then the test results are not accurate. In Jamovi: Frequencies &gt; Independent Samples In R: chisq.test(x, y) 6.3 Degrees of freedom See Navarro and Foxcroft (2018) section 10.1.5. Degrees of freedom (DoF) indicates the number of values that are free to vary. This is relevant for evaluating the \\(\\chi^2\\)-test statistic as it indicates the critical value on \\(\\chi^2\\)-distribution. For goodness of fit \\(\\chi^2\\)-test DOF is calculated simply as \\(df = k - 1\\) where \\(k\\) is the number of categories. For \\(\\chi^2\\)-test of independence DoF is calculated as \\[df = (r - 1)(c - 1),\\] where \\(r\\) is the number of rows and \\(c\\) the number of columns in a contingency table. 6.3.1 Interpretation of test results Suppose that a \\(\\chi^2\\)-test indicates that p-value is \\(\\le \\alpha\\). There are several ways in which we could interpret this result: at least one of the frequencies is different from what is expected, there is a statistically significant relationship between the two variables (\\(\\chi^2\\)-test of independence), or one variable has a statistically significant effect on another (based on theory). References "],["comparing-numerical-data.html", "Section 7 Comparing numerical data 7.1 How to decide which test to use? 7.2 One or two samples 7.3 Unpaired or paired samples 7.4 One-tailed or two-tailed tests 7.5 Parametric or nonparametric tests", " Section 7 Comparing numerical data Statistical testing for numerical data is different from when we have categorical data as we are no longer limited to using frequencies. Instead, we can test differences in parameters, ranks and other measures. However, nonparametric tests explained at the end can also be used for ordinal variables. This section covers only comparisons with one or two samples, i.e. when we compare values of one or two groups. Comparing more than two mean values is done using Analysis of variance and is considered in the next section. Also, this section only outlines some classical tests and there are actually many more. 7.1 How to decide which test to use? The particular test that we should choose to compare numeric data depends on how many samples we have, if the values are normally distributed, if samples have equal variances, and if the samples are paired. This section describes the tests that we can apply in each of these cases. The list below can be used as a guide when deciding which test you should use. One sample Unpaired samples Normally distributed One sample T-test Not normally distributed Wilcoxon signed-rank test Two samples Unpaired samples Normally distributed Equal variance Independent samples T-test assuming equal variances (Student test) Unequal variance Independent samples T-test assuming unequal variances (Welch test) Not normally distributed Mann-Whitney U test Paired samples Normally distributed Paired samples T-test / Wilcoxon rank-sum test Not normally distributed Wilcoxon signed-rank test Three samples Unpaired samples Normally distributed Analysis of Variance 7.2 One or two samples 7.2.1 One sample 7.2.1.1 One sample T-test See Navarro and Foxcroft (2018) section 11.2. This test is used to determine if our sample is taken from a population with a given mean. So we test if the sample mean \\(\\bar{x}\\) is equal to a hypothetical population mean \\(\\mu\\). Hypotheses: \\(H_0: \\bar{x} = \\mu\\) \\(H_1: \\bar{x} \\neq \\mu\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x} - \\mu}{s \\div \\sqrt{n}},\\] where \\(\\bar{x}\\) is the sample mean and \\(\\mu\\) the hypothetical population mean that it is tested against and \\(s\\) is sample standard deviation. Test statistic is evaluated on t-distribution with \\(n-1\\) degrees of freedom ( \\(df\\) ). Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.2.3. In Jamovi: T-tests &gt; One Sample T-test. In R: t.test(x, mu) 7.2.2 Two samples 7.2.2.1 Independent samples T-test assuming equal variances (Student test) See Navarro and Foxcroft (2018) section 11.3. This test compares mean values of two samples to determine if these are equal. Equality of means implies that samples come from the same population. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = s_p\\sqrt{\\frac{1}{n_1} +{\\frac{1}{n_2}}},\\] where \\(n_1\\) and \\(n_2\\) are sample sizes and \\(s_p\\) is the pooled standard deviation calculated as \\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}},\\] where \\(s_1\\) and \\(s_2\\) are standard deviations of the samples. Test statistic is evaluated on t-distribution with \\(n_1+n_2-2\\) \\(df\\). Test assumes normality and independence of data and homogeneity of variance. The latter means that variances of the samples need to be equal. This is true if \\(\\frac{1}{2} &lt; \\frac{s_1}{s_2}&lt;2\\). See Navarro and Foxcroft (2018) section 11.3.7. In Jamovi: T-tests &gt; Independent Samples T-test. In R: t.test(x, y) 7.2.2.2 Independent samples T-test not assuming equal variances (Welch test) See Navarro and Foxcroft (2018) section 11.4. This test is equivalent to previously described test but now we don’t assume equal variances. Variances are unequal if \\(s_1 &gt; 2s_2\\) or \\(s_2 &gt; 2s_1\\). Hypotheses are also the same as in case of the Student test. \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic is the same as for Student test: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{se_{\\bar{x}_1 - \\bar{x}_2}},\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the means of samples and \\(se_{\\bar{x}_1 - \\bar{x}_2}\\) is the standard error of the difference of means that is calculated as follows: \\[se_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}},\\] where \\(s_1\\) and \\(s_2\\) are unbiased standard deviations of the samples. Test statistic is evaluated on t-distribution where \\(df\\) is calculated as \\[df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(s_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(s_2^2/n_2\\right)^2}{n_2-1}}.\\] Test assumes normality and independence of data. See Navarro and Foxcroft (2018) section 11.4.2 In Jamovi: T-tests &gt; Independent Samples T-test | Welch's. In R: t.test(x, y, var.equal = FALSE) 7.3 Unpaired or paired samples Previous tests assumed independence of samples. This is not true if we have paired values. For instance, if samples contain measurements of same observations in two different points in time, then the values representing same observations are paired. In tests for such paired data we test if the differences between each pair of values are large enough to be also present in population. 7.3.0.1 Paired samples T-test See Navarro and Foxcroft (2018) section 11.5. Hypotheses: \\(H_0: \\bar{x}_1 = \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\neq \\bar{x}_2\\) Test statistic \\(t\\) is calculated as \\[t = \\frac{\\hat{D}}{s_\\Delta\\div \\sqrt{n_1 + n_2}},\\] where \\(s_\\Delta\\) is difference in standard deviation expressed as \\(s_\\Delta = s_1 - s_2\\) and \\(\\hat{D}\\) is the mean of differences between paired values calculated as \\[\\hat{D} = \\frac{1}{n} \\sum_{i=1}^i{(X_{i1} - X_{i2})}.\\] Test statistic is evaluated on t-distribution with \\(n-1\\) \\(df\\) where \\(n\\) is the number of pairs. Test assumes normality of data. In Jamovi: T-tests &gt; Paired Samples T-test. In R: t.test(x, y, paired = T) 7.4 One-tailed or two-tailed tests See Navarro and Foxcroft (2018) section 11.6. All the tests described above were presented as one-tailed tests. This means that we were not interested in whether the differences are positive or negative. Two-tailed versions of these tests allow us to test not only equality of means but also if one mean is greater or smaller than another. If we wish to test if mean of sample (\\(\\bar{x}\\)) is greater than hypothetical population mean (\\(\\mu\\)), then our hypotheses are the following: \\(H_0: \\bar{x} \\le \\mu\\) \\(H_1: \\bar{x} \\gt \\mu\\) If we wish to test if mean of one sample (\\(\\bar{x}_1\\)) is greater than mean of another sample (\\(\\bar{x}_2\\)), then our hypotheses are the following: \\(H_0: \\bar{x}_1 \\le \\bar{x}_2\\) \\(H_1: \\bar{x}_1 \\gt \\bar{x}_2\\) In Jamovi: T-tests &gt; ... T-test | Group 1 &gt; Group 2 or Group 1 &lt; Group 2. In R: t.test(x, y, alternative = 'less') or t.test(x, y, alternative = 'greater') 7.5 Parametric or nonparametric tests Population parameters (e.g. mean, standard deviance) can be estimated from sample parameters only if we can assume that the distribution of values in population (and thus in sample) follows normal distribution. If we can not make assumptions about the distribution or parameters of underlying population values we need to use nonparametric tests. 7.5.1 Normality There are various ways to determine whether or not values are normally distributed or not. Here we look at QQ plots and Shapiro-Wilk test. 7.5.1.1 QQ plot See Navarro and Foxcroft (2018) section 11.8.1. On such plots quantiles of data are plotted against theoretical quantiles representing normal distribution. If quantiles of data are highly correlated to these theoretical quantiles (relationship follows a straight line), then data is normally distributed. Interpretation of QQ plot is thus not precise. In Jamovi: Exploration &gt; Descriptives &gt; Plots | Q-Q. In R: qqnorm(y) 7.5.1.2 Shapiro-Wilk test See Navarro and Foxcroft (2018) section 11.8.2. This test determines if data is normally distributed or not. In other words, it tests if sample comes from a normally distributed population. Test statistic \\(W\\) is calculated as \\[W = \\frac{(\\sum^n_{i=1}{a_ix_i})^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2}\\] The exact explanation of \\(a\\) and thus the logic is complicated but as always, more extreme value of the test statistic \\(W\\) indicates non-normality. If the test statistic is statistically significant, then data is not normally distributed: \\(H_0\\): Data is normally distributed \\(H_1\\): Data is not normally distributed Note that Shapiro-Wilk test is sensitive to even small deviations from normality if sample size is large (thousands of observations). Also, the test can only be used for sample sizes less than 5000 observations. In such case, consider QQ plot. In Jamovi: Exploration &gt; Descriptives &gt; Statistics | Shapiro-Wilk. In R: shapiro.test(x) 7.5.2 Nonparametric tests 7.5.2.1 Mann-Whitney U test See Navarro and Foxcroft (2018) section 11.9.1. This is a test to compare distributions (and medians) of two unpaired samples if we can’t assume normality. The test is also known as Wilcoxon rank-sum test. Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. Formal definition of \\(H_{0}\\) is as follows: a randomly selected value from one sample is equally likely to be less than or greater than a randomly selected value from a second sample. Test statistic \\(U\\) is calculated as \\[U = \\sum^n_{i=1} \\sum^m_{j=1} S(X_1, X_2),\\] where \\(n\\) are rows and \\(m\\) columns of a matrix \\(S(X_1, X_2)\\) described as below. \\[S(X_1, X_2) = \\begin{cases} 1 &amp; \\text{if } Y &lt; X\\\\ \\frac12 &amp; \\text{if } Y = X\\\\\\ 1 &amp; \\text{if } Y &gt; X\\ \\end{cases}\\] Basically, we compare all values and count the times when values from one sample are higher than values from another sample. The \\(U\\) is just the count of these differences. For \\(n \\ge 20\\), p-value for \\(U\\) is calculated on a normal distribution. Test assumes independence of samples. In Jamovi: T-tests &gt; Independent Samples T-test | Mann-Whitney U. In R: wilcox.test(x, y) 7.5.2.2 Wilcoxon signed-rank test See Navarro and Foxcroft (2018) section 11.9.2 This is similar to Mann-Whitney U test but used for paired samples. The test is also known as One sample Wilcoxon test. Hypotheses: \\(H_{0}\\): Distributions (medians) of both samples are the same. \\(H_{1}\\): Distributions (medians) of samples are different. The W statistic is calculated as: \\[W = \\sum^n_{i = 1}(sgn(x_{1i}x_{2i}) \\times R_i),\\] where \\(sgn(x_{1i}x_{2i})\\) is sign function (1 for positive difference, -1 for negative) and \\(R_i\\) is the rank of absolute difference. Basically, we are comparing how different is the ranking of values between two samples. The \\(W\\) is just the sum of ranked differences. For \\(n \\ge 20\\), p-value for \\(W\\) is calculated on normal distribution. Test has no relevant assumptions. In Jamovi: T-tests &gt; Paired Samples T-test | Wilcoxon rank. In R: wilcox.test(x, y) 7.5.2.3 Kolmogorov-Smirnov test This test is equivalent to Mann-Whitney U test, although the calculation is very different. This test compares the overall shape of two distributions using cumulative distribution function. Hypotheses: \\(H_{0}\\): Distributions of both samples are the same. \\(H_{1}\\): Distributions of samples are different. Test statistic \\(D\\) is simply the maximum absolute difference between two cumulative distribution functions. P-value is determined by the extremity of the test statistic \\(D\\) on Kolmogorov distribution. In R: ks.test(x, y) 7.5.3 Interpretation of test results Suppose that one of the tests above indicates that p-value is \\(\\le \\alpha\\). There are several ways in which we could interpret this result: the differences between the means/medians of two samples are different, there is a statistically significant relationship between the two variables, one variable has a statistically significant effect on another, or the samples come from differen populations. References "],["analysis-of-variance.html", "Section 8 Analysis of variance 8.1 How to decide which test to use? 8.2 One-way ANOVA 8.3 Factorial ANOVA 8.4 Post-hoc tests 8.5 Assumptions of ANOVA", " Section 8 Analysis of variance Whereas tests in the previous section could be used to compare only two samples we often wish to compare parameters of more than two samples. We can use Analysis of variance (ANOVA) to test if several samples are different. In this context samples can be considered as groups of a factor variable. ANOVA can be used to test the effect of one (one-way ANOVA) or multiple factors (factorial ANOVA). 8.1 How to decide which test to use? Depending on the number of factors (grouopings) and whether or not the assumptions are satisfied, there are several types of ANOVA. These are all explained in Navarro and Foxcroft (2018) sections 13 and 14. One factor Unpaired groups Normally distributed Equal variances Fisher’s one-way ANOVA Unequal variances Welch’s one-way ANOVA Not normally distributed Kruskal-Wallis rank-sum test Paired groups (repeated measures) Normally distributed Repeated measures ANOVA Not normally distributed Friedman test More than one factor Unpaired groups Normally distributed Factorial ANOVA 8.2 One-way ANOVA See Navarro and Foxcroft (2018) section 13.2 about how ANOVA works. This test is used to compare mean values of two or more samples of one factor. Samples are distinguished by a grouping or factor variable, so each sample is one group of a factor variable. \\(H_0: \\bar{x}_1 = \\bar{x}_2 = \\bar{x}_3\\) \\(H_1: \\bar{x}_1 = \\bar{x}_2 \\neq \\bar{x}_3\\) or \\(\\bar{x}_1 \\neq \\bar{x}_2 \\neq \\bar{x}_3\\) In other words, our \\(H_1\\) is that at least one group mean is different from others. The basic principle of ANOVA is to evaluate if the variation in data can be explained by a factor, i.e. the variable that defines the samples we are comparing. We can express the variation in variable \\(Y\\) via a linear model: \\[Y_{ij}=\\mu+\\alpha_{j}+\\varepsilon_{ij},\\] where \\(Y_{ij}\\) is the value of \\(Y\\) for observation \\(i\\) from group \\(j\\), \\(\\mu\\) the overall mean, \\(\\alpha_{i}\\) the difference between mean of group \\(j\\) and overall mean \\(\\mu\\), and \\(\\varepsilon_{ij}\\) the difference between group mean and value for observation \\(i\\) from factor \\(j\\). Similarly to how we ordinarily calculate variance, we could also summarize the variance in a variable \\(Y\\) in perhaps a more familiar form as follows: \\[Var(Y) = \\frac{1}{n} \\sum^{G}_{j=1}{\\sum^{N_k}_{i=1}{(Y_{ij} - \\bar{Y})^2}},\\] where \\(n\\) is the number of all observations, \\(Y_{ij}\\) is the value of variable \\(Y\\) for observation \\(i\\) in group \\(j\\), and \\(\\bar{Y}\\) the overall mean of variable \\(Y\\) of all observations in all samples. We can see that a part of variation in variable \\(Y\\) comes from factor (between-group variation) while another part from individual observations (within-group variation). What both of these approaches indicate is that we can divide variation into several parts and express each part separately by using sums of squares: within-group sum of squares, i.e. sum of squares of errors (\\(SSE\\)) \\(SSE=\\sum_{j=1}^{k}\\sum_{i=1}^{n} (Y_{ij}-\\bar{Y}_{j})^{2}\\), \\(df = n - k\\) between group sum of squares, i.e. sum of squares between groups (\\(SSA\\)) \\(SSA=\\sum_{j=1}^{k} (\\bar{Y}_{j}-\\bar{Y})^{2}\\), \\(df = k - 1\\) total sum of squares (\\(SST=SSE+SSA\\)) \\(SST=\\sum_{j=1}^{k}\\sum_{i=1}^{n} (Y_{ij}-\\bar{Y})^{2}\\), \\(df = n - 1\\) In short, \\(SSE\\) represents deviations of observations from group means and \\(SSA\\) deviations of groups from overall mean. For hypothesis testing we also need to consider number of groups (\\(k\\)) and total number of observations (\\(n\\)), so both of these measures need to be divided by respective \\(df\\): mean squares for SSE (MSE) \\(MSE = SSE / (n - k)\\) mean squares for SSA (MSA) \\(MSA = SSA / (k - 1)\\) Test statistic \\(F\\) is simply the ratio between the two: \\[F = MSA / MSE,\\] where \\(MSA\\) could be said to represent variation caused by factor and \\(MSE\\) the variation coming from observations individually. See Navarro and Foxcroft (2018, 334) table 13.1 that summarizes all of these relevant equations as a standard ANOVA table. Thus, the test statistic \\(F\\) expresses how much of the variation comes from a factor relative to variation originating from individual observation. If the \\(F\\) statistic is above 1, then more variation comes from a factor than from random variation. Also, the higher the value of the test statistic, the higher the effect of a grouping variable and the lower the p-value. Test statistic \\(F\\) is evaluated on F-distribution. Test assumes normality within groups, independence of data and homogeneity of variance (the latter only for Fisher’s test). In Jamovi: ANOVA &gt; One-way ANOVA. In R: summary(aov(x, y)) 8.3 Factorial ANOVA Explained in Navarro and Foxcroft (2018) sections 14.1, 14.2 and 14.10. To test the effect of several factors we can use factorial ANOVA. It follows the same logic as described in previous section but it is more complicated since we now have to consider variances across multiple groupings simultaneously. There are multiple types of factorial ANOVA, depending on if we have the same number of observations in each combination of factors (balanced or unbalanced design), consider only main effects or also interactions between factors, and wish to include a continuous variable in addition to a factor. As a result, there are several different ways to construct the model and hypotheses in factorial ANOVA. Because of this complexity we are not going to cover factorial ANOVA in this course, however it would be good if you know that it exists. In Jamovi: ANOVA &gt; ANOVA. In R: summary(aov(y ~ x1 + x2, data)) 8.4 Post-hoc tests The result of ANOVA tells us if there is a difference between any of the groups but it tells us nothing about which particular groups are different from which other groups. To find it out, we need to test the differences between each group separately. See Navarro and Foxcroft (2018) sections 13.5. One way to do that is to simply conduct needed tests manually and use either Bonferroni or preferably Holm correction to correct p-values for multiple testing. To apply Bonferroni correction we simply need to multipy each p-value by the number of tests we ran. The more accurate Holm correction follows a similar idea but also considers the ranking of all p-values. In both cases we increase p-values by the number of tests. See Navarro and Foxcroft (2018) sections 14.8 Another option is to use a procedure that tests all pairwise differences between all group means. One such procedure is Tukey’s HSD. The result of the procedure provides the magnitude and statistical significance of each possible difference. In Jamovi: ANOVA &gt; One-way ANOVA &gt; Post-Hoc test | Games-Howell or Tukey. In R: TukeyHSD(aov(x, y)) 8.5 Assumptions of ANOVA See Navarro and Foxcroft (2018) sections 13.6. The simplest (Fisher’s one-way) ANOVA assumes normality, independence of observations and homogeneity of variance. While the former two can be evaluated in the same way as in case of a T-test, evaluating homogeneity of variance is more intricate since we now have more than two samples to compare. 8.5.1 Levene’s and Brown-Forsythe tests Both of these tests involve calculating each observation’s difference from its group average and then running an ANOVA on the resulting values to test if these differences are the same or not. Levene’s test uses differences from mean value, while Brown-Forsythe evaluates differeces from median value and is thus better suited for data that is not normally distributed. \\(H_0: s_1 = s_2 = s_3\\) \\(H_1: s_1 = s_2 \\neq s_3\\) or \\(s_1 \\neq s_2 \\neq s_3\\) The assumption is satisfied if we fail to reject \\(H_0\\) (p-value is \\(\\ge \\alpha\\)) and retain that all within-group deviations are the same. In Jamovi: ANOVA &gt; One-way ANOVA &gt; Homogeneity test. References "],["correlation-analysis.html", "Section 9 Correlation analysis 9.1 Covariance 9.2 Correlation coefficients 9.3 Correlation matrices and heatmaps 9.4 Statistical significance 9.5 Interpretation", " Section 9 Correlation analysis Correlation analysis involves measuring the strength and direction of association between two variables. A correlation coefficient summarizes the association on a scale from -1 to 1 where the sign indicates if the relationship is positive or negative, value of 0 means a lack of relationship and value of -1 or 1 means that there is perfect association. There are several different ways to calculate correlation coefficients. 9.1 Covariance The simplest way to represent the tendency of two variables to vary similarly is covariance. To find covariance between variables \\(x\\) and \\(y\\), for each observation we multiply the differences of both variables from their mean values and then find the unbiased mean of these multiplications as follows: \\[Cov(x,y) = \\frac{1}{n-1} \\sum^n_{i=1}(x_i - \\bar{x})(y_i - \\bar{y}),\\] where \\(x_i\\) and \\(y_i\\) are the values of variables \\(x\\) and \\(y\\) for observation \\(i\\), \\(\\bar{x}\\) and \\(\\bar{y}\\) mean values of the variables over all observations, and \\(n\\) the number of observations. Because the range of covariance depends on the scales of variables \\(x\\) and \\(y\\), we can not really interpret the absolute value of covariance. This is why we use correlations that have a fixed range of possible values as explained in the introduction of this section. 9.2 Correlation coefficients The main difference lies between parametric (Pearson’s \\(r\\)) and non-parametric correlation coefficients (Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\)). Parametric correlation uses more information and is thus more powerful, however, it is also limited to normally distributed interval or ratio data and measures the linearity and not monotonicity of association. The difference between the two non-parametric correlations is that Spearman’s \\(\\rho\\) is more simple to compute but less suitable for small sample sizes and can not be directly interpreted as opposed to Kendall’s \\(\\tau\\). 9.2.1 Pearson’s correlation See Navarro and Foxcroft (2018) section 12.1.3. This measure evaluates linearity of a relationship and is a parametric method. Perfect positive correlation implies that when \\(x\\) increases by 1 unit, \\(y\\) always increases by fixed unit(s). Pearson’s \\(r\\) for association between variables \\(x\\) and \\(y\\) can be calculated as follows: \\[r = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2 \\sum_{i=1}^{n}(y_i-\\bar{y})^2},\\] where \\(x_i\\) and \\(y_i\\) are the values of variables \\(x\\) and \\(y\\) for observation \\(i\\), \\(\\bar{x}\\) and \\(\\bar{y}\\) mean values of the variables over all observations, and \\(n\\) the number of observations. Essentially, we compare differences from mean value for values of each variable. The coefficient assumes normally distributed and interval or ratio data, linear relationship, and a lack of substantial outliers. In Jamovi: Regression &gt; Correlation Matrix &gt; Correlation Coefficients | Pearson. In R: cor(x) 9.2.2 Spearman’s rank-order correlation See Navarro and Foxcroft (2018) section 12.1.6. Calculation of the coefficient is based on the ranking of values and it thus measures a monotonicity of an association. In case of monotonic relationship, perfect positive correlation implies only that when \\(x\\) increases, then \\(y\\) also always increases. Spearman’s \\(\\rho\\) is calculated as follows: \\[\\rho = 1 - \\frac{6\\sum (R(x_{i})-R(y_{i}))^{2}}{n(n^{2}-1)},\\] where \\(R(x_i)\\) and \\(R(y_i)\\) are the ranks of variables \\(x\\) and \\(y\\) for observation \\(i\\) and \\(n\\) the number of observations. Simply put, we compare rankings of values from each group. The coefficient assumes ordinal, interval or ratio data and monotonic relationship. In Jamovi: Regression &gt; Correlation Matrix &gt; Correlation Coefficients | Spearman. In R: cor(x, method = 'spearman') 9.2.3 Kendall’s rank correlation Kendall’s coefficient is similar to Spearman in that rankings of values rather than actual values are used. Accordingly, the correlation coefficient also measures monotonicity of a relationship. Kendall’s \\(\\tau\\) is calculated using the idea of concordant pairs of rankings of all observations pairwise. For example, when for any two observations values of \\(x\\) is higher (or lower) than vale of \\(y\\), then this pair is counted concordant, otherwise it is discordant. Calculation of the coefficient includes finding the difference between all pairwise concordant and discordant observations: \\[\\tau = \\frac{n_c - n_d}{\\frac{1}{2} n (n-1)}, \\] where \\(n_c\\) is the number of concordant pairs, \\(n_d\\) the number of discordant pairs and \\(n\\) the number of observations. We are essentially evaluating if rankings of \\(x\\) and \\(y\\) are similar when we compare all observations pairwise. The coefficient assumes ordinal, interval or ratio data and monotonic relationship. In Jamovi: Regression &gt; Correlation Matrix &gt; Correlation Coefficients | Kendall's tau-b. In R: cor(x, method = 'kendall') 9.3 Correlation matrices and heatmaps A single correlation between two variables is rarely what we are interested in. Correlation analysis is more useful in situations when we have a large number of variables and we wish to get a quick understanding of all the associations between them. In such case correlation coefficients can be calculated for each pairwise association and summarized as a correlation matrix. This matrix can further be represented as a heatmap where the direction and size of coefficients determine the color of a cell, giving a quick overview of all associations. In Jamovi: Regression &gt; Correlation Matrix &gt; Plot | Correlation matrix ​ or Factor &gt; Reliability Analysis &gt; Additional Options | Correlation heatmap. In R: heatmap(cor(x)) 9.4 Statistical significance Statistical significance for correlation coefficients can also be estimated. In this case hypotheses are simple: \\(H_0:\\) Variables are not associated, $H_1: $ Variables are associated. When we reject \\(H_0\\) we can conclude that a given correlation coefficient also applies to the population where sample was taken from. P-value for Pearson’s \\(r\\) correlation coefficient can be found by calculating the probability of t-statistic on t-distribution: \\[t=r\\sqrt{n-2}/\\sqrt{1-r^{2}},\\] where \\(r\\) is the correlation coefficient and \\(n\\) the number of observations. Thus, the higher the absolute value of correlation coefficient and the more observations it is based on, the more likely is the coefficient generalizable to population. In Jamovi: Regression &gt; Correlation Matrix &gt; Additional Options | Report significance 9.5 Interpretation See Navarro and Foxcroft (2018) section 12.1.5. The value of a correlation coefficient can be interpreted by: its sign that indicates if a correlation is positive or negative, its absolute size to determine the strength of the relationship, and its statistical significance. While the interpretation of the size of a correlation coefficient depends on particular variables and research discipline, the table below can be used as a guide (Navarro and Foxcroft (2018, 288)). Correlation Interpretation -1.0 to -0.9 Very strong negative -0.9 to -0.7 Strong negative -0.7 to -0.4 Moderate negative -0.4 to -0.2 Weak negative -0.2 to 0 Negligible negative 0 to 0.2 Negligible positive 0.2 to 0.4 Weak positive 0.4 to 0.7 Moderate positive 0.7 to 0.9 Strong positive 0.9 to 1.0 Very strong positive It’s important to understand that correlation itself does not imply anything about the causality between two variables. A high correlation might indicate that either one of the variables has an effect on another, that a third variable has an effect on both, or that the correlation is just spurious. References "],["simple-linear-regression.html", "Section 10 Simple linear regression 10.1 Ordinary least squares 10.2 Elements of (OLS) regression models 10.3 Assumptions and diagnostics", " Section 10 Simple linear regression Regression analysis is a statistical procedure that allows us to model relationships between variables so that the causal relationship between variables is defined. It can be considered as the main statistical technique used in economics, i.e. econometrics. There is a large variety of regression models, depending on estimation method, model specification and assumed distributions. This section introduces the most basic of these, the simple linear regression model, i.e. ordinary least squares (OLS) with one independent variable. See Navarro and Foxcroft (2018) section 12.3 for an introduction. 10.1 Ordinary least squares 10.1.1 Model specification Simple linear regression model has one predictor and its model is mathematically expressed as follows: \\[y = \\alpha + \\beta x + \\varepsilon,\\] where \\(y\\) is dependent or explained variable, response or regressand, \\(\\alpha\\) is the intercept or constant, \\(\\beta\\) is a coefficient of \\(x\\), \\(x\\) is independent or explanatory variable, predictor or regressor, and \\(\\varepsilon\\) is the model error. 10.1.2 Calculation The underlying idea behind (ordinary) least squares regression is the minimization of (squared) residuals. Model parameters are calculated (unlike maximum likelihood estimation based on iterations). To estimate the model \\(Y = \\alpha + \\beta x + \\varepsilon\\) we estimate the parameters \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\) as follows: \\(\\hat{\\beta} = \\frac{\\sum{x_{i} y_{i}} - \\frac{1}{n} \\sum{x_{i}}\\sum{y_{i}}}{\\sum{x_{i}^{2}} - \\frac{1}{n} (\\sum{x_{i}})^{2}} = \\frac{Cov[x,y]} {Var[x]}\\) \\(\\hat{\\alpha} = \\overline{y} - \\beta \\overline{x}\\) For a simple model \\(y = \\beta x + \\varepsilon\\) we can simply use matrix algebra on values of \\(x\\) and \\(y\\) to find \\(\\hat{\\beta}\\): \\[\\hat{\\beta} = (X^{T} X)^{-1} X^{T} Y,\\] where \\(X\\) is the matrix of predictor and \\(Y\\) the matrix of response. See Navarro and Foxcroft (2018) section 12.4 for illustrations of the idea. In Jamovi: Regression &gt; Linear regression. In R: lm(y ~ x, data) or summary(lm(y ~ x, data)) 10.2 Elements of (OLS) regression models 10.2.1 Intercept In the equation this is the \\(\\alpha\\) and often referred to as the constant. Intercept is the value of \\(y\\) where regression line crosses the Y-axis, so intercept is the value of \\(y\\) when \\(x\\) is zero ( \\(y|x=0\\) ). Intercept does not need to be theoretically valid but it sometimes is. The statistical significance of the intercept is usually not relevant. Interpretation of the intercept: \\[weight2_{i} = 195.135 + 1.176 * weight1_{i} + \\varepsilon_{i}\\] When weight1 is 0 units, then the value of weight2 is 195.135 units. 10.2.2 Coefficient(s) In the equation the \\(\\beta\\) represents coefficient of \\(x\\). It indicates y how many units \\(y\\) increases when \\(x\\) increases by one unit. We can not be sure if the coefficients are actually significant (when estimation is done on a sample). It is thus necessary to test whether or not coefficients are different from 0. This is done by calculating t-statistic from coefficient and standard error. Coefficients are only relevant if their difference from 0 is statistically significant. 10.2.2.1 Numeric predictors Interpretation of coefficient(s): \\[weight2_{i} = 195.135 + 1.176 * weight1_{i} + \\varepsilon_{i}\\] When weight1 increases by 1 unit, then weight2 increases by 1.176 units. 10.2.2.2 Categorical predictors Interpretation of coefficient(s): Suppose that a categorical variable called diet has the following levels: Low, Medium, High. Then in regression analysis the first level (Low) is considered as the base level. All other factor values are compared against this base value. \\[weight2_{i} = 1019.406 + 1.898dietMedium_{i} + 12.01dietHigh_{i} + \\varepsilon_{i}\\] When diet is Low, then weight2 is 1019.406. When diet is Medium, then weight2 is higher by 1.898 units compared to when diet is Low, i.e. 1021.304. Suppose now that the coefficient for dietHigh is not statistically significant. When diet is High, weight2 is no different compared to when diet is Low. If it was statistically significant, we could say that when diet is High, then weight2 is higher by 12.01 units compared to when diet is Low. 10.2.3 Fitted values These are the values of \\(y\\) calculated using the model for every \\(x\\) in the data. In other words, fitted values are predictions. 10.2.4 Residuals Residuals are model errors, represented by the \\(\\varepsilon\\) in the equation. Residuals are the difference in response between observed and fitted (model predicted) values. We use residuals to evaluate how well model fits data. If residuals are large, the model is not very good. 10.2.5 The \\(R^2\\) This is a way to measure goodness of fit, i.e. how well model fits data. \\(R^2\\) indicates the part of variation in response variable that is explained by the model: \\[R^{2} = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS},\\] where the elements are defined a follows: explained sum of squares, \\(ESS\\); \\(\\sum_{i = 1}^{n} (\\hat{y}_{i} - \\bar{y})^2\\) residual sum of squares, \\(RSS\\); \\(\\sum_{i = 1}^{n} (y_{i} - \\hat{y}_{i})^2\\) total sum of squares, \\(TSS\\); \\(\\sum_{i = 1}^{n} (y_{i} - \\bar{y})^2.\\) Above \\(\\hat{y}_{i}\\) are fitted values, \\(\\overline{y}\\) is the mean value, and \\(y_{i}\\) are the actual values of \\(y\\). Mathematically, the \\(R^2\\) measures how much better is model at explaining the variance of \\(y\\) compared to just the mean. 10.2.6 The adjusted- \\(R^2\\) The more variables we add, the more the model explains. So \\(R^2\\) can be inflated just by adding variables. To penalize a model for the number of predictors ( \\(k\\) ) while considering the number of observations ( \\(n\\) ), the adjusted \\(R^{2}\\) can also be used, particularly for model comparison: \\[\\overline{R^{2}} = 1 - \\frac{RSS/(n-k)}{TSS/(n-1)}\\] 10.3 Assumptions and diagnostics After the estimation of a regression model it should be diagnosed to make sure that it meets at least the following assumptions: Residuals are normally distributed Residuals have equal variance, i.e. variance of residuals does not depend on the value of \\(x\\) In Jamovi: Regression &gt; Linear regression &gt; Assumption Checks. In R: plot(lm(formula, data)) See Navarro and Foxcroft (2018) section 12.9 for an explanation of assumptions and section 12.10 on model checking. 10.3.1 Gauss-Markov assumptions In addition to the practical considerations outlined above, a theoretical way of expressing the assumptions of OLS is via the Gauss–Markov theorem. It posits the following assumptions: linear in parameters \\(Y = \\alpha + \\beta x + \\varepsilon\\) expected error is zero \\(E(\\varepsilon) = 0\\) homoscedasticity \\(var(\\varepsilon) = E(\\varepsilon^{2})\\) no autocorrelation \\(cov(\\varepsilon_{i}, \\varepsilon_{j}) = 0, i \\neq j\\) independence of predictor(s) and residuals \\(cov(x,\\varepsilon) = 0\\) If these are true, then the model is the best linear unbiased estimator (BLUE). References "],["data-transformations.html", "Section 11 Data transformations 11.1 Log-transformations 11.2 Polynomials", " Section 11 Data transformations Sometimes the response variable is not best expressed as a linear function of predictors. This problem often results in residuals that are then not normally distributed or not constant. In such cases estimating the model on transformed data might improve model fit because many relationships in real world are e.g. “logarithmic” in nature. There are two main ways of transforming data for improving the fit of a linear model: using natural logarithms or polynomials. 11.1 Log-transformations Particularly when the distribution of a variable has a strong right skew, linear model could be improved by using naturally logged (\\(log_ex\\)) values of such variable(s). As a result we are essentially converting the distribution of a variable to normal. Depending on whether response or predictor(s) are log-transformed, the resulting model can be one of the following three explained below. It’s important to note that in such transformations the coefficients represent elasticities, i.e. not absolute values but per cent changes in values of response (log-linear) or both, predictors and response (log-log). To illustrate the interpretation of coefficients, examples are provided of a model where the fuel consumption (mpg) was predicted from horsepower (hp). 11.1.1 Log-linear When we use log-transformation on response variable, then we have a log-linear model: \\[ln(y) = \\alpha + \\beta_{1} x + \\varepsilon.\\] Keep in mind that measures of model fit (e.g. \\(R^2\\), AIC) are only valid for model comparison when models have the same predictor variable. Thus, such measures should not be used to compare log-linear model to a model with untransformed response. Interpretation: \\(ln(mpg_{i}) = 3.46 + -0.003 * hp_{i} + \\varepsilon_{i}\\) When hp increases by 1 unit, mpg increases by -0.003429 * 100 = -0.343 percent. In Jamovi: Regression &gt; Linear regression after transforming response variable. In R: lm(log(y) ~ x, data) 11.1.2 Linear-log When one or more predictors are used in logged form, then the model is referred to as a linear-log model: \\[y = \\alpha + \\beta_{1} ln(x) + \\varepsilon\\] Interpretation: \\(mpg_{i} = 72.64 + -10.764 * ln(hp_{i}) + \\varepsilon_{i}\\) When hp increases by 1 per cent, mpg increases by -10.764 / 100 = -0.108 units. In Jamovi: Regression &gt; Linear regression after transforming predictor variable. In R: lm(y ~ log(x), data) 11.1.3 Log-log When either of the previous transformations have not improved the fit of a linear model, we can also transform both response and predictor(s) as follows: \\[ln(y) = \\alpha + \\beta_{1} ln(x) + \\varepsilon\\] Interpretation: \\(ln(mpg_{i}) = 5.545 + -0.53 * ln(hp_{i}) + \\varepsilon_{i}\\) When hp increases by 1 per cent, mpg increases by -0.53 percent. In Jamovi: Regression &gt; Linear regression after transforming response and predictor variables. In R: lm(log(y) ~ log(x), data) 11.2 Polynomials Application of polynomials usually improves the fit of a model when a curve rather than a straight line describes a relationship. We choose the degree of a polynomial depending on how many curves best express the relationship. For example, if there is a single curve, the relationship is quadratic and we should use the 2nd degree polynomial as follows: \\(Y = \\alpha + \\beta_{1} x + \\beta_{2} x^2 + \\varepsilon.\\) See Crawley (2013) sections 7.1.4 on polynomial functions and 10.3 on polynomial regression. Note that as a result of including polynomials the coefficients for the respective predictor are estimated more than once, so there is no reasonable way to interpret the coefficients. In Jamovi: Regression &gt; Linear regression after transforming predictor variable. In R: lm(y ~ x + I(x^2), data) 11.2.1 Ramsey RESET test The test can be used to determine if adding polynomials to a model improves fit of the model or not. We simply do an F-test to determine whether or not there is a difference between a reduced model \\(y = \\alpha + \\beta x + \\varepsilon\\) and a full model \\(y = \\alpha + \\beta_1 x + \\beta_2 x^2 + \\varepsilon\\). Empirically, we are testing if residuals (\\(RSS\\)) of the two models are different or not: \\(H_0: RSS_1 = RSS_2\\) \\(H_1: RSS_1 \\neq RSS_2.\\) References "],["multiple-linear-regression.html", "Section 12 Multiple linear regression 12.1 Multiple predictors 12.2 Predictor selection 12.3 Multicollinearity", " Section 12 Multiple linear regression In addition to modeling the relationship between two variables, we can use regression to estimate models with multiple predictors. An OLS model with more than one predictor is thus called multiple linear regression. See Navarro and Foxcroft (2018) section 12.5 for an introduction and Crawley (2013) section 10.13 for a more detailed explanations. 12.1 Multiple predictors The estimation of a least squares model with multiple predictors is very similar to single linear regression. The only difference in estimation is that an additional term (\\(\\beta_i x_i\\)) for each predictor is added to the model. A model with two predictors can be expressed as follows: \\(y = \\alpha + \\beta_{1} x_1 + \\beta_{2} x_2 + \\varepsilon,\\) where now \\(\\beta_1\\) is a coefficient of \\(x_1\\) and \\(\\beta_2\\) is a coefficient of \\(x_2\\). The interpretation and diagnostics of the model is now more complicated as now we need to consider both predictors. The addition of a second term means that our model is no longer a line but rather a pane (see Figure 12.14 in Navarro and Foxcroft (2018)). This entire pane is now affected by multiple variables which means that \\(\\beta_1\\) is influenced by the addition of \\(\\beta_2\\). Why use multiple predictors? This allows to control the effects of different variables on the response. Also, adding meaningful predictors improves the fit of the model and thus results in more accurate predictions. 12.2 Predictor selection Constructing a model involves choosing which predictors to include. To get a quick understanding of how variables are related, first take a look at pairwise scatterplots. In Jamovi: Regression &gt; Correlation Matrix | Correlation Matrix. In R: pairs(data) How do we know which predictors to include and which to omit? We should start with estimating a model with all the predictors that we theoretically expect to have an effect on the response and then move on to empirical considerations. While there is no general consensus about what to do with coefficients that are not statistically significant, from an empirical point of view these should be omitted from a model. A traditional approach to testing combinations of predictors would be to examine statistical significance of coefficients but this would lead to the multiple comparisons problem and is not suggested anymore. Instead, we can use a parameter such as the Akaike information criterion (AIC) and apply backward elimination or forward selection to determine which combination of predictors results in the best model. See Hastie, Tibshirani, and Friedman (2017) section 3.3 on subset selection methods. 12.2.1 Akaike information criterion For linear regression the information criterion is calculated as follows: \\(AIC = n \\times log(\\frac{RSS}{n}) + 2 K,\\) where \\(n\\) is the number of observations, \\(RSS\\) the residual sums of squares and \\(K\\) the number of predictors in a model. The model is higher if model performs worse and has more predictors. Thus, lower value of AIC indicates a better model. Note that values of AIC can only be compared for models that have the same response variable. See Navarro and Foxcroft (2018) section 12.11. 12.2.2 Model comparison with F-test Model fit can be estimated in the hypothesis testing framework using F-test. This is often used to estimate a general model fit as it allows us to compare the statistical significance of difference between two models. The F-statistic is calculated as follows: \\[F = \\frac{RSS_1 - RSS_1 / k}{RSS_2/(n-p-1)},\\] where \\(RSS_1\\) is the residual sum of squares of reduced model and \\(RSS_2\\) the same for full model, \\(k\\) the difference in the number of parameters between two models, \\(p\\) the number of parameters and \\(n\\) the number of observations. The test statistic is estimated on F-distribution with \\(df_1 = k\\) and \\(df_2 = n-p-1\\). For example, if our reduced model is \\(y = \\alpha + \\varepsilon\\) and full model \\(y = \\alpha + \\beta_{1} x_1 + \\beta_{2} x_2 + \\varepsilon\\), then we would test whether or not the two terms improve model fit compared to just the intercept (\\(E(y)\\)) and our hypotheses would thus be as follows: \\(H_0: \\beta_1 = \\beta_2 = 0\\) \\(H_1: \\beta_1 \\neq 0 \\text{ or } \\beta_2 \\neq 0\\) 12.3 Multicollinearity In addition the assumptions described in the previous section, multiple predictors introduce the possibility of multicollinearity. The idea is that it should not be possible to linearly predict any of the predictors from others predictors, i.e. predictors should not be (highly) correlated. Otherwise the coefficients are not reliable. Multicollinearity can be detected with variance inflation factor (VIF) by using \\(R^2\\) to estimate for each predictor how much of the variation in one predictor can be predicted from others. \\[VIF_{k} = \\frac{1}{1 - R^{2}_{k-1}},\\] where \\(R^{2}_{k-1}\\) is \\(R^{2}\\) for a model that has predictor \\(k\\) as a response variable and all other predictors as predictor variables. Often VIF value of 5 is considered to be too high and indicate that some predictors should omitted from the model. References "],["logistic-regression.html", "Section 13 Logistic regression 13.1 Generalized linear models 13.2 Maximum likelihood estimation 13.3 Interpretation of coefficients 13.4 Model fit", " Section 13 Logistic regression An important assumption of linear regression models is the normality and constant variance of residuals. With some particular types of response variable these assumptions about model errors can not be met. An example of such variable would be binary variables, i.e. variables that can take only two values (\\(0\\) and \\(1\\)). A common way to model relationships with a binary response variable is logistic regression. 13.1 Generalized linear models The reason why the assumptions regarding the residuals are often not met using ordinary linear model is the structure or errors. Thus, it is necessary to define an error structure prior to estimation. In case of binary data (proportions) we can use binomial distribution to obtain a suitable error structure. Binomial distribution can be used to represent the proportion of successes at a particular number of trials. A distribution of errors can not be applied when estimating a model using the idea of least squares. Instead, the estimation needs to be applied more generally, hence the name general linear models (GLM). See section 13 in Crawley (2013). 13.1.1 Logit link In order to apply a particular distribution on model errors in the framework of GLM, we need to use a link function that relates the mean value of response to predictor variable(s). Link function transforms the mean of response variable. Note that we are not transforming the values themselves but using a function of mean value of response. There is no direct relationship between response and predictor variables, which is why we can not use least squares estimation. The canonical link function for binomial errors is the logit link (in econometrics it is often common that probit link is used instead). The logit link is mathematically expressed as follows: \\[ln(\\frac{p}{1 - p}) = \\alpha + \\beta x,\\] where \\(p\\) is the probability that \\(y = 1\\). In addition to the assumptions of normality and consistency of errors, the logit link also takes into consideration the fact that proportions are bounded to values between 0 and 1. See Crawley (2013) section 13.3 on link function and section 13.4 on proportion data and binomial errors. 13.2 Maximum likelihood estimation The parameters of a GLM can not be simply calculated using the least squares method. Instead, we need to use the maximum likelihood (ML) estimation. See Crawley (2013) section 7.3.3 for an idea behind the ML. It’s a little bit complicated idea but can be summarized as follows: models with various parameters are iteratively fitted until such parameters are found for which the data we have is most likely. While the value of this likelihood can not be directly interpreted, it is used to calculate measures of model fit (see below). In Jamovi: Regression &gt; 2 Outcomes. In R: glm(y ~ x, data, family = binomial(link = 'logit')) 13.3 Interpretation of coefficients Due to the transformation of response variable, the interpretation of coefficients of logistic regression with the logit link is somewhat obscure. Recall that the form of the response variable is \\(log(\\frac{p}{1 - p})\\), where \\(p\\) is the probability that \\(y = 1\\) and hence \\(1-p\\) the probability that \\(y \\neq 1\\). Therefore, the coefficients in logistic regression model represent increase in logged odds of \\(y=1\\) when predictor variable increases by one unit. We can also exponentiate the model equation above. Then \\(\\frac{p}{1-p} = e^{\\alpha + \\beta x}\\) which means that one unit of increase in predictor multiplies the odds of \\(y = 1\\) by \\(e^\\beta\\). As this demonstrates, there isn’t an intuitive way to interpret the coefficients. Still, we can interpret the direction and statistical significance. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Coefficients | Odds ratio. 13.4 Model fit 13.4.1 Deviance When fitted values are determined as a result of ML, using residuals is not straightforward (see Crawley (2013) section 13.11). A ML equivalent of RSS in case of OLS is deviance that can also be used to assess the goodness of fit of a LGM. Deviance expresses the difference in log-likelihood between current and saturated model. For binomial error structure the deviance is expressed as \\[D = 2 \\sum y ln(y/\\mu) + (n - y) ln(n - y) / (n - \\mu),\\] where \\(y\\) are the observed values of response, \\(\\mu\\) the mean value of response and \\(n\\) the number of observations. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Fit | Deviance. 13.4.2 Pseudo \\(R^2\\) Because ML does not involve the minimization of squared residuals, it is not suitable to use residuals via \\(R^2\\) to determine model fit. Still, there are equivalent pseudo-\\(R^2\\) measures for expressing model fit for ML that are calculated using likelihoods. While there are several such measures, most common is the McFadden’s pseduo-\\(R^2\\): \\[R^2 = 1 - \\frac{ln\\theta_{reduced}}{ln\\theta_{full}},\\] where \\(\\theta_{reduced}\\) is the likelihood of model with only intercept and \\(\\theta_{full}\\) likelihood of model with predictors. As such, the measure illustrates how much does the addition of predictors improve the model. The values lie between 0 and 1 and usually values \\(&gt;0.4\\) are considered to indicate good fit. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Fit | McFadden's R^2. 13.4.3 Likelihood ratio test This test can be used to determine if a complex model is different from a simpler one. This can be used to estimate the overall model fit. The test statistic is \\[LR = -2ln(\\frac{\\theta_{reduced}}{\\theta_{full}}),\\] where \\(\\theta_{reduced}\\) is the likelihood of simpler model and \\(\\theta_{full}\\) the likelihood of complex model. The hypotheses are as follows: \\(H_0: L_{reduced} = L_{full}\\) \\(H_1: L_{reduced} \\neq L_{full}.\\) If difference can not be shown, then the simpler model should be preferred. In Jamovi: Regression &gt; 2 Outcomes &gt; Model Coefficients | Likelihood ratio tests. 13.4.4 Overdispersion Unobserved omitted variables inflate residual deviances resulting in difficulties establishing the significance of predictors. See Crawley (2013) section 13.12 for more. 13.4.5 Classification The model fitted values of response variable lie between 0 and 1. These can be interpreted as probabilities of \\(y = 1\\) for each observation. In case we wish to have predictions on original binary scale, we need to choose a cutoff point that determines if a probability should be classified as 0 or 1. The classified predictions can be tabulated against true classes which results in a classification table or a confusion matrix. Such table can then be used to calculate measures such as accuracy, specificity, sensitivity, AUC and others that illustrate how good the model is at predicting. In Jamovi: Regression &gt; 2 Outcomes &gt; Prediction. References "],["principal-component-analysis.html", "Section 14 Principal component analysis 14.1 Intuition 14.2 Estimation of PCs 14.3 Calculation 14.4 Number of components 14.5 Interpretation", " Section 14 Principal component analysis Principal component analysis (PCA) is a method in the field of multivariate statistics. In such methods we do not define any causality between variables by distinguishing between response and predictor variables. Instead, methods in multivariate statistics look at the overall structure of data and attempt to identify patterns. PCA allows to find such patterns that could be used to reduce the number of variables without losing too much information. This can be useful e.g. when there are a lot of correlated predictors in a regression model. See Crawley (2013) section 25.1 and Navarro and Foxcroft (2018) section 15.2 (and Hastie, Tibshirani, and Friedman (2017) section 14.5). 14.1 Intuition The underlying goal of PCA in simple terms is to summarize many variables into one, few or several new variables. More precisely, the aim is to determine a set of standardized linear combinations that would explain a maximum amount of variation in data. Sometimes only one linear combination is found such that it best separates observations. As such, PCA is a dimensionality reduction technique. Each linear combination that summarizes part of variance in data is called a principal component (PC). These linear combinations \\(\\xi_j\\) can be expressed as \\(\\xi_j = b_{j1}x_1 + b_{j2}x_2+ ... a_{jp}x_p\\), where \\(b_{jp}\\) is a weight of variable \\(x_p\\) of a particular \\(\\xi_j\\), i.e. \\(j\\)th PC. PCs are thus essentially weighted sum of variables. The ordering of PCs is fixed: PCs are ordered decreasingly, starting from the PC that explains the most variation in data. The maximum number of PCs we can determine is \\(min(p, n - 1)\\). 14.2 Estimation of PCs There are several ways to understand the estimation PCs. An intuitive approach would be to visualize a data cloud from which we incrementally derive the PCs. The first PC should explain the maximum amount of variation in the data cloud. Thus, we choose the first PC so that (1) it follows the direction of largest variance in data cloud and (2) is then also a line with smallest orthogonal distance to all data points. As such the idea is similar to least squares estimation, except that in PCA we consider variation in all variables (not just the response) and do not use squared residuals. See the relevant question on Stack Exchange for some visual explanations. Establishing the direction of largest variation requires transforming data points to a new coordinate system. This involves the following steps: (1) centering data points, (2) scaling the axes so that they would be equal, (3) rotating axes into the direction of a PC. In the last step the first PC is rotated so as to maximize variation, whereas each following PC is rotated to be orthogonal to preceding PC. Estimation of PCs requires data to be centered, i.e. for each variable \\(\\mu = 0\\). This is usually done by software. Another aspect to consider is scale of data. If software does calculations on correlation matrix then scale is irrelevant because correlation matrix is scale agnostic. However, if covariance matrix is used then variables with higher variance are given more importance (recall that PCA involves maximizing variation). Whether or not this is preferred depends on data and research problem. In Jamovi: Factor &gt; Principal Component Analysis | Rotation: none. In R: prcomp(data, scale = TRUE) 14.3 Calculation The transformation of data points into a new coordinate system is done according to the eigenvalues (scaling) and eigenvectors (rotation) derived from data. There are two methods for obtaining these: (1) using covariance or correlation matrix of data or (2) using data matrix. These methods are described below. 14.3.1 Spectral decomposition A data matrix \\(X : n \\times p\\) can be represented by its correlation or covariance matrix \\(\\Sigma\\). This can be decomposed as \\(\\Sigma = V D V^T\\), where \\(D = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_{n-1})\\) is a diagonal matrix of eigenvalues of \\(\\Sigma\\) and matrix \\(V = (v_1, v_2, ..., v_{n-1})\\) the corresponding eigenvectors of \\(\\Sigma\\). This can also be expressed as \\(\\Sigma w_i = \\lambda_i w_i\\) where \\(\\lambda_i\\) is the eigenvalue that corresponds to eigenvector \\(w_i\\) of \\(i\\)th PC. 14.3.2 Singular value decomposition A data matrix \\(X : n \\times p\\) with \\(rank(X) = k\\) can also be decomposed as \\(X = UDV^T\\), where \\(U = (u_1, u_2, \\dots, u_k)\\) is an \\(n \\times n\\) matrix of eigenvectors of \\(XX^T\\), \\(V = (v_1, v_2, ..., v_k)\\) is an \\(p \\times p\\) matrix of eigenvectors of \\(X^TX\\), and \\(D = diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_k)\\) is a diagonal matrix containing nonzero eigenvalues of \\(X^TX\\) and \\(XX^T\\). Such decomposition of our data matix \\(X\\) then gives us the eigenvalues \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\) and corresponding eigenvectors \\(u_1, u_2, \\dots, u_k\\) for \\(X\\). 14.4 Number of components Recall that the aim of PCA is to summarize variables into a smaller number of PCs. The number of PCs that are used is decided by user. Depending on our research problem, we might wish to obtain just one PC or few or more PCs. There also several possible criteria that may help us make the choice: choose a threshold of variation in data that PCs should cumulatively explain, e.g. 80%; use PCs that have eigenvalue above average or above one; use PCs that are above an “elbow” in scree plot. In Jamovi: Factor &gt; Principal Component Analysis | Number of Components. 14.5 Interpretation 14.5.1 Variation explained The proportion of total variation explained by a \\(j\\)th PC can be found from its eigenvalues: it is the proportion of sum of eigenvalues, i.e. \\(\\lambda_j / \\Sigma^p_{j=1} \\lambda\\). The proportion of variation explained by a number of first PCs can be found by cumulative sum of the respective values. This measure illustrates how much information is preserved when summarizing data into a particular number of PCs. In Jamovi: Factor &gt; Principal Component Analysis | Component summary. In R: prcomp(data, scale = TRUE) 14.5.2 Loadings Each PC is defined by the influence that each variable has on it. As such, each variable has a particular loading on each PC which is why the values of this influence are called loadings. Loadings can also be understood as variable weights. When the linear combinations are calculated for each observation using loadings (weights) and initial variables, we obtain principal component scores. These are in essence the new summarized variables. Loadings can be used to interpret PCs to give them a theoretical meaning. This is a rather creative process. In Jamovi: Factor &gt; Principal Component Analysis | Rotation: none &gt; Save | Component scores. In R: prcomp(data, scale = TRUE)$x 14.5.3 Biplots These plots concisely summarize the first two PCs by depicting observations on these two dimensions together with loadings of each variable as arrows. Directions of these arrows indicate the direction of loading and length of arrows indicate the magnitude of loading. Then, the angle between arrows represents correlations between variables. In Jamovi: snowCluster &gt; PCA plot | Biplot. In R: biplot(prcomp(data, scale = TRUE)) References "],["factor-analysis.html", "Section 15 Factor analysis 15.1 FA and PCA 15.2 Orthogonal factor model 15.3 Rotations 15.4 Assumptions 15.5 Number of components 15.6 Interpretation", " Section 15 Factor analysis If we think that there might be some underlying variables behind the variables that we observed, we can use Pactor Analysis (FA) to discover these. Here we will only be covering exploratory FA. See Crawley (2013) section 25.2 and Navarro and Foxcroft (2018) section 15.1 (and Hastie, Tibshirani, and Friedman (2017) section 14.7.1). 15.1 FA and PCA Both FA and PCA are variation maximization techniques. In PCA we are attempting to maximize the total variation in data. In factor analysis we instead try to maximize some underlying shared variation between variables. As such, factors in FA are the underlying latent variables that load the observed variables that we have. Factors are equivalent to principal components (PCs) in PCA. A common example are personality traits: we can not measure these directly but we can ask subjects various questions and then use factor analysis to summarize the responses into factors that represent these traits. The distinction leads to some differences between FA and PCA: PCA aims to explain total variance while FA aims to explain covariances. PCs are uniquely defined while factors depend on estimation and rotation method In PCA each PC is a linear combination of variables but in FA the variables are expressed as linear combinations of factors. How do we decide which method to use? If our goal would be to summarize variables into a smaller set of variables, then we should use PCA. If the goal is to determine some underlying variables, then we should use FA. 15.2 Orthogonal factor model The factor model is defined as \\[x_j = q_{jl} F_l + U_j +\\mu_j,\\] where \\(x_j\\) is a variable \\(j\\), \\(q_{jl}\\) the loading of the \\(j\\)th variable on the \\(l\\)th common factor, \\(F_l\\) the \\(l\\)th common factor, \\(U_j\\) the \\(j\\)th specific factor and \\(\\mu_j\\) the mean of \\(x_j\\). There are three methods for extracting factors. We are not going to cover the differences but these are: minimum residuals, maximum likelihood, and principal axis method. In Jamovi: Factor &gt; Exploratory Factor Analysis. 15.3 Rotations To better define factors that best separate our variables, we can apply a rotation on factors. This results in an adjusted loadings matrix. There are several techniques for rotation. Of these the three most common are described below. 15.3.1 Varimax This rotation involves maximizing variances of squared loadings within factors. Factors remain orthogonal, thus uncorrelated. In Jamovi: Factor &gt; Exploratory Factor Analysis | Method | Rotation: Varimax. 15.3.2 Oblimin This rotation also involves maximization of squared loading variances within factors. The difference from Varimax rotation is that factors are not necessarily orthogonal but oblique, thus can be correlated. The resulting factors may be more similar to each other than in case of Varimax but the variances of loadings within factors are likely to be higher. In Jamovi: Factor &gt; Exploratory Factor Analysis | Method | Rotation: Oblimin. 15.3.3 Quartimax This rotation aims to maximize variances of factor loadings within variables (not factors). The result is a factor structure in which each variable loads as few factors as possible. In Jamovi: Factor &gt; Exploratory Factor Analysis | Method | Rotation: Quartimax. 15.4 Assumptions While PCA has essentially no assumptions, FA has a few. These can be described and tested for as explained below. 15.4.1 Sphericity Variables need to be correleated to a certain extent for us to able to summarize them into a smaller set of factors. Whether or not correlation matrix is statistically significantly different from zero can be tested with Bartlett’s test. \\(H_0:\\) Correlation matrix is zero \\(H_1:\\) Correlation matrix is different from zero If the test returns \\(p \\ge 0.05\\), then we should not trust the results of factor analysis. In Jamovi: Factor &gt; Exploratory Factor Analysis | Assumption Checks | Bartlett's test of sphericity. 15.4.2 Sampling adequacy In addition to correlations it is necessary that each factor is not related to only few variables. We can use Kaiser-Meyer-Olkin (KMO) measure to determine if sum of partial correlations is higher than sum of correlations or not. The value of the measure is interpreted as follows regarding the suitability of FA. 0 … 0.6 - Not suitable 6… 0.7 - Suitable 0.7 … 0.9 - Adequate 0.9… 1 - Excellent In Jamovi: Factor &gt; Exploratory Factor Analysis | Assumption Checks | KMO measure of sampling adequacy. 15.5 Number of components The methods that can be used to determine an optimal number of components are the same as those for PCA. For FA a further consideration can be the factor loadings or uniqueness. If some variable has a very high uniqueness, adding factors might increase the respective communality. In Jamovi: Factor &gt; Exploratory Factor Analysis | Number of Components. 15.6 Interpretation 15.6.1 Loadings The interpretation of loadings in FA is similar to PCA. For FA only loadings that are higher than 0.3 or 0.5 should be considered and used for interpretation of factors. In Jamovi: Factor &gt; Exploratory Factor Analysis | Factor Loadings | Hide loadings below: 0.5. 15.6.2 Uniqueness and communality Uniqueness is a measure of proportion of variation explained by a particular variable and not by factors. Communality is the inverse of this (\\(1-Uniquenss\\)), i.e. the variation of a variable that is shared with other variables. 15.6.3 Complexity This can be calculated for each variable. It expresses the number of factors which load the variable. FA is a controversial method in statistics because we can never be sure if the factors actually exist and were correctly estimated. This is due to the different options for choosing the number of factors, methods of estimating the model, rotation techniques and subjectivity of interpreting factors. References "],["cluster-analysis.html", "Section 16 Cluster analysis 16.1 Cluster analysis 16.2 Hierarchical clustering 16.3 K-means clustering", " Section 16 Cluster analysis The purpose of cluster analysis is to categorize objects into some homogeneous groups. It is different from classification in the sense that we do not have prior information on classes. The assignment of objects in cluster analysis is based on the differences between observations by some variables. See the beginning of section 14.3 in Hastie, Tibshirani, and Friedman (2017). 16.1 Cluster analysis Clustering methods can be divided into model-based and distance-based methods. Here we considered the latter. The distance-based methods can be further divided into hierarchical clustering and partitioning (K-means) methods. These are explained below. Although a common application of cluster analysis is to categorize observations, it can also be applied to group variables. In the latter case distances by observations are considered, so we treat observations as variables and vice versa. As with the previous multivariate statistics methods, it is important to be mindful of variances of variables. Variables with higher variances will be given more leverage in deciding clusters. If this is deemed undesirable, data should be standardized prior to clustering. 16.1.1 Distance measures The assignment of objects into groups should be such that observations are more similar within groups than between groups. This means that clustering is done according to measures of similarities or dissimilarities, i.e. distances. For matrix \\(X : n \\times p\\) with \\(n\\) observations and \\(p\\) variables the distances \\(d\\) between objects \\(i\\) and \\(j\\) can be described as a matrix \\(D : n \\times n\\) where \\(d_{ij} = d(x_i,x_j)\\). There are various measures for dissimilarity but most common are Euclidean and Manhattan distances. These can be calculated for numeric variables. Euclidean distance expresses a “direct” distance between two points. It is calculated according to the Pythagorean theorem. Manhattan distance can be thought of as a “city block” distance. The distances between objects \\(x_i\\) and \\(x_j\\) can be calculated respectively as follows: \\(d_{Euclidean}(x_i,x_j) = [\\sum^p_{k=1}(x_{ik} - x_{jk})^2]^{1/2}\\), \\(d_{Manhattan}(x_i,x_j) = \\sum^p_{k=1} |x_{ik} - x_{jk}|\\). Distance measures for ordinal and nominal variables also exist but are not explained here. 16.2 Hierarchical clustering Hierarchical clustering is a process where clusters are incrementally constructed. Grouping of objects can start from the most similar objects in which case the process is agglomerative (“bottom-up”). In contrast, a divisive process (“top-down”) begins with most different objects and moves on to more similar objects. The agglomerative method is more common and also explained here. See Hastie, Tibshirani, and Friedman (2017) section 14.3.12. Agglomerative hierarchical clustering has the following steps. Initial number of clusters is \\(n\\), so each cluster contains one object. Calculate distance matrix \\(D\\) that expresses pairwise distances between clusters (objects). Find the smallest distance and merge the objects with smallest distance into a single cluster. Calculate a new distance matrix \\(D\\) that now includes distance between the new cluster and all other clusters using a linkage method (see below). Repeat the previous two steps until all objects are in a single cluster. In Jamovi: snowCluster &gt; Hierarchical clustering method. 16.2.1 Linkage methods Computing distance between two objects is simple because each object represents a single point in an Euclidean space. Calculating distances becomes more complicated once we have clusters containing more than one point. There are various approaches to finding the distance between cluster \\(IJ\\) and all other clusters \\(K\\): single linkage: \\(d_{IJ,k} = min(d_{i,K}, d_{j,K})\\); complete linkage: \\(d_{IJ,k} = max(d_{I,K}, d_{J,K})\\); average linkage: \\(d_{IJ,k} = \\sum_{i \\in IJ} \\sum_{k \\in K} d_{ik} / (n_{ij}n_k)\\); Ward’s method: compares the within-cluster and between-cluster squared distances. 16.2.2 Visualization The clusters in case of hierarchical clustering are estimated incrementally, resulting in a nested structure. This tree-shaped structure can be visualized by a dendrogram. In Jamovi: snowCluster &gt; Hierarchical clustering method &gt; Plot | Plot dendrogram. 16.2.3 Number of clusters The number of clusters to be acquired can be decided on using the dendrogram. Cutting the dendrogram at certain height results in a number of clusters that exist on that particular height. A rule that can be used to find the optimal number of clusters is to find the longest consecutive height and cut between the ends of that at the point where another heights are the longest. 16.3 K-means clustering This method groups objects by partitioning them iteratively. The number of clusters \\(K\\) has to be defined before estimation. The goal is to partition objects \\(x\\) into \\(K\\) clusters so that distances between objects within cluster are small compared to distances to points outside the cluster. We can achieve this by assigning each object to the closest centroid. This mean point of clusters \\(\\bar x_1, ... \\bar x_K\\) needs to be calculated for each of the \\(K\\) clusters, hence the name K-means. The optimal mean vector can be found by minimizing the following function: \\[ESS = \\sum^K_{k = 1} \\sum_{c(i)=k} (x_i - \\bar x_k)^T(x_i - \\bar x_k),\\] where \\(c(i)\\) is the cluster containing \\(x_i\\). Simply put, we attempt to minimize the sum of distances of objects from centroid within all clusters. The function \\(ESS\\) is minimized iteratively as described below. See Hastie, Tibshirani, and Friedman (2017) section 14.3.6. K-means clustering involves the following steps: We start with a distance matrix \\(D\\) based on random assignment of objects to \\(K\\) clusters with cluster means, or some (random) cluster means. Calculate squared Euclidean distance between each object and each cluster mean. Reassign each item to its nearest cluster mean, resulting in decreased \\(ESS\\). Update cluster means. Repeat the previous two steps until objects can not be reassigned, so each object is closest to its own cluster mean. In Jamovi: snowCluster &gt; K-means clustering method. An alternative is K-medoids clustering in which case the centroids are not some mean values but represented by actual objects. 16.3.1 Number of clusters (Gap statistic) The Gap statistic is a technique used to determine the optimal number of clusters. The measure compares the sum of average distances within cluster to the same sum obtained from uniformly distributed data. Higher value indicates better fit of data to the \\(K\\) but only clusters that substantially contribute to the value should be considered. The optimal number of clusters is at the value of the Gap statistic \\(k\\) where \\(k + se(k)\\) is higher or equal to the estimate \\(k\\) of next number of clusters. If this optimal value indicates that \\(K = 1\\), then the data does not contain any natural clusters and clustering methods are not suitable. In Jamovi: snowCluster &gt; K-means clustering method &gt; Plots | Optimal number of clusters. 16.3.2 Visualization When we apply K-means clustering we do not obtain a nested structure and therefore can not express the clustering as a dendrogram. We can create a plot that shows the locations of objects and cluster centroids on a two-dimensional plot (Cluster plot in Jamovi). The dimensions can be found via PCA or multidimensional scaling. In Jamovi: snowCluster &gt; K-means clustering method &gt; Plots | Cluster plot. For the interpretation of clusters we can see the mean values of variables by clusters. In Jamovi: snowCluster &gt; K-means clustering method &gt; Plots | Plot means across clusters. References "],["data-sets.html", "Section 17 Data sets", " Section 17 Data sets These data sets are used for practical application of methods. See the info for details on what the variables in data sets represent. barley - Yield data from a Minnesota barley trial (info) chickwts - Chicken Weights by Feed Type (info) Computers - Prices of Personal Computers (info) Fatalities - US Traffic Fatalities (info) Griliches - Wage Data (info) M&amp;Ms - Color counts and net weight for a sample of 30 bags of M&amp;M’s (info) mtcars - Motor Trend Car Road Tests (info) PhDPublications - Doctoral Publications (info) PSID1976 - Labor Force Participation Data (info) "],["slides.html", "Section 18 Slides", " Section 18 Slides Here is an index of slides used to explain the methods during meetings. The essential information on the slides is covered in the course notes but slides might add some explanations and visuals using example data sets. On slides press h button to see keyboard shortcuts for navigation. Introduction, data management, descriptive statistics Hypothesis testing, comparing categorical data Comparing numerical data Analysis of variance Correlation analysis "],["references.html", "Section 19 References", " Section 19 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
